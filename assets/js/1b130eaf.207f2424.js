"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[3546],{2624:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>c,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"module-3-isaac/ch8-perception/object-detection","title":"Object Detection with DNN Inference","description":"Overview","source":"@site/docs/module-3-isaac/ch8-perception/object-detection.md","sourceDirName":"module-3-isaac/ch8-perception","slug":"/module-3-isaac/ch8-perception/object-detection","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/object-detection","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ch8-perception/object-detection.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Visual SLAM (VSLAM)","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/vslam"},"next":{"title":"Depth Estimation","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/depth-estimation"}}');var t=s(4848),r=s(8453);const c={},o="Object Detection with DNN Inference",l={},a=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Part 1: Object Detection Fundamentals",id:"part-1-object-detection-fundamentals",level:2},{value:"What is Object Detection?",id:"what-is-object-detection",level:3},{value:"Popular Detection Architectures",id:"popular-detection-architectures",level:3},{value:"How YOLO Works (Simplified)",id:"how-yolo-works-simplified",level:3},{value:"Part 2: Isaac ROS DNN Inference Setup",id:"part-2-isaac-ros-dnn-inference-setup",level:2},{value:"Step 1: Install Isaac ROS DNN Inference",id:"step-1-install-isaac-ros-dnn-inference",level:3},{value:"Step 2: Download Pre-trained YOLO v8 Model",id:"step-2-download-pre-trained-yolo-v8-model",level:3},{value:"Part 3: Running Object Detection in Isaac Sim",id:"part-3-running-object-detection-in-isaac-sim",level:2},{value:"Step 1: Setup Scene with Objects",id:"step-1-setup-scene-with-objects",level:3},{value:"Step 2: Enable ROS 2 Camera Bridge",id:"step-2-enable-ros-2-camera-bridge",level:3},{value:"Step 3: Launch Isaac ROS Object Detection",id:"step-3-launch-isaac-ros-object-detection",level:3},{value:"Step 4: Visualize Detections in RViz",id:"step-4-visualize-detections-in-rviz",level:3},{value:"Step 5: Inspect Detection Messages",id:"step-5-inspect-detection-messages",level:3},{value:"Part 4: Understanding Detection Output",id:"part-4-understanding-detection-output",level:2},{value:"Detection Message Structure",id:"detection-message-structure",level:3},{value:"Filtering by Confidence Threshold",id:"filtering-by-confidence-threshold",level:3},{value:"Part 5: Training a Custom Object Detector",id:"part-5-training-a-custom-object-detector",level:2},{value:"Step 1: Prepare Synthetic Dataset",id:"step-1-prepare-synthetic-dataset",level:3},{value:"Step 2: Convert COCO Annotations to YOLO Format",id:"step-2-convert-coco-annotations-to-yolo-format",level:3},{value:"Step 3: Create Dataset Configuration",id:"step-3-create-dataset-configuration",level:3},{value:"Step 4: Train YOLOv8",id:"step-4-train-yolov8",level:3},{value:"Step 5: Deploy Custom Model",id:"step-5-deploy-custom-model",level:3},{value:"Part 6: Integrating Detection with Robot Actions",id:"part-6-integrating-detection-with-robot-actions",level:2},{value:"Use Case: Navigate to Detected Object",id:"use-case-navigate-to-detected-object",level:3},{value:"Use Case: Grasp Detected Object",id:"use-case-grasp-detected-object",level:3},{value:"Part 7: Performance Optimization",id:"part-7-performance-optimization",level:2},{value:"Metric 1: Inference FPS",id:"metric-1-inference-fps",level:3},{value:"Metric 2: Detection Accuracy (mAP)",id:"metric-2-detection-accuracy-map",level:3},{value:"Part 8: Troubleshooting Common Issues",id:"part-8-troubleshooting-common-issues",level:2},{value:"Issue 1: No Detections Output",id:"issue-1-no-detections-output",level:3},{value:"Issue 2: Low Confidence Scores",id:"issue-2-low-confidence-scores",level:3},{value:"Issue 3: Duplicate Detections",id:"issue-3-duplicate-detections",level:3},{value:"Part 9: Hands-On Exercise",id:"part-9-hands-on-exercise",level:2},{value:"Exercise: Build a &quot;Fruit Picker&quot; Robot",id:"exercise-build-a-fruit-picker-robot",level:3},{value:"Requirements",id:"requirements",level:4},{value:"Deliverables",id:"deliverables",level:4},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"object-detection-with-dnn-inference",children:"Object Detection with DNN Inference"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object detection"}),' enables robots to identify and locate objects in camera images in real-time. Using deep neural networks (DNNs) like YOLO and Faster R-CNN, robots can detect multiple objects per frame with bounding boxes and class labels, enabling tasks like "pick up the red apple" or "navigate around people."']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand object detection architectures (YOLO, Faster R-CNN)"}),"\n",(0,t.jsx)(n.li,{children:"Run pre-trained models on Isaac ROS with GPU acceleration"}),"\n",(0,t.jsx)(n.li,{children:"Train custom object detectors on synthetic data"}),"\n",(0,t.jsx)(n.li,{children:"Integrate detection with robot control (grasp detected objects)"}),"\n",(0,t.jsx)(n.li,{children:"Optimize inference for real-time performance (30+ FPS)"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed: VSLAM lesson (Chapter 8, previous)"}),"\n",(0,t.jsx)(n.li,{children:"Synthetic Data Generation (Chapter 7)"}),"\n",(0,t.jsx)(n.li,{children:"Python and basic deep learning concepts"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Estimated Time"}),": 3-4 hours"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Explain how YOLO and Faster R-CNN architectures work"}),"\n",(0,t.jsx)(n.li,{children:"Install and configure Isaac ROS DNN Inference nodes"}),"\n",(0,t.jsx)(n.li,{children:"Run object detection on live camera streams from Isaac Sim"}),"\n",(0,t.jsx)(n.li,{children:"Visualize detection results (bounding boxes, labels, confidence scores)"}),"\n",(0,t.jsx)(n.li,{children:"Train a custom detector on synthetic data from Chapter 7"}),"\n",(0,t.jsx)(n.li,{children:"Achieve real-time inference (30+ FPS) with GPU acceleration"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-1-object-detection-fundamentals",children:"Part 1: Object Detection Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-object-detection",children:"What is Object Detection?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"})," = ",(0,t.jsx)(n.strong,{children:"Classification"})," (what is it?) + ",(0,t.jsx)(n.strong,{children:"Localization"})," (where is it?)"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Input"}),": RGB image (e.g., 1280x720)\n",(0,t.jsx)(n.strong,{children:"Output"}),": List of detected objects, each with:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Bounding box"}),": ",(0,t.jsx)(n.code,{children:"(x_min, y_min, x_max, y_max)"})," pixel coordinates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Class label"}),': e.g., "person", "box", "robot"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Confidence score"}),": 0-1 (model certainty)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'[\n  {"class": "apple", "bbox": [320, 180, 420, 280], "confidence": 0.95},\n  {"class": "mug", "bbox": [500, 200, 600, 350], "confidence": 0.87}\n]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"popular-detection-architectures",children:"Popular Detection Architectures"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Architecture"}),(0,t.jsx)(n.th,{children:"Speed (FPS)"}),(0,t.jsx)(n.th,{children:"Accuracy (mAP)"}),(0,t.jsx)(n.th,{children:"Best For"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"YOLO v8"})}),(0,t.jsx)(n.td,{children:"50-120 FPS"}),(0,t.jsx)(n.td,{children:"50-55%"}),(0,t.jsx)(n.td,{children:"Real-time robotics, mobile platforms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Faster R-CNN"})}),(0,t.jsx)(n.td,{children:"5-15 FPS"}),(0,t.jsx)(n.td,{children:"60-70%"}),(0,t.jsx)(n.td,{children:"High accuracy when speed is not critical"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"SSD"})}),(0,t.jsx)(n.td,{children:"20-40 FPS"}),(0,t.jsx)(n.td,{children:"45-50%"}),(0,t.jsx)(n.td,{children:"Balanced speed/accuracy"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"EfficientDet"})}),(0,t.jsx)(n.td,{children:"15-30 FPS"}),(0,t.jsx)(n.td,{children:"55-60%"}),(0,t.jsx)(n.td,{children:"Efficient for edge devices (Jetson)"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For this course"}),": We focus on ",(0,t.jsx)(n.strong,{children:"YOLO v8"})," (fastest, good enough accuracy for robotics)"]}),"\n",(0,t.jsx)(n.h3,{id:"how-yolo-works-simplified",children:"How YOLO Works (Simplified)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"YOLO (You Only Look Once)"})," is a single-stage detector:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Input Image (640x640)\n    \u2193\nBackbone Network (CSPDarknet)  \u2190 Extract features\n    \u2193\nNeck (PANet)  \u2190 Fuse multi-scale features\n    \u2193\nHead (Detection Layers)  \u2190 Predict boxes + classes\n    \u2193\n[Bounding Boxes + Class Probabilities]\n    \u2193\nNon-Maximum Suppression (NMS)  \u2190 Remove duplicate detections\n    \u2193\nFinal Detections\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Insight"}),": YOLO divides the image into a grid and predicts boxes directly, making it much faster than two-stage detectors (Faster R-CNN)."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-2-isaac-ros-dnn-inference-setup",children:"Part 2: Isaac ROS DNN Inference Setup"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS provides GPU-accelerated inference using NVIDIA TensorRT."}),"\n",(0,t.jsx)(n.h3,{id:"step-1-install-isaac-ros-dnn-inference",children:"Step 1: Install Isaac ROS DNN Inference"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Navigate to workspace\ncd ~/ros2_ws/src\n\n# Clone Isaac ROS DNN Inference\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_dnn_inference.git\n\n# Clone Isaac ROS Object Detection\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_object_detection.git\n\n# Install dependencies\ncd ~/ros2_ws\nrosdep install --from-paths src --ignore-src -r -y\n\n# Build\ncolcon build --packages-select isaac_ros_dnn_inference isaac_ros_yolov8\n\n# Source\nsource install/setup.bash\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-download-pre-trained-yolo-v8-model",children:"Step 2: Download Pre-trained YOLO v8 Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Create model directory\nmkdir -p ~/models/yolov8\n\n# Download YOLOv8 nano (smallest, fastest)\ncd ~/models/yolov8\nwget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt\n\n# Convert to TensorRT engine (GPU optimized)\npython3 -c \"\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')\nmodel.export(format='engine', device=0, half=True)  # FP16 precision\n\"\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Model Variants"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"yolov8n.pt"})," - Nano (fastest, 6MB)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"yolov8s.pt"})," - Small (22MB)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"yolov8m.pt"})," - Medium (50MB)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"yolov8l.pt"})," - Large (100MB)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For ",(0,t.jsx)(n.strong,{children:"real-time robotics"}),", use Nano or Small."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-3-running-object-detection-in-isaac-sim",children:"Part 3: Running Object Detection in Isaac Sim"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-setup-scene-with-objects",children:"Step 1: Setup Scene with Objects"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Launch Isaac Sim"}),"\n",(0,t.jsxs)(n.li,{children:["Load warehouse scene: ",(0,t.jsx)(n.code,{children:"NVIDIA/Assets/Isaac/Environments/Simple_Warehouse/warehouse.usd"})]}),"\n",(0,t.jsx)(n.li,{children:"Add Carter robot with camera (from VSLAM lesson)"}),"\n",(0,t.jsxs)(n.li,{children:["Place various objects in view:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Boxes (class: "box")'}),"\n",(0,t.jsx)(n.li,{children:'Person model (class: "person")'}),"\n",(0,t.jsx)(n.li,{children:'Chair (class: "chair")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-2-enable-ros-2-camera-bridge",children:"Step 2: Enable ROS 2 Camera Bridge"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac Utils"})," > ",(0,t.jsx)(n.strong,{children:"ROS 2 Bridge"})]}),"\n",(0,t.jsxs)(n.li,{children:["Enable topic: ",(0,t.jsx)(n.code,{children:"/front_camera/image_raw"})," (RGB camera)"]}),"\n",(0,t.jsxs)(n.li,{children:["Enable topic: ",(0,t.jsx)(n.code,{children:"/front_camera/camera_info"})," (calibration)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-3-launch-isaac-ros-object-detection",children:"Step 3: Launch Isaac ROS Object Detection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch detection node\nros2 launch isaac_ros_yolov8 isaac_ros_yolov8.launch.py \\\n  model_file_path:=~/models/yolov8/yolov8n.engine \\\n  input_image_topic:=/front_camera/image_raw \\\n  output_topic:=/detections\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[isaac_ros_yolov8]: Loading model from ~/models/yolov8/yolov8n.engine\n[isaac_ros_yolov8]: TensorRT engine loaded successfully\n[isaac_ros_yolov8]: Inference node ready, waiting for images...\n[isaac_ros_yolov8]: Processing images at 78.3 FPS\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-visualize-detections-in-rviz",children:"Step 4: Visualize Detections in RViz"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 2: Launch RViz\nrviz2\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Add visualizations"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Add"})," > ",(0,t.jsx)(n.strong,{children:"Image"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Topic: ",(0,t.jsx)(n.code,{children:"/front_camera/image_raw"})," (raw camera feed)"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Add"})," > ",(0,t.jsx)(n.strong,{children:"Detection2DArray"})," (custom RViz plugin for bounding boxes)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Topic: ",(0,t.jsx)(n.code,{children:"/detections"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"You should see bounding boxes drawn on the camera image with class labels."}),"\n",(0,t.jsx)(n.h3,{id:"step-5-inspect-detection-messages",children:"Step 5: Inspect Detection Messages"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 3: Echo detections\nros2 topic echo /detections\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output Example"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'detections:\n  - results:\n      - id: 0\n        score: 0.94\n        class_id: 39  # "box" in COCO dataset\n    bbox:\n      center:\n        x: 370.5\n        y: 230.0\n      size_x: 141.0\n      size_y: 100.0\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-4-understanding-detection-output",children:"Part 4: Understanding Detection Output"}),"\n",(0,t.jsx)(n.h3,{id:"detection-message-structure",children:"Detection Message Structure"}),"\n",(0,t.jsxs)(n.p,{children:["Isaac ROS uses ",(0,t.jsx)(n.code,{children:"vision_msgs/Detection2DArray"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Detection2DArray\n\u251c\u2500\u2500 header (timestamp, frame_id)\n\u2514\u2500\u2500 detections[] (list of detected objects)\n    \u251c\u2500\u2500 results[]\n    \u2502   \u251c\u2500\u2500 id (class ID, e.g., 0=person, 39=box)\n    \u2502   \u2514\u2500\u2500 score (confidence, 0-1)\n    \u2514\u2500\u2500 bbox (bounding box)\n        \u251c\u2500\u2500 center (x, y in pixels)\n        \u2514\u2500\u2500 size (width, height in pixels)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"filtering-by-confidence-threshold",children:"Filtering by Confidence Threshold"}),"\n",(0,t.jsx)(n.p,{children:"Ignore low-confidence detections:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom vision_msgs.msg import Detection2DArray\n\nclass DetectionFilter:\n    def __init__(self):\n        self.node = rclpy.create_node('detection_filter')\n        self.sub = self.node.create_subscription(\n            Detection2DArray,\n            '/detections',\n            self.callback,\n            10\n        )\n        self.pub = self.node.create_publisher(\n            Detection2DArray,\n            '/detections_filtered',\n            10\n        )\n\n    def callback(self, msg):\n        filtered = Detection2DArray()\n        filtered.header = msg.header\n\n        for det in msg.detections:\n            if det.results[0].score > 0.7:  # 70% confidence threshold\n                filtered.detections.append(det)\n\n        self.pub.publish(filtered)\n        print(f\"Filtered: {len(msg.detections)} \u2192 {len(filtered.detections)} detections\")\n\n# Usage\nrclpy.init()\nfilter_node = DetectionFilter()\nrclpy.spin(filter_node.node)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-5-training-a-custom-object-detector",children:"Part 5: Training a Custom Object Detector"}),"\n",(0,t.jsx)(n.p,{children:"Pre-trained models (like YOLO v8 on COCO dataset) detect 80 common classes. For custom objects (e.g., specific tools, parts, robots), train your own detector."}),"\n",(0,t.jsx)(n.h3,{id:"step-1-prepare-synthetic-dataset",children:"Step 1: Prepare Synthetic Dataset"}),"\n",(0,t.jsx)(n.p,{children:"Use the dataset generated in Chapter 7 (Synthetic Data Generation):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"~/synthetic_data/tabletop_dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 rgb_0000.png\n\u2502   \u251c\u2500\u2500 rgb_0001.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 0000.txt  # YOLO format labels\n\u2502   \u251c\u2500\u2500 0001.txt\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 dataset.yaml\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"YOLO Label Format"})," (per line in ",(0,t.jsx)(n.code,{children:".txt"})," file):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"<class_id> <x_center> <y_center> <width> <height>\n"})}),"\n",(0,t.jsx)(n.p,{children:"All values normalized to 0-1 range."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example"})," (",(0,t.jsx)(n.code,{children:"0000.txt"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"0 0.45 0.35 0.12 0.08  # Apple at center (0.45, 0.35), size 12% x 8%\n1 0.67 0.42 0.10 0.15  # Mug at (0.67, 0.42)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-convert-coco-annotations-to-yolo-format",children:"Step 2: Convert COCO Annotations to YOLO Format"}),"\n",(0,t.jsx)(n.p,{children:"If you generated data in COCO format (from Chapter 7), convert it:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import json\nfrom pathlib import Path\n\n# Load COCO annotations\nwith open('annotations.json') as f:\n    coco = json.load(f)\n\n# Create class mapping\nclass_map = {cat['id']: cat['name'] for cat in coco['categories']}\n\n# Convert each annotation\nfor ann in coco['annotations']:\n    image_id = ann['image_id']\n    class_id = ann['category_id']\n    bbox = ann['bbox']  # [x, y, width, height] in pixels\n\n    # Get image dimensions\n    img_info = next(img for img in coco['images'] if img['id'] == image_id)\n    img_w, img_h = img_info['width'], img_info['height']\n\n    # Normalize to 0-1\n    x_center = (bbox[0] + bbox[2] / 2) / img_w\n    y_center = (bbox[1] + bbox[3] / 2) / img_h\n    width = bbox[2] / img_w\n    height = bbox[3] / img_h\n\n    # Write YOLO label file\n    label_file = f\"labels/{image_id:04d}.txt\"\n    with open(label_file, 'a') as f:\n        f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-create-dataset-configuration",children:"Step 3: Create Dataset Configuration"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"dataset.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Dataset paths\ntrain: ~/synthetic_data/tabletop_dataset/train/images\nval: ~/synthetic_data/tabletop_dataset/val/images\n\n# Class names\nnc: 6  # Number of classes\nnames: ['apple', 'orange', 'mug', 'plate', 'spoon', 'bowl']\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-train-yolov8",children:"Step 4: Train YOLOv8"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\n# Load pre-trained model (transfer learning)\nmodel = YOLO('yolov8n.pt')\n\n# Train on custom dataset\nresults = model.train(\n    data='dataset.yaml',\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    device=0,  # GPU 0\n    project='tabletop_detector',\n    name='yolov8_custom'\n)\n\n# Export trained model to TensorRT\nmodel.export(format='engine', device=0, half=True)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Training Time"}),": 1-2 hours for 100 epochs on 2000 images (RTX 4070 Ti)"]}),"\n",(0,t.jsx)(n.h3,{id:"step-5-deploy-custom-model",children:"Step 5: Deploy Custom Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Use your custom model instead of pre-trained\nros2 launch isaac_ros_yolov8 isaac_ros_yolov8.launch.py \\\n  model_file_path:=~/tabletop_detector/yolov8_custom/weights/best.engine \\\n  input_image_topic:=/front_camera/image_raw \\\n  output_topic:=/custom_detections\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-6-integrating-detection-with-robot-actions",children:"Part 6: Integrating Detection with Robot Actions"}),"\n",(0,t.jsx)(n.h3,{id:"use-case-navigate-to-detected-object",children:"Use Case: Navigate to Detected Object"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),': Robot detects a "box" and drives toward it.']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Python Script"})," (",(0,t.jsx)(n.code,{children:"drive_to_detected_box.py"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import Twist\nimport math\n\nclass DriveToObject(Node):\n    def __init__(self):\n        super().__init__(\'drive_to_object\')\n\n        # Subscribe to detections\n        self.sub = self.create_subscription(\n            Detection2DArray,\n            \'/detections\',\n            self.detection_callback,\n            10\n        )\n\n        # Publish velocity commands\n        self.pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        self.target_class = "box"  # Change to your target\n\n    def detection_callback(self, msg):\n        # Find target object\n        target_detection = None\n        for det in msg.detections:\n            class_id = det.results[0].id\n            class_name = self.get_class_name(class_id)  # Map ID to name\n\n            if class_name == self.target_class and det.results[0].score > 0.7:\n                target_detection = det\n                break\n\n        if target_detection is None:\n            self.get_logger().info(f"No {self.target_class} detected")\n            return\n\n        # Calculate error from image center\n        bbox_center_x = target_detection.bbox.center.x\n        image_width = 1280  # Camera resolution\n\n        error = bbox_center_x - (image_width / 2)  # Pixels from center\n\n        # Proportional controller\n        Kp = 0.002  # Tuning parameter\n        angular_velocity = -Kp * error  # Negative to turn toward target\n\n        # Drive forward while correcting angle\n        cmd = Twist()\n        cmd.linear.x = 0.3  # Forward speed (m/s)\n        cmd.angular.z = angular_velocity  # Turn to center object\n\n        self.pub.publish(cmd)\n        self.get_logger().info(f"Driving toward {self.target_class}, error: {error:.1f}px")\n\n    def get_class_name(self, class_id):\n        # COCO class mapping (simplified)\n        coco_classes = {0: "person", 39: "box", 41: "cup", ...}\n        return coco_classes.get(class_id, "unknown")\n\n# Run node\nrclpy.init()\nnode = DriveToObject()\nrclpy.spin(node)\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Result"}),': Robot turns to center the "box" in view and drives toward it.']}),"\n",(0,t.jsx)(n.h3,{id:"use-case-grasp-detected-object",children:"Use Case: Grasp Detected Object"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Robot arm reaches toward detected object."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"3D position (from depth camera + detection)"}),"\n",(0,t.jsx)(n.li,{children:"Inverse kinematics (calculate joint angles to reach position)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"(We'll cover this in detail in Module 4: Humanoid Control)"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-7-performance-optimization",children:"Part 7: Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"metric-1-inference-fps",children:"Metric 1: Inference FPS"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": 30+ FPS for real-time robotic control"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Check FPS"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 topic hz /detections\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"average rate: 78.3\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"If FPS is low (<20)"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Use smaller model"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"yolov8n"})," instead of ",(0,t.jsx)(n.code,{children:"yolov8l"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reduce input resolution"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Train/run at 416x416 instead of 640x640"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Use FP16 precision"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Already enabled with ",(0,t.jsx)(n.code,{children:"half=True"})," in TensorRT export"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Batch inference"})," (if processing multiple cameras):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Combine images into batches for GPU efficiency"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"metric-2-detection-accuracy-map",children:"Metric 2: Detection Accuracy (mAP)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Mean Average Precision (mAP)"})," measures detection quality."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Evaluate on test set"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\nmodel = YOLO('~/models/yolov8/best.engine')\nmetrics = model.val(data='dataset.yaml')\n\nprint(f\"mAP@0.5: {metrics.box.map50:.3f}\")  # Typical: 0.70-0.90 for good models\nprint(f\"mAP@0.5:0.95: {metrics.box.map:.3f}\")  # Stricter: 0.50-0.70\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"If accuracy is low (<0.5 mAP)"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"More training data"}),": Generate 5000+ images with domain randomization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"More training epochs"}),": Train for 200-300 epochs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data augmentation"}),": Flips, rotations, color jitter (YOLO does this automatically)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Check label quality"}),": Manually review 50 random annotations for errors"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-8-troubleshooting-common-issues",children:"Part 8: Troubleshooting Common Issues"}),"\n",(0,t.jsx)(n.h3,{id:"issue-1-no-detections-output",children:"Issue 1: No Detections Output"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptom"}),": ",(0,t.jsx)(n.code,{children:"/detections"})," topic has no messages"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Check image topic"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 topic list | grep image\nros2 topic hz /front_camera/image_raw  # Should be 30+ Hz\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Verify model loaded"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Check terminal output for "TensorRT engine loaded successfully"'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Check topic remapping"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Ensure ",(0,t.jsx)(n.code,{children:"input_image_topic"})," matches your camera topic"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"issue-2-low-confidence-scores",children:"Issue 2: Low Confidence Scores"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptom"}),": All detections have score < 0.3"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Domain gap"}),": Model trained on real images, testing on simulation"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Fine-tune on synthetic data (100-500 images)"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Wrong class labels"}),": Model expects different classes"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Verify class IDs match between model and scene"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Poor image quality"}),": Dark, blurry, or low resolution"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Improve lighting in Isaac Sim, increase camera resolution"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"issue-3-duplicate-detections",children:"Issue 3: Duplicate Detections"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptom"}),": Multiple boxes for the same object"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Cause"}),": Non-Maximum Suppression (NMS) threshold too high"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Lower NMS threshold in model config:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"model = YOLO('yolov8n.pt')\nmodel.export(format='engine', device=0, half=True, nms_threshold=0.3)  # Default: 0.45\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-9-hands-on-exercise",children:"Part 9: Hands-On Exercise"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-build-a-fruit-picker-robot",children:'Exercise: Build a "Fruit Picker" Robot'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Detect and navigate to fruits (apples, oranges) on a tabletop."]}),"\n",(0,t.jsx)(n.h4,{id:"requirements",children:"Requirements"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scene Setup"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Tabletop with 10 fruits (5 apples, 5 oranges)"}),"\n",(0,t.jsx)(n.li,{children:"Carter robot with camera facing tabletop"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Train custom YOLOv8 model on synthetic fruit dataset"}),"\n",(0,t.jsx)(n.li,{children:'Classes: ["apple", "orange"]'}),"\n",(0,t.jsx)(n.li,{children:"Inference at 30+ FPS"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robot Control"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Implement ",(0,t.jsx)(n.code,{children:"drive_to_fruit.py"})," script"]}),"\n",(0,t.jsx)(n.li,{children:'Robot detects nearest "apple"'}),"\n",(0,t.jsx)(n.li,{children:"Drives toward apple until distance < 0.5 meters"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Validation"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Detection accuracy: mAP > 0.80"}),"\n",(0,t.jsx)(n.li,{children:"Robot successfully navigates to 8/10 apples"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"deliverables",children:"Deliverables"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Trained model: ",(0,t.jsx)(n.code,{children:"fruit_detector_yolov8.engine"})]}),"\n",(0,t.jsxs)(n.li,{children:["Control script: ",(0,t.jsx)(n.code,{children:"drive_to_fruit.py"})]}),"\n",(0,t.jsxs)(n.li,{children:["Video recording showing:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Live detection (bounding boxes on fruits)"}),"\n",(0,t.jsx)(n.li,{children:"Robot driving toward apple"}),"\n",(0,t.jsx)(n.li,{children:"Successful approach (<0.5m from target)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Estimated Time"}),": 2-3 hours"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"In this lesson, you learned:"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Object detection fundamentals (YOLO architecture)\n\u2705 Installing and running Isaac ROS DNN Inference\n\u2705 Real-time object detection on camera streams (30+ FPS)\n\u2705 Training custom detectors on synthetic data\n\u2705 Integrating detection with robot control (navigation, grasping)\n\u2705 Performance optimization (FPS, accuracy tuning)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"YOLO"})," provides real-time detection suitable for robotics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS"})," enables GPU-accelerated inference with TensorRT"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthetic data"})," from Chapter 7 can train production-quality detectors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Detection \u2192 Action"})," integration is straightforward with ROS 2 topics"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.p,{children:["In the next lesson (",(0,t.jsx)(n.strong,{children:"Depth Estimation"}),"), you'll learn:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Stereo depth estimation from camera pairs"}),"\n",(0,t.jsx)(n.li,{children:"Monocular depth estimation with deep learning"}),"\n",(0,t.jsx)(n.li,{children:"3D point cloud generation from RGB-D data"}),"\n",(0,t.jsx)(n.li,{children:"Combining detection + depth for 3D object localization"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"YOLOv8 Documentation"}),": ",(0,t.jsx)(n.a,{href:"https://docs.ultralytics.com/",children:"https://docs.ultralytics.com/"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Object Detection"}),": ",(0,t.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_object_detection/index.html",children:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_object_detection/index.html"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TensorRT Optimization Guide"}),": ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html",children:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"COCO Dataset"}),": ",(0,t.jsx)(n.a,{href:"https://cocodataset.org/",children:"https://cocodataset.org/"})," (80 common object classes)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection Papers"}),": ",(0,t.jsx)(n.a,{href:"https://paperswithcode.com/task/object-detection",children:"https://paperswithcode.com/task/object-detection"})]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{title:"Production Robotics",type:"tip",children:(0,t.jsx)(n.p,{children:"Amazon warehouse robots use object detection to identify packages, bins, and shelves for autonomous picking and sorting. The same techniques work for humanoid manipulation tasks!"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>c,x:()=>o});var i=s(6540);const t={},r=i.createContext(t);function c(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);