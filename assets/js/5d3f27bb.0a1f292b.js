"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[7354],{3586:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-isaac/ch8-perception/pose-estimation","title":"6DOF Pose Estimation","description":"Overview","source":"@site/docs/module-3-isaac/ch8-perception/pose-estimation.md","sourceDirName":"module-3-isaac/ch8-perception","slug":"/module-3-isaac/ch8-perception/pose-estimation","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/pose-estimation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ch8-perception/pose-estimation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Depth Estimation","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/depth-estimation"},"next":{"title":"Map Building with SLAM","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch9-navigation/map-building"}}');var o=s(4848),t=s(8453);const r={},a="6DOF Pose Estimation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Part 1: 6DOF Pose Fundamentals",id:"part-1-6dof-pose-fundamentals",level:2},{value:"What is 6DOF?",id:"what-is-6dof",level:3},{value:"Why Pose Estimation is Hard",id:"why-pose-estimation-is-hard",level:3},{value:"Part 2: DOPE (Deep Object Pose Estimation)",id:"part-2-dope-deep-object-pose-estimation",level:2},{value:"Step 1: Install Isaac ROS DOPE",id:"step-1-install-isaac-ros-dope",level:3},{value:"Step 2: Download Pre-trained DOPE Models",id:"step-2-download-pre-trained-dope-models",level:3},{value:"Step 3: Configure DOPE Parameters",id:"step-3-configure-dope-parameters",level:3},{value:"Step 4: Launch DOPE Node",id:"step-4-launch-dope-node",level:3},{value:"Step 5: Visualize Pose in RViz",id:"step-5-visualize-pose-in-rviz",level:3},{value:"Part 3: Understanding Pose Output",id:"part-3-understanding-pose-output",level:2},{value:"DOPE Pose Message",id:"dope-pose-message",level:3},{value:"Converting Quaternion to Euler Angles (for Debugging)",id:"converting-quaternion-to-euler-angles-for-debugging",level:3},{value:"Part 4: Training Custom DOPE Models",id:"part-4-training-custom-dope-models",level:2},{value:"Step 1: Generate Synthetic Training Data",id:"step-1-generate-synthetic-training-data",level:3},{value:"Step 2: Train DOPE Model",id:"step-2-train-dope-model",level:3},{value:"Part 5: Grasp Pose Generation",id:"part-5-grasp-pose-generation",level:2},{value:"Step 1: Define Gripper Parameters",id:"step-1-define-gripper-parameters",level:3},{value:"Step 2: Compute Grasp Pose from Object Pose",id:"step-2-compute-grasp-pose-from-object-pose",level:3},{value:"Step 3: Publish Grasp Pose for Motion Planning",id:"step-3-publish-grasp-pose-for-motion-planning",level:3},{value:"Part 6: Pose Tracking",id:"part-6-pose-tracking",level:2},{value:"Using Kalman Filter for Pose Smoothing",id:"using-kalman-filter-for-pose-smoothing",level:3},{value:"Part 7: Hands-On Exercise",id:"part-7-hands-on-exercise",level:2},{value:"Exercise: Pick-and-Place with 6DOF Pose Estimation",id:"exercise-pick-and-place-with-6dof-pose-estimation",level:3},{value:"Requirements",id:"requirements",level:4},{value:"Deliverables",id:"deliverables",level:4},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"6dof-pose-estimation",children:"6DOF Pose Estimation"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"6DOF (Six Degrees of Freedom) pose estimation"})," determines an object's complete position and orientation in 3D space. This is essential for robotic manipulation - to grasp an object, the robot needs to know not just ",(0,o.jsx)(n.em,{children:"where"})," it is (3D position), but also ",(0,o.jsx)(n.em,{children:"how it's oriented"})," (3D rotation). This enables precise pick-and-place, assembly, and manipulation tasks."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand 6DOF pose representation (position + rotation)"}),"\n",(0,o.jsx)(n.li,{children:"Run pose estimation networks (DOPE, FoundationPose)"}),"\n",(0,o.jsx)(n.li,{children:"Estimate poses from RGB-D data"}),"\n",(0,o.jsx)(n.li,{children:"Generate grasp poses for manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Track object poses across frames"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Prerequisites"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Completed: Depth Estimation (previous lesson)"}),"\n",(0,o.jsx)(n.li,{children:"Understanding of 3D transformations (rotation matrices, quaternions)"}),"\n",(0,o.jsx)(n.li,{children:"Object detection and depth sensing from previous lessons"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Estimated Time"}),": 3-4 hours"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Represent 6DOF poses (translation vectors and rotation quaternions)"}),"\n",(0,o.jsx)(n.li,{children:"Run DOPE (Deep Object Pose Estimation) in Isaac ROS"}),"\n",(0,o.jsx)(n.li,{children:"Estimate object poses from RGB-D images"}),"\n",(0,o.jsx)(n.li,{children:"Generate viable grasp poses for robot manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Track object poses over time for dynamic grasping"}),"\n",(0,o.jsx)(n.li,{children:"Integrate pose estimation with robot planning"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"part-1-6dof-pose-fundamentals",children:"Part 1: 6DOF Pose Fundamentals"}),"\n",(0,o.jsx)(n.h3,{id:"what-is-6dof",children:"What is 6DOF?"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"6 Degrees of Freedom"})," = 3 position coordinates + 3 rotation angles"]}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"DOF"}),(0,o.jsx)(n.th,{children:"Description"}),(0,o.jsx)(n.th,{children:"Range"}),(0,o.jsx)(n.th,{children:"Example"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"X"})}),(0,o.jsx)(n.td,{children:"Position along X-axis"}),(0,o.jsx)(n.td,{children:"\xb1\u221e meters"}),(0,o.jsx)(n.td,{children:"0.5m (right of camera)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Y"})}),(0,o.jsx)(n.td,{children:"Position along Y-axis"}),(0,o.jsx)(n.td,{children:"\xb1\u221e meters"}),(0,o.jsx)(n.td,{children:"-0.2m (below camera)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Z"})}),(0,o.jsx)(n.td,{children:"Position along Z-axis"}),(0,o.jsx)(n.td,{children:"\xb1\u221e meters"}),(0,o.jsx)(n.td,{children:"1.2m (forward from camera)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Roll"})}),(0,o.jsx)(n.td,{children:"Rotation around X-axis"}),(0,o.jsx)(n.td,{children:"0-360\xb0"}),(0,o.jsx)(n.td,{children:"15\xb0 (tilted right)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Pitch"})}),(0,o.jsx)(n.td,{children:"Rotation around Y-axis"}),(0,o.jsx)(n.td,{children:"0-360\xb0"}),(0,o.jsx)(n.td,{children:"-30\xb0 (tilted down)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Yaw"})}),(0,o.jsx)(n.td,{children:"Rotation around Z-axis"}),(0,o.jsx)(n.td,{children:"0-360\xb0"}),(0,o.jsx)(n.td,{children:"90\xb0 (facing right)"})]})]})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 Representation"})," (",(0,o.jsx)(n.code,{children:"geometry_msgs/Pose"}),"):"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"position:\n  x: 0.5\n  y: -0.2\n  z: 1.2\norientation:  # Quaternion (x, y, z, w)\n  x: 0.130\n  y: -0.259\n  z: 0.389\n  w: 0.909\n"})}),"\n",(0,o.jsx)(n.admonition,{title:"Why Quaternions?",type:"tip",children:(0,o.jsx)(n.p,{children:'Quaternions avoid "gimbal lock" (mathematical singularities) that occur with Euler angles. They\'re the standard for 3D rotation in robotics.'})}),"\n",(0,o.jsx)(n.h3,{id:"why-pose-estimation-is-hard",children:"Why Pose Estimation is Hard"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Challenges"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ambiguity"}),": Symmetric objects (spheres, cylinders) have infinite valid orientations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Occlusion"}),": Objects partially hidden behind others"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Lighting"}),": Shiny surfaces create reflections, textures change appearance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scale"}),": Similar-looking objects at different distances"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Use ",(0,o.jsx)(n.strong,{children:"depth data"})," (RGB-D) to resolve scale ambiguity"]}),"\n",(0,o.jsxs)(n.li,{children:["Use ",(0,o.jsx)(n.strong,{children:"learning-based methods"})," (neural networks) to handle lighting/texture variation"]}),"\n",(0,o.jsxs)(n.li,{children:["Use ",(0,o.jsx)(n.strong,{children:"tracking"})," to maintain pose across frames even with occlusion"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"part-2-dope-deep-object-pose-estimation",children:"Part 2: DOPE (Deep Object Pose Estimation)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"DOPE"})," is NVIDIA's pose estimation network optimized for Isaac ROS."]}),"\n",(0,o.jsx)(n.h3,{id:"step-1-install-isaac-ros-dope",children:"Step 1: Install Isaac ROS DOPE"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_pose_estimation.git\ncd ~/ros2_ws\nrosdep install --from-paths src --ignore-src -r -y\ncolcon build --packages-select isaac_ros_dope\nsource install/setup.bash\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-2-download-pre-trained-dope-models",children:"Step 2: Download Pre-trained DOPE Models"}),"\n",(0,o.jsx)(n.p,{children:"DOPE provides models trained on common objects (YCB dataset):"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/models/dope\ncd ~/models/dope\n\n# Download cracker box model (example)\nwget https://nvidia-isaac-ros.github.io/models/Cracker.pth\n\n# Available models:\n# - Cracker.pth (cracker box)\n# - Soup.pth (soup can)\n# - Mustard.pth (mustard bottle)\n# - TomatoSoup.pth (tomato soup can)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-3-configure-dope-parameters",children:"Step 3: Configure DOPE Parameters"}),"\n",(0,o.jsxs)(n.p,{children:["Create ",(0,o.jsx)(n.code,{children:"dope_config.yaml"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'dope:\n  ros__parameters:\n    # Model configuration\n    model_file_path: "/home/user/models/dope/Cracker.pth"\n    object_name: "cracker_box"\n\n    # Detection threshold\n    detection_threshold: 0.5  # Confidence threshold\n\n    # Camera intrinsics (from camera_info topic)\n    camera_info_topic: "/front_camera/camera_info"\n\n    # Input topics\n    image_topic: "/front_camera/image_raw"\n\n    # Output topics\n    pose_topic: "/dope/pose"\n    marker_topic: "/dope/markers"  # For RViz visualization\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-4-launch-dope-node",children:"Step 4: Launch DOPE Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_ros_dope isaac_ros_dope.launch.py config_file:=dope_config.yaml\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"[isaac_ros_dope]: Model loaded: Cracker.pth\n[isaac_ros_dope]: Listening for images on /front_camera/image_raw\n[isaac_ros_dope]: Publishing poses to /dope/pose\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-5-visualize-pose-in-rviz",children:"Step 5: Visualize Pose in RViz"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"rviz2\n"})}),"\n",(0,o.jsx)(n.p,{children:"Add displays:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Image"})," \u2192 ",(0,o.jsx)(n.code,{children:"/front_camera/image_raw"})," (camera view)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Marker"})," \u2192 ",(0,o.jsx)(n.code,{children:"/dope/markers"})," (shows object pose as 3D axes)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"TF"})," \u2192 Shows coordinate frames"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"You should see:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Camera image with detected object"}),"\n",(0,o.jsx)(n.li,{children:"3D coordinate axes overlaid on object (X=red, Y=green, Z=blue)"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"part-3-understanding-pose-output",children:"Part 3: Understanding Pose Output"}),"\n",(0,o.jsx)(n.h3,{id:"dope-pose-message",children:"DOPE Pose Message"}),"\n",(0,o.jsxs)(n.p,{children:["Topic: ",(0,o.jsx)(n.code,{children:"/dope/pose"}),"\nType: ",(0,o.jsx)(n.code,{children:"geometry_msgs/PoseStamped"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'header:\n  stamp: {sec: 1234567890, nanosec: 123456789}\n  frame_id: "camera_color_optical_frame"\npose:\n  position:\n    x: 0.45  # Meters from camera\n    y: -0.12\n    z: 0.82\n  orientation:  # Quaternion\n    x: 0.013\n    y: 0.987\n    z: -0.159\n    w: 0.019\n'})}),"\n",(0,o.jsx)(n.h3,{id:"converting-quaternion-to-euler-angles-for-debugging",children:"Converting Quaternion to Euler Angles (for Debugging)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import math\n\ndef quaternion_to_euler(x, y, z, w):\n    """\n    Convert quaternion to roll, pitch, yaw (radians).\n    """\n    # Roll (X-axis rotation)\n    sinr_cosp = 2 * (w * x + y * z)\n    cosr_cosp = 1 - 2 * (x * x + y * y)\n    roll = math.atan2(sinr_cosp, cosr_cosp)\n\n    # Pitch (Y-axis rotation)\n    sinp = 2 * (w * y - z * x)\n    if abs(sinp) >= 1:\n        pitch = math.copysign(math.pi / 2, sinp)  # Use 90\xb0 if out of range\n    else:\n        pitch = math.asin(sinp)\n\n    # Yaw (Z-axis rotation)\n    siny_cosp = 2 * (w * z + x * y)\n    cosy_cosp = 1 - 2 * (y * y + z * z)\n    yaw = math.atan2(siny_cosp, cosy_cosp)\n\n    return roll, pitch, yaw\n\n# Usage\nroll, pitch, yaw = quaternion_to_euler(0.013, 0.987, -0.159, 0.019)\nprint(f"Roll: {math.degrees(roll):.1f}\xb0")\nprint(f"Pitch: {math.degrees(pitch):.1f}\xb0")\nprint(f"Yaw: {math.degrees(yaw):.1f}\xb0")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"part-4-training-custom-dope-models",children:"Part 4: Training Custom DOPE Models"}),"\n",(0,o.jsx)(n.p,{children:"For custom objects (e.g., tools, parts specific to your application), train DOPE on synthetic data."}),"\n",(0,o.jsx)(n.h3,{id:"step-1-generate-synthetic-training-data",children:"Step 1: Generate Synthetic Training Data"}),"\n",(0,o.jsx)(n.p,{children:"Use Isaac Sim Replicator (from Chapter 7):"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\n\n# Load your custom object model\ncustom_object = rep.create.from_usd("path/to/your/object.usd")\n\n# Randomize pose\nwith rep.trigger.on_frame(num_frames=5000):\n    with custom_object:\n        rep.modify.pose(\n            position=rep.distribution.uniform((-0.3, -0.3, 0.5), (0.3, 0.3, 1.5)),\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n        )\n\n# Capture RGB + ground truth pose annotations\ncamera = rep.create.camera(position=(-1.0, 0, 0.8), look_at=(0, 0, 0.8))\ncamera.add_annotator(rep.AnnotatorRegistry.get_annotator("rgb"))\ncamera.add_annotator(rep.AnnotatorRegistry.get_annotator("6dof_pose"))\n\n# Output writer\nrep.WriterRegistry.get("BasicWriter").initialize(output_dir="/data/custom_object_poses")\nrep.orchestrator.run()\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Result"}),": 5000 images with ground truth 6DOF pose labels"]}),"\n",(0,o.jsx)(n.h3,{id:"step-2-train-dope-model",children:"Step 2: Train DOPE Model"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Clone DOPE training repository\ngit clone https://github.com/NVlabs/Deep_Object_Pose.git\ncd Deep_Object_Pose\n\n# Prepare dataset (convert to DOPE format)\npython scripts/prepare_dataset.py \\\n  --input /data/custom_object_poses \\\n  --output /data/dope_dataset\n\n# Train model\npython train.py \\\n  --data /data/dope_dataset \\\n  --object custom_tool \\\n  --epochs 60 \\\n  --outf /models/dope/custom_tool\n\n# Convert trained model to TensorRT (for Isaac ROS)\npython export_to_trt.py --model /models/dope/custom_tool/net_epoch_60.pth\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Training Time"}),": 2-4 hours on RTX 4070 Ti for 5000 images, 60 epochs"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"part-5-grasp-pose-generation",children:"Part 5: Grasp Pose Generation"}),"\n",(0,o.jsx)(n.p,{children:"Once you have an object's 6DOF pose, generate viable grasp poses for a robot gripper."}),"\n",(0,o.jsx)(n.h3,{id:"step-1-define-gripper-parameters",children:"Step 1: Define Gripper Parameters"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class GripperParams:\n    def __init__(self):\n        self.finger_length = 0.08  # 8cm gripper fingers\n        self.max_opening = 0.10    # 10cm max grasp width\n        self.approach_distance = 0.15  # 15cm pre-grasp distance\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-2-compute-grasp-pose-from-object-pose",children:"Step 2: Compute Grasp Pose from Object Pose"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\ndef generate_grasp_pose(object_pose, gripper_params):\n    """\n    Generate grasp pose from detected object pose.\n\n    Args:\n        object_pose: geometry_msgs/Pose of detected object\n        gripper_params: GripperParams instance\n\n    Returns:\n        geometry_msgs/Pose of grasp (gripper target pose)\n    """\n    # Extract object position and orientation\n    obj_pos = np.array([object_pose.position.x,\n                         object_pose.position.y,\n                        object_pose.position.z])\n\n    obj_quat = [object_pose.orientation.x,\n                object_pose.orientation.y,\n                object_pose.orientation.z,\n                object_pose.orientation.w]\n\n    obj_rot = R.from_quat(obj_quat)\n\n    # Grasp from top (approach along Z-axis)\n    approach_vector = np.array([0, 0, -1])  # Downward\n\n    # Apply object orientation to approach vector\n    rotated_approach = obj_rot.apply(approach_vector)\n\n    # Pre-grasp position (offset by approach distance)\n    pre_grasp_pos = obj_pos + rotated_approach * gripper_params.approach_distance\n\n    # Gripper orientation: align gripper Z-axis with approach vector\n    # (Implementation depends on gripper URDF frame definitions)\n    grasp_orientation = obj_quat  # Simplified: use object orientation\n\n    # Create grasp pose\n    grasp_pose = Pose()\n    grasp_pose.position.x = pre_grasp_pos[0]\n    grasp_pose.position.y = pre_grasp_pos[1]\n    grasp_pose.position.z = pre_grasp_pos[2]\n    grasp_pose.orientation = object_pose.orientation\n\n    return grasp_pose\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-3-publish-grasp-pose-for-motion-planning",children:"Step 3: Publish Grasp Pose for Motion Planning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class GraspPlanner:\n    def __init__(self):\n        self.node = rclpy.create_node('grasp_planner')\n\n        # Subscribe to object poses\n        self.sub = self.node.create_subscription(\n            PoseStamped,\n            '/dope/pose',\n            self.pose_callback,\n            10\n        )\n\n        # Publish grasp poses\n        self.pub = self.node.create_publisher(\n            PoseStamped,\n            '/grasp_target',\n            10\n        )\n\n        self.gripper = GripperParams()\n\n    def pose_callback(self, msg):\n        # Generate grasp pose\n        grasp_pose_msg = PoseStamped()\n        grasp_pose_msg.header = msg.header\n        grasp_pose_msg.pose = generate_grasp_pose(msg.pose, self.gripper)\n\n        # Publish for motion planner\n        self.pub.publish(grasp_pose_msg)\n        self.node.get_logger().info(f\"Generated grasp pose at {grasp_pose_msg.pose.position.z:.2f}m height\")\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"part-6-pose-tracking",children:"Part 6: Pose Tracking"}),"\n",(0,o.jsx)(n.p,{children:"For moving objects, track pose across frames for dynamic grasping."}),"\n",(0,o.jsx)(n.h3,{id:"using-kalman-filter-for-pose-smoothing",children:"Using Kalman Filter for Pose Smoothing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from filterpy.kalman import KalmanFilter\n\nclass PoseTracker:\n    def __init__(self):\n        self.kf = KalmanFilter(dim_x=7, dim_z=7)  # State: [x, y, z, qx, qy, qz, qw]\n\n        # State transition matrix (constant position model)\n        self.kf.F = np.eye(7)\n\n        # Measurement function (direct observation)\n        self.kf.H = np.eye(7)\n\n        # Process and measurement noise (tune these)\n        self.kf.Q *= 0.01  # Process noise\n        self.kf.R *= 0.1   # Measurement noise\n\n        # Initial state\n        self.kf.x = np.zeros(7)\n        self.kf.P *= 1000  # High initial uncertainty\n\n    def update(self, pose):\n        """Update filter with new pose measurement."""\n        measurement = np.array([\n            pose.position.x,\n            pose.position.y,\n            pose.position.z,\n            pose.orientation.x,\n            pose.orientation.y,\n            pose.orientation.z,\n            pose.orientation.w\n        ])\n\n        self.kf.predict()\n        self.kf.update(measurement)\n\n        # Return smoothed pose\n        smoothed_pose = Pose()\n        smoothed_pose.position.x = self.kf.x[0]\n        smoothed_pose.position.y = self.kf.x[1]\n        smoothed_pose.position.z = self.kf.x[2]\n        smoothed_pose.orientation.x = self.kf.x[3]\n        smoothed_pose.orientation.y = self.kf.x[4]\n        smoothed_pose.orientation.z = self.kf.x[5]\n        smoothed_pose.orientation.w = self.kf.x[6]\n\n        return smoothed_pose\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"part-7-hands-on-exercise",children:"Part 7: Hands-On Exercise"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-pick-and-place-with-6dof-pose-estimation",children:"Exercise: Pick-and-Place with 6DOF Pose Estimation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Objective"}),": Estimate object poses and execute pick-and-place operations in simulation."]}),"\n",(0,o.jsx)(n.h4,{id:"requirements",children:"Requirements"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Scene Setup"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Tabletop with 3 objects (e.g., cracker boxes)"}),"\n",(0,o.jsx)(n.li,{children:"Robot arm (Franka Emika) with gripper"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Pose Estimation"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Run DOPE on camera stream"}),"\n",(0,o.jsx)(n.li,{children:"Detect and estimate 6DOF pose of each object"}),"\n",(0,o.jsx)(n.li,{children:"Visualize poses in RViz (3D coordinate axes)"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grasp Planning"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Generate grasp poses for detected objects"}),"\n",(0,o.jsx)(n.li,{children:"Compute pre-grasp and grasp waypoints"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Execution"})," (manual for now - full automation in Module 4):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Display grasp target pose in RViz"}),"\n",(0,o.jsx)(n.li,{children:"Verify pose is reachable by robot arm (within workspace)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"deliverables",children:"Deliverables"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["DOPE configuration file: ",(0,o.jsx)(n.code,{children:"pick_place_dope.yaml"})]}),"\n",(0,o.jsxs)(n.li,{children:["Python script: ",(0,o.jsx)(n.code,{children:"grasp_pose_planner.py"})]}),"\n",(0,o.jsxs)(n.li,{children:["RViz recording showing:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Object detection and pose estimation"}),"\n",(0,o.jsx)(n.li,{children:"Generated grasp pose visualization"}),"\n",(0,o.jsx)(n.li,{children:"Coordinate frames (camera, object, grasp)"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"Accuracy test: Compare estimated poses to ground truth (error <5cm position, <10\xb0 orientation)"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Estimated Time"}),": 3 hours"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this lesson, you learned:"}),"\n",(0,o.jsx)(n.p,{children:"\u2705 6DOF pose representation (position + quaternion rotation)\n\u2705 Running DOPE for object pose estimation\n\u2705 Training custom DOPE models on synthetic data\n\u2705 Generating grasp poses from object poses\n\u2705 Pose tracking with Kalman filtering\n\u2705 Integrating pose estimation with robot planning"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"6DOF pose"})," is essential for manipulation (not just 3D position)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"DOPE"})," provides real-time pose estimation optimized for Isaac ROS"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Synthetic data"})," from Isaac Sim enables training on custom objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasp planning"})," requires object pose + gripper constraints"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Congratulations!"})," You've completed ",(0,o.jsx)(n.strong,{children:"Chapter 8: Perception with Isaac ROS"}),"! You can now:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Build maps with VSLAM"}),"\n",(0,o.jsx)(n.li,{children:"Detect objects with YOLO"}),"\n",(0,o.jsx)(n.li,{children:"Estimate depth and generate 3D point clouds"}),"\n",(0,o.jsx)(n.li,{children:"Estimate 6DOF poses for manipulation"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["In ",(0,o.jsx)(n.strong,{children:"Chapter 9: Autonomous Navigation (Nav2)"}),", you'll learn:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Map building with SLAM and map saving"}),"\n",(0,o.jsx)(n.li,{children:"Path planning algorithms (A*, DWA, TEB)"}),"\n",(0,o.jsx)(n.li,{children:"Obstacle avoidance with costmap layers"}),"\n",(0,o.jsx)(n.li,{children:"Behavior trees for navigation logic"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"DOPE Paper"}),": ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/1809.10790",children:"https://arxiv.org/abs/1809.10790"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS Pose Estimation"}),": ",(0,o.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_pose_estimation/index.html",children:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_pose_estimation/index.html"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Quaternion Math"}),": ",(0,o.jsx)(n.a,{href:"https://eater.net/quaternions",children:"https://eater.net/quaternions"})," (interactive visualization)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasp Planning"}),": ",(0,o.jsx)(n.a,{href:"https://manipulation.csail.mit.edu/",children:"https://manipulation.csail.mit.edu/"})," (MIT robotics course)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"YCB Object Dataset"}),": ",(0,o.jsx)(n.a,{href:"https://www.ycbbenchmarks.com/",children:"https://www.ycbbenchmarks.com/"})," (standard objects for benchmarking)"]}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{title:"Module Milestone!",type:"success",children:(0,o.jsx)(n.p,{children:"You've completed 8 perception lessons (VSLAM, object detection, depth, pose)! These are the foundational AI perception skills for autonomous robots. Next up: autonomous navigation!"})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const o={},t=i.createContext(o);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);