"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[3808],{7680:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/ch14-multimodal/human-robot-interaction","title":"Human-Robot Interaction Patterns","description":"Learning Objectives","source":"@site/docs/module-4-vla/ch14-multimodal/human-robot-interaction.md","sourceDirName":"module-4-vla/ch14-multimodal","slug":"/module-4-vla/ch14-multimodal/human-robot-interaction","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/human-robot-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/ch14-multimodal/human-robot-interaction.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Task Monitoring and Execution Feedback","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/task-monitoring"},"next":{"title":"Module 4 Assessment: Voice-Controlled Cognitive Robotics System","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/assessment"}}');var r=t(4848),i=t(8453);const a={},s="Human-Robot Interaction Patterns",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Conversational Patterns",id:"conversational-patterns",level:2},{value:"Summary",id:"summary",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"human-robot-interaction-patterns",children:"Human-Robot Interaction Patterns"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Implement conversational interaction patterns"}),"\n",(0,r.jsx)(n.li,{children:"Handle clarification dialogues"}),"\n",(0,r.jsx)(n.li,{children:"Provide natural language feedback"}),"\n",(0,r.jsx)(n.li,{children:"Build trust through transparent communication"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 2-3 hours"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"conversational-patterns",children:"Conversational Patterns"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ConversationalRobot:\n    def __init__(self):\n        self.context = []\n\n    def greet_user(self):\n        return "Hello! I\'m ready to help. What would you like me to do?"\n\n    def confirm_understanding(self, parsed_command):\n        return f"Just to confirm: you want me to {parsed_command}. Is that correct?"\n\n    def report_progress(self, action, progress):\n        if progress < 30:\n            return f"Starting to {action}..."\n        elif progress < 70:\n            return f"Halfway through {action}..."\n        else:\n            return f"Almost done with {action}..."\n\n    def report_success(self, action):\n        return f"\u2713 Successfully completed {action}!"\n\n    def report_failure(self, action, reason):\n        return f"Sorry, I couldn\'t {action} because {reason}. Would you like me to try something else?"\n\n    def ask_clarification(self, ambiguity):\n        return f"I\'m not sure about {ambiguity}. Could you please clarify?"\n\n# Usage\nrobot = ConversationalRobot()\n\nprint(robot.greet_user())\nprint(robot.confirm_understanding("pick up the red cup"))\nprint(robot.report_progress("picking up cup", 50))\nprint(robot.report_success("pick up the cup"))\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Module 4 (VLA) Complete!"})," You can now:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Capture audio with ReSpeaker"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Transcribe speech with Whisper (cloud + local)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Parse commands with NLU"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Plan with LLMs (GPT-4/Claude)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Execute actions via ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Control humanoid kinematics and locomotion"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Integrate vision-language-action pipeline"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": Module 5 covers the Capstone Project where you'll integrate all systems into an autonomous humanoid robot."]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var o=t(6540);const r={},i=o.createContext(r);function a(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);