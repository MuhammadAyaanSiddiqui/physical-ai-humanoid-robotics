"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[1802],{2994:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-isaac/ch7-isaac-sim/synthetic-data","title":"Synthetic Data Generation","description":"Overview","source":"@site/docs/module-3-isaac/ch7-isaac-sim/synthetic-data.md","sourceDirName":"module-3-isaac/ch7-isaac-sim","slug":"/module-3-isaac/ch7-isaac-sim/synthetic-data","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch7-isaac-sim/synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ch7-isaac-sim/synthetic-data.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Photorealistic Environments & Scene Creation","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch7-isaac-sim/photorealistic-environments"},"next":{"title":"Domain Randomization","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch7-isaac-sim/domain-randomization"}}');var a=t(4848),s=t(8453);const r={},o="Synthetic Data Generation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Part 1: Understanding Synthetic Data Types",id:"part-1-understanding-synthetic-data-types",level:2},{value:"Data Modality Overview",id:"data-modality-overview",level:3},{value:"Part 2: Setting Up a Camera Sensor",id:"part-2-setting-up-a-camera-sensor",level:2},{value:"Step 1: Create a Scene with Objects",id:"step-1-create-a-scene-with-objects",level:3},{value:"Step 2: Add a Camera via GUI",id:"step-2-add-a-camera-via-gui",level:3},{value:"Step 3: Enable Replicator (Isaac Sim&#39;s Data Generation Tool)",id:"step-3-enable-replicator-isaac-sims-data-generation-tool",level:3},{value:"Part 3: Capturing Data via Python API",id:"part-3-capturing-data-via-python-api",level:2},{value:"Step 1: Create a Data Capture Script",id:"step-1-create-a-data-capture-script",level:3},{value:"Step 2: Run the Script in Isaac Sim",id:"step-2-run-the-script-in-isaac-sim",level:3},{value:"Step 3: Capture a Single Frame",id:"step-3-capture-a-single-frame",level:3},{value:"Part 4: Understanding Depth and Segmentation Data",id:"part-4-understanding-depth-and-segmentation-data",level:2},{value:"Depth Maps",id:"depth-maps",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"Instance Segmentation",id:"instance-segmentation",level:3},{value:"Part 5: Annotating Objects for Detection",id:"part-5-annotating-objects-for-detection",level:2},{value:"Step 1: Define Semantic Classes",id:"step-1-define-semantic-classes",level:3},{value:"Step 2: Extract 2D Bounding Boxes",id:"step-2-extract-2d-bounding-boxes",level:3},{value:"Step 3: Visualize Bounding Boxes on Image",id:"step-3-visualize-bounding-boxes-on-image",level:3},{value:"Part 6: Generating Large-Scale Datasets",id:"part-6-generating-large-scale-datasets",level:2},{value:"Step 1: Randomize Camera Viewpoints",id:"step-1-randomize-camera-viewpoints",level:3},{value:"Step 2: Randomize Object Poses",id:"step-2-randomize-object-poses",level:3},{value:"Step 3: Randomize Lighting (covered in next lesson)",id:"step-3-randomize-lighting-covered-in-next-lesson",level:3},{value:"Part 7: Export Formats for AI Training",id:"part-7-export-formats-for-ai-training",level:2},{value:"COCO Format (Object Detection)",id:"coco-format-object-detection",level:3},{value:"KITTI Format (3D Object Detection)",id:"kitti-format-3d-object-detection",level:3},{value:"Custom Format",id:"custom-format",level:3},{value:"Part 8: Validating Synthetic Data Quality",id:"part-8-validating-synthetic-data-quality",level:2},{value:"Quality Checklist",id:"quality-checklist",level:3},{value:"Visualization Tools",id:"visualization-tools",level:3},{value:"Spot-Check Annotations",id:"spot-check-annotations",level:3},{value:"Part 9: Hands-On Exercise",id:"part-9-hands-on-exercise",level:2},{value:"Exercise: Generate a Tabletop Object Detection Dataset",id:"exercise-generate-a-tabletop-object-detection-dataset",level:3},{value:"Requirements",id:"requirements",level:4},{value:"Deliverables",id:"deliverables",level:4},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"synthetic-data-generation",children:"Synthetic Data Generation"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(n.p,{children:["One of Isaac Sim's most powerful capabilities is ",(0,a.jsx)(n.strong,{children:"synthetic data generation"})," - the ability to automatically produce labeled training data for AI perception models. Instead of manually collecting and annotating thousands of real-world images, you can generate diverse, perfectly labeled datasets in simulation."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Extract RGB camera images, depth maps, and segmentation masks"}),"\n",(0,a.jsx)(n.li,{children:"Programmatically place cameras for multi-view data collection"}),"\n",(0,a.jsx)(n.li,{children:"Annotate objects automatically for object detection"}),"\n",(0,a.jsx)(n.li,{children:"Export datasets in formats for AI training (COCO, KITTI, custom)"}),"\n",(0,a.jsx)(n.li,{children:"Generate diverse training data at scale"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prerequisites"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Completed: Installation & Photorealistic Environments"}),"\n",(0,a.jsx)(n.li,{children:"Python programming basics"}),"\n",(0,a.jsx)(n.li,{children:"Understanding of computer vision concepts (helpful but not required)"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Estimated Time"}),": 2-3 hours"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Capture RGB, depth, and semantic segmentation data from Isaac Sim cameras"}),"\n",(0,a.jsx)(n.li,{children:"Configure camera sensors programmatically via Python API"}),"\n",(0,a.jsx)(n.li,{children:"Annotate 3D bounding boxes and instance segmentation masks"}),"\n",(0,a.jsx)(n.li,{children:"Export datasets in standard formats (COCO JSON, KITTI)"}),"\n",(0,a.jsx)(n.li,{children:"Generate thousands of labeled images automatically"}),"\n",(0,a.jsx)(n.li,{children:"Validate synthetic data quality for AI training"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-1-understanding-synthetic-data-types",children:"Part 1: Understanding Synthetic Data Types"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim can generate multiple data modalities simultaneously from a single scene:"}),"\n",(0,a.jsx)(n.h3,{id:"data-modality-overview",children:"Data Modality Overview"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Modality"}),(0,a.jsx)(n.th,{children:"Description"}),(0,a.jsx)(n.th,{children:"Use Case"}),(0,a.jsx)(n.th,{children:"File Format"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"RGB"})}),(0,a.jsx)(n.td,{children:"Standard color camera images"}),(0,a.jsx)(n.td,{children:"Object recognition, scene understanding"}),(0,a.jsx)(n.td,{children:".png, .jpg"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Depth"})}),(0,a.jsx)(n.td,{children:"Distance from camera to each pixel"}),(0,a.jsx)(n.td,{children:"3D reconstruction, obstacle avoidance"}),(0,a.jsx)(n.td,{children:".npy, .exr"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Semantic Segmentation"})}),(0,a.jsx)(n.td,{children:"Per-pixel class labels"}),(0,a.jsx)(n.td,{children:"Scene parsing, terrain classification"}),(0,a.jsx)(n.td,{children:".png (color-coded)"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Instance Segmentation"})}),(0,a.jsx)(n.td,{children:"Per-pixel object instance IDs"}),(0,a.jsx)(n.td,{children:"Counting objects, tracking instances"}),(0,a.jsx)(n.td,{children:".png (ID map)"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"2D Bounding Boxes"})}),(0,a.jsx)(n.td,{children:"Rectangular object bounds in image"}),(0,a.jsx)(n.td,{children:"Object detection (YOLO, Faster R-CNN)"}),(0,a.jsx)(n.td,{children:"JSON, XML"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"3D Bounding Boxes"})}),(0,a.jsx)(n.td,{children:"Oriented 3D boxes in world space"}),(0,a.jsx)(n.td,{children:"3D object detection, LiDAR fusion"}),(0,a.jsx)(n.td,{children:"JSON"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Normals"})}),(0,a.jsx)(n.td,{children:"Surface normal vectors"}),(0,a.jsx)(n.td,{children:"3D reconstruction, relighting"}),(0,a.jsx)(n.td,{children:".exr"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Optical Flow"})}),(0,a.jsx)(n.td,{children:"Pixel motion between frames"}),(0,a.jsx)(n.td,{children:"Motion estimation, tracking"}),(0,a.jsx)(n.td,{children:".exr"})]})]})]}),"\n",(0,a.jsx)(n.admonition,{title:"Why Synthetic Data?",type:"info",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cost"}),": Free vs. $0.01-$0.10/image for manual annotation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Speed"}),": 1000s of images/hour vs. days for real collection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Diversity"}),": Unlimited scene variations vs. limited real scenarios"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perfection"}),": No annotation errors vs. 5-10% human error rate"]}),"\n"]})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-2-setting-up-a-camera-sensor",children:"Part 2: Setting Up a Camera Sensor"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-create-a-scene-with-objects",children:"Step 1: Create a Scene with Objects"}),"\n",(0,a.jsx)(n.p,{children:"Before generating data, we need a scene with labeled objects. Let's use the warehouse from the previous lesson or create a simple one:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Launch Isaac Sim"}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"File"})," > ",(0,a.jsx)(n.strong,{children:"Open"})," > Select ",(0,a.jsx)(n.code,{children:"warehouse_pickplace_v1.usd"})," (from previous lesson)\nOR create a new scene with:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Ground plane"}),"\n",(0,a.jsx)(n.li,{children:"5-10 props (boxes, bins, robots)"}),"\n",(0,a.jsx)(n.li,{children:"Lighting"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"step-2-add-a-camera-via-gui",children:"Step 2: Add a Camera via GUI"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create"})," > ",(0,a.jsx)(n.strong,{children:"Camera"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["In ",(0,a.jsx)(n.strong,{children:"Property"})," panel:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Name"}),": ",(0,a.jsx)(n.code,{children:"DataCollectionCamera"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Position"}),": ",(0,a.jsx)(n.code,{children:"X: -200, Y: 200, Z: 150"})," (angled view of scene)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rotation"}),": Point toward objects (use ",(0,a.jsx)(n.strong,{children:"Look At"})," tool)"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Set camera parameters:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Focal Length"}),": ",(0,a.jsx)(n.code,{children:"24mm"})," (wide angle for full scene coverage)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Horizontal Aperture"}),": ",(0,a.jsx)(n.code,{children:"36mm"})," (sensor size)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": ",(0,a.jsx)(n.code,{children:"1280x720"})," (720p for training data)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"step-3-enable-replicator-isaac-sims-data-generation-tool",children:"Step 3: Enable Replicator (Isaac Sim's Data Generation Tool)"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Utils"})," > ",(0,a.jsx)(n.strong,{children:"Replicator"})," > ",(0,a.jsx)(n.strong,{children:"Enable Replicator"})]}),"\n",(0,a.jsxs)(n.li,{children:["A new ",(0,a.jsx)(n.strong,{children:"Replicator"})," panel will appear (bottom of screen)"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-3-capturing-data-via-python-api",children:"Part 3: Capturing Data via Python API"}),"\n",(0,a.jsx)(n.p,{children:"For scalable data generation, we use Isaac Sim's Python API. This allows automation and batch processing."}),"\n",(0,a.jsx)(n.h3,{id:"step-1-create-a-data-capture-script",children:"Step 1: Create a Data Capture Script"}),"\n",(0,a.jsxs)(n.p,{children:["Create a file: ",(0,a.jsx)(n.code,{children:"~/Documents/Isaac-Sim/scripts/capture_synthetic_data.py"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\nimport omni.isaac.core.utils.stage as stage_utils\nfrom omni.isaac.core import SimulationContext\nimport numpy as np\n\n# Initialize Isaac Sim\nsimulation_context = SimulationContext(physics_dt=1.0/60.0, rendering_dt=1.0/60.0)\nsimulation_context.initialize_physics()\n\n# Create camera\ncamera = rep.create.camera(\n    position=(-200, 200, 150),\n    look_at=(0, 0, 50),  # Point camera at scene center\n    resolution=(1280, 720),\n    name="synthetic_data_camera"\n)\n\n# Define output annotators (data types to capture)\nrgb_annotator = rep.AnnotatorRegistry.get_annotator("rgb")\ndepth_annotator = rep.AnnotatorRegistry.get_annotator("distance_to_camera")\nsemantic_annotator = rep.AnnotatorRegistry.get_annotator("semantic_segmentation")\ninstance_annotator = rep.AnnotatorRegistry.get_annotator("instance_segmentation_fast")\nbbox_2d_annotator = rep.AnnotatorRegistry.get_annotator("bounding_box_2d_tight")\n\n# Attach annotators to camera\ncamera.add_annotator(rgb_annotator)\ncamera.add_annotator(depth_annotator)\ncamera.add_annotator(semantic_annotator)\ncamera.add_annotator(instance_annotator)\ncamera.add_annotator(bbox_2d_annotator)\n\n# Set output directory\noutput_dir = "/home/user/synthetic_data/warehouse_dataset"\nrep.WriterRegistry.register_writer("BasicWriter", output_dir=output_dir)\n\nprint("Camera and annotators configured successfully!")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-run-the-script-in-isaac-sim",children:"Step 2: Run the Script in Isaac Sim"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["In Isaac Sim: ",(0,a.jsx)(n.strong,{children:"Window"})," > ",(0,a.jsx)(n.strong,{children:"Script Editor"})]}),"\n",(0,a.jsx)(n.li,{children:"Paste the script above"}),"\n",(0,a.jsxs)(n.li,{children:["Click ",(0,a.jsx)(n.strong,{children:"Run"})]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["You should see: ",(0,a.jsx)(n.code,{children:"Camera and annotators configured successfully!"})]}),"\n",(0,a.jsx)(n.h3,{id:"step-3-capture-a-single-frame",children:"Step 3: Capture a Single Frame"}),"\n",(0,a.jsx)(n.p,{children:"Add to the script:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Capture one frame of data\nsimulation_context.step(render=True)  # Run physics for 1 frame\n\n# Retrieve annotator outputs\nrgb_data = rgb_annotator.get_data()\ndepth_data = depth_annotator.get_data()\nsemantic_data = semantic_annotator.get_data()\ninstance_data = instance_annotator.get_data()\nbbox_data = bbox_2d_annotator.get_data()\n\n# Save RGB image\nimport cv2\ncv2.imwrite(f"{output_dir}/rgb_0000.png", rgb_data)\nprint(f"Saved RGB image to {output_dir}/rgb_0000.png")\n\n# Save depth map (as numpy array)\nnp.save(f"{output_dir}/depth_0000.npy", depth_data)\nprint(f"Saved depth map")\n\n# Semantic segmentation is saved automatically by Replicator\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Run the updated script. Check ",(0,a.jsx)(n.code,{children:"/home/user/synthetic_data/warehouse_dataset"})," for output files."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-4-understanding-depth-and-segmentation-data",children:"Part 4: Understanding Depth and Segmentation Data"}),"\n",(0,a.jsx)(n.h3,{id:"depth-maps",children:"Depth Maps"}),"\n",(0,a.jsxs)(n.p,{children:["Depth data represents the distance from the camera to each pixel in ",(0,a.jsx)(n.strong,{children:"meters"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Visualizing Depth"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Load depth map\ndepth = np.load(\"/home/user/synthetic_data/warehouse_dataset/depth_0000.npy\")\n\n# Visualize (closer = darker, farther = lighter)\nplt.imshow(depth, cmap='gray')\nplt.colorbar(label='Distance (meters)')\nplt.title('Depth Map')\nplt.show()\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Use Cases"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Obstacle avoidance (identify objects <2m from robot)"}),"\n",(0,a.jsx)(n.li,{children:"3D reconstruction (convert depth to point cloud)"}),"\n",(0,a.jsx)(n.li,{children:"Grasp planning (estimate object distance)"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,a.jsx)(n.p,{children:'Each pixel is labeled with its object class (e.g., "box", "robot", "floor").'}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Visualizing Semantic Segmentation"}),":"]}),"\n",(0,a.jsx)(n.p,{children:"Segmentation masks are saved as color-coded PNG images where each color represents a class:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from PIL import Image\n\n# Load semantic segmentation\nseg_img = Image.open("/home/user/synthetic_data/warehouse_dataset/semantic_0000.png")\nseg_img.show()\n\n# Color mapping (example):\n# Red (255, 0, 0) = Robot\n# Green (0, 255, 0) = Box\n# Blue (0, 0, 255) = Floor\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Use Cases"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Terrain classification (drivable vs. obstacle)"}),"\n",(0,a.jsx)(n.li,{children:"Scene understanding (identify all objects in view)"}),"\n",(0,a.jsx)(n.li,{children:"Training segmentation models (U-Net, DeepLab)"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"instance-segmentation",children:"Instance Segmentation"}),"\n",(0,a.jsx)(n.p,{children:'Similar to semantic, but each individual object gets a unique ID (e.g., "box_001", "box_002").'}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Load instance segmentation\ninstance_img = np.array(Image.open("/home/user/synthetic_data/warehouse_dataset/instance_0000.png"))\n\n# Each unique pixel value is an instance ID\nunique_instances = np.unique(instance_img)\nprint(f"Number of instances detected: {len(unique_instances)}")\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Use Cases"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'Counting objects (e.g., "How many boxes on shelf?")'}),"\n",(0,a.jsx)(n.li,{children:"Tracking (follow specific object across frames)"}),"\n",(0,a.jsx)(n.li,{children:"Robotic grasping (identify individual target objects)"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-5-annotating-objects-for-detection",children:"Part 5: Annotating Objects for Detection"}),"\n",(0,a.jsx)(n.p,{children:"For object detection models (YOLO, Faster R-CNN), we need 2D bounding boxes with class labels."}),"\n",(0,a.jsx)(n.h3,{id:"step-1-define-semantic-classes",children:"Step 1: Define Semantic Classes"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim automatically assigns semantic labels based on USD prim names. To ensure correct labeling:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Select an object (e.g., a box) in the ",(0,a.jsx)(n.strong,{children:"Stage"})," panel"]}),"\n",(0,a.jsxs)(n.li,{children:["In ",(0,a.jsx)(n.strong,{children:"Property"})," panel, find ",(0,a.jsx)(n.strong,{children:"Semantics"})]}),"\n",(0,a.jsxs)(n.li,{children:["Add semantic label:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Type"}),": ",(0,a.jsx)(n.code,{children:"class"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data"}),": ",(0,a.jsx)(n.code,{children:"box"})," (or your class name)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Repeat for all objects in your scene (e.g., ",(0,a.jsx)(n.code,{children:"robot"}),", ",(0,a.jsx)(n.code,{children:"bin"}),", ",(0,a.jsx)(n.code,{children:"pallet"}),")."]}),"\n",(0,a.jsx)(n.h3,{id:"step-2-extract-2d-bounding-boxes",children:"Step 2: Extract 2D Bounding Boxes"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Get bounding box data from annotator\nbbox_data = bbox_2d_annotator.get_data()\n\n# bbox_data structure:\n# {\n#   'data': [\n#     {\n#       'semantic_id': 'box',\n#       'x_min': 320, 'y_min': 180,\n#       'x_max': 480, 'y_max': 360\n#     },\n#     ...\n#   ]\n# }\n\n# Convert to COCO format (for YOLO training)\ncoco_annotations = []\nfor i, bbox in enumerate(bbox_data['data']):\n    coco_annotations.append({\n        \"id\": i,\n        \"image_id\": 0,  # Frame number\n        \"category_id\": class_to_id[bbox['semantic_id']],  # Map 'box' -> 1, 'robot' -> 2, etc.\n        \"bbox\": [bbox['x_min'], bbox['y_min'],\n                 bbox['x_max'] - bbox['x_min'],  # width\n                 bbox['y_max'] - bbox['y_min']],  # height\n        \"area\": (bbox['x_max'] - bbox['x_min']) * (bbox['y_max'] - bbox['y_min']),\n        \"iscrowd\": 0\n    })\n\n# Save to JSON\nimport json\nwith open(f\"{output_dir}/annotations.json\", 'w') as f:\n    json.dump({\"annotations\": coco_annotations}, f, indent=2)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-visualize-bounding-boxes-on-image",children:"Step 3: Visualize Bounding Boxes on Image"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import cv2\n\n# Load RGB image\nimg = cv2.imread(f\"{output_dir}/rgb_0000.png\")\n\n# Draw bounding boxes\nfor bbox in bbox_data['data']:\n    cv2.rectangle(img,\n                  (bbox['x_min'], bbox['y_min']),\n                  (bbox['x_max'], bbox['y_max']),\n                  (0, 255, 0), 2)  # Green box\n    cv2.putText(img, bbox['semantic_id'],\n                (bbox['x_min'], bbox['y_min'] - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Save annotated image\ncv2.imwrite(f\"{output_dir}/rgb_0000_annotated.png\", img)\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-6-generating-large-scale-datasets",children:"Part 6: Generating Large-Scale Datasets"}),"\n",(0,a.jsxs)(n.p,{children:["To train robust AI models, we need thousands of diverse images. Isaac Sim's Replicator enables this through ",(0,a.jsx)(n.strong,{children:"randomization"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"step-1-randomize-camera-viewpoints",children:"Step 1: Randomize Camera Viewpoints"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import random\n\n# Generate 1000 images with random camera positions\nnum_frames = 1000\n\nfor i in range(num_frames):\n    # Randomize camera position (orbit around scene)\n    radius = random.uniform(150, 300)  # Distance from center\n    angle = random.uniform(0, 360)  # Degrees around Z-axis\n    height = random.uniform(50, 200)  # Elevation\n\n    # Calculate position\n    x = radius * np.cos(np.radians(angle))\n    y = radius * np.sin(np.radians(angle))\n    z = height\n\n    # Update camera\n    camera.set_position((x, y, z))\n    camera.set_look_at((0, 0, 50))  # Always point at scene center\n\n    # Step simulation\n    simulation_context.step(render=True)\n\n    # Capture data\n    rgb_data = rgb_annotator.get_data()\n    cv2.imwrite(f"{output_dir}/rgb_{i:04d}.png", rgb_data)\n\n    if i % 100 == 0:\n        print(f"Generated {i}/{num_frames} images")\n\nprint(f"Dataset generation complete! {num_frames} images saved.")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-randomize-object-poses",children:"Step 2: Randomize Object Poses"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Randomize object positions for each frame\nobjects_to_randomize = [\n    "/World/Warehouse/Box_001",\n    "/World/Warehouse/Box_002",\n    "/World/Warehouse/Bin_001"\n]\n\nfor i in range(num_frames):\n    # Randomize each object\'s position\n    for obj_path in objects_to_randomize:\n        obj_prim = stage_utils.get_prim_at_path(obj_path)\n\n        # Random position within bounds\n        x = random.uniform(-200, 200)\n        y = random.uniform(-200, 200)\n        z = random.uniform(10, 150)  # Above ground\n\n        # Apply transform\n        obj_prim.GetAttribute(\'xformOp:translate\').Set((x, y, z))\n\n    # Capture data...\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-randomize-lighting-covered-in-next-lesson",children:"Step 3: Randomize Lighting (covered in next lesson)"}),"\n",(0,a.jsxs)(n.p,{children:["We'll explore lighting randomization in detail in the ",(0,a.jsx)(n.strong,{children:"Domain Randomization"})," lesson."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-7-export-formats-for-ai-training",children:"Part 7: Export Formats for AI Training"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim Replicator supports multiple export formats:"}),"\n",(0,a.jsx)(n.h3,{id:"coco-format-object-detection",children:"COCO Format (Object Detection)"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Structure"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "images": [\n    {"id": 0, "file_name": "rgb_0000.png", "width": 1280, "height": 720}\n  ],\n  "annotations": [\n    {\n      "id": 0,\n      "image_id": 0,\n      "category_id": 1,\n      "bbox": [320, 180, 160, 180],\n      "area": 28800,\n      "iscrowd": 0\n    }\n  ],\n  "categories": [\n    {"id": 1, "name": "box"},\n    {"id": 2, "name": "robot"}\n  ]\n}\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Use"}),": YOLOv8, Detectron2, MMDetection training"]}),"\n",(0,a.jsx)(n.h3,{id:"kitti-format-3d-object-detection",children:"KITTI Format (3D Object Detection)"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Structure"})," (per-frame ",(0,a.jsx)(n.code,{children:".txt"})," file):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Box 0.00 0 -1.57 320 180 480 360 1.5 1.0 2.0 2.5 1.2 10.3 -1.57\nRobot 0.00 0 -0.78 500 200 650 400 0.8 0.8 1.5 1.0 0.5 8.2 -0.78\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Fields"}),": Class, truncation, occlusion, alpha, bbox_2d (x1, y1, x2, y2), dimensions_3d (h, w, l), location_3d (x, y, z), rotation_y"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Use"}),": 3D object detection, LiDAR-camera fusion"]}),"\n",(0,a.jsx)(n.h3,{id:"custom-format",children:"Custom Format"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Define your own export structure\ncustom_data = {\n    "frame_id": i,\n    "timestamp": simulation_context.current_time,\n    "camera_pose": {\n        "position": camera.get_position(),\n        "rotation": camera.get_rotation()\n    },\n    "rgb_path": f"rgb_{i:04d}.png",\n    "depth_path": f"depth_{i:04d}.npy",\n    "annotations": bbox_data[\'data\']\n}\n\nwith open(f"{output_dir}/frame_{i:04d}.json", \'w\') as f:\n    json.dump(custom_data, f, indent=2)\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-8-validating-synthetic-data-quality",children:"Part 8: Validating Synthetic Data Quality"}),"\n",(0,a.jsx)(n.p,{children:"Before using synthetic data for training, validate it matches your requirements:"}),"\n",(0,a.jsx)(n.h3,{id:"quality-checklist",children:"Quality Checklist"}),"\n",(0,a.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Class distribution"}),": Each class has >500 examples"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Viewpoint diversity"}),": Objects seen from 360\xb0 angles"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Occlusion"}),": Some objects partially hidden (realistic scenarios)"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Lighting variation"}),": Multiple light conditions (covered in domain randomization)"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Scale variation"}),": Objects at different distances from camera"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Annotation accuracy"}),": Bounding boxes tightly fit objects (manual spot-check)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"visualization-tools",children:"Visualization Tools"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\n\n# Plot class distribution\nclass_counts = {}\nfor bbox in all_annotations:\n    cls = bbox['semantic_id']\n    class_counts[cls] = class_counts.get(cls, 0) + 1\n\nplt.bar(class_counts.keys(), class_counts.values())\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.title('Class Distribution in Synthetic Dataset')\nplt.show()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"spot-check-annotations",children:"Spot-Check Annotations"}),"\n",(0,a.jsx)(n.p,{children:"Manually review 10-20 random images with overlaid bounding boxes to ensure:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"No missing objects"}),"\n",(0,a.jsx)(n.li,{children:"Correct class labels"}),"\n",(0,a.jsx)(n.li,{children:"Tight bounding boxes (not too loose or too tight)"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"part-9-hands-on-exercise",children:"Part 9: Hands-On Exercise"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-generate-a-tabletop-object-detection-dataset",children:"Exercise: Generate a Tabletop Object Detection Dataset"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Objective"}),": Create a dataset for training a model to detect kitchen objects on a countertop."]}),"\n",(0,a.jsx)(n.h4,{id:"requirements",children:"Requirements"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Scene Setup"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Tabletop from previous lesson"}),"\n",(0,a.jsx)(n.li,{children:"6 object classes: Apple, orange, mug, plate, spoon, bowl"}),"\n",(0,a.jsx)(n.li,{children:"3 instances of each class (18 objects total)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Data Generation"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"500 RGB images (1280x720 resolution)"}),"\n",(0,a.jsx)(n.li,{children:"500 depth maps"}),"\n",(0,a.jsx)(n.li,{children:"500 semantic segmentation masks"}),"\n",(0,a.jsx)(n.li,{children:"500 bounding box annotation files (COCO JSON format)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Randomization"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Camera: Random viewpoints (orbit radius 80-150cm, height 50-120cm)"}),"\n",(0,a.jsx)(n.li,{children:"Objects: Random positions on tabletop (no overlap)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Validation"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Each class has 80-120 instances across 500 images"}),"\n",(0,a.jsx)(n.li,{children:"Manual review of 20 random images shows correct annotations"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"deliverables",children:"Deliverables"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Python script: ",(0,a.jsx)(n.code,{children:"generate_tabletop_dataset.py"})]}),"\n",(0,a.jsxs)(n.li,{children:["Dataset folder with:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"rgb/"})," (500 images)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"depth/"})," (500 .npy files)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"semantic/"})," (500 segmentation masks)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"annotations.json"})," (COCO format)"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Validation report: Class distribution bar chart + 20 annotated image samples"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Estimated Time"}),": 1-2 hours"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this lesson, you learned:"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Types of synthetic data (RGB, depth, segmentation, bounding boxes)\n\u2705 How to configure cameras and annotators in Isaac Sim\n\u2705 Programmatic data capture via Python API\n\u2705 Automatic object annotation for detection\n\u2705 Large-scale dataset generation with randomization\n\u2705 Export formats for AI training (COCO, KITTI, custom)\n\u2705 Validation techniques for synthetic datasets"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synthetic data"})," dramatically reduces manual annotation costs"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Replicator"})," enables thousands of images per hour"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Randomization"})," is critical for dataset diversity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validation"})," ensures synthetic data quality matches real-world needs"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.p,{children:["In the next lesson (",(0,a.jsx)(n.strong,{children:"Domain Randomization"}),"), you'll learn:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Advanced randomization techniques (textures, lighting, physics)"}),"\n",(0,a.jsx)(n.li,{children:"Sim-to-real transfer strategies"}),"\n",(0,a.jsx)(n.li,{children:"Closing the reality gap with diverse training data"}),"\n",(0,a.jsx)(n.li,{children:"Balancing realism vs. diversity in synthetic datasets"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Replicator Documentation"}),": ",(0,a.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html",children:"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"COCO Dataset Format"}),": ",(0,a.jsx)(n.a,{href:"https://cocodataset.org/#format-data",children:"https://cocodataset.org/#format-data"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"KITTI Dataset Format"}),": ",(0,a.jsx)(n.a,{href:"https://www.cvlibs.net/datasets/kitti/eval_object.php",children:"https://www.cvlibs.net/datasets/kitti/eval_object.php"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synthetic Data Best Practices"}),": ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2109.09930",children:"https://arxiv.org/abs/2109.09930"})," (academic paper)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim Python API Reference"}),": ",(0,a.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/api.html",children:"https://docs.omniverse.nvidia.com/isaacsim/latest/api.html"})]}),"\n"]}),"\n",(0,a.jsx)(n.admonition,{title:"Real-World Impact",type:"tip",children:(0,a.jsx)(n.p,{children:"Many production AI models (e.g., Tesla Autopilot, Amazon warehouse robots) use synthetic data to augment real data, dramatically improving model robustness!"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(6540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);