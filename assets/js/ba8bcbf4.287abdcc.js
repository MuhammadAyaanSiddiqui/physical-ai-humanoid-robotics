"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[203],{516:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-3-isaac/ch10-rl/domain-randomization-techniques","title":"Advanced Domain Randomization","description":"Overview","source":"@site/docs/module-3-isaac/ch10-rl/domain-randomization-techniques.md","sourceDirName":"module-3-isaac/ch10-rl","slug":"/module-3-isaac/ch10-rl/domain-randomization-techniques","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch10-rl/domain-randomization-techniques","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ch10-rl/domain-randomization-techniques.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Sim-to-Real Transfer","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch10-rl/sim-to-real"},"next":{"title":"Module 3 Assessment: Perception + Navigation Project","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/assessment"}}');var i=n(4848),o=n(8453);const s={},t="Advanced Domain Randomization",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Part 1: Curriculum Learning",id:"part-1-curriculum-learning",level:2},{value:"Progressive Difficulty",id:"progressive-difficulty",level:3},{value:"Part 2: Adversarial Randomization",id:"part-2-adversarial-randomization",level:2},{value:"Maximize Policy Challenge",id:"maximize-policy-challenge",level:3},{value:"Part 3: Automatic Domain Randomization (ADR)",id:"part-3-automatic-domain-randomization-adr",level:2},{value:"Algorithm",id:"algorithm",level:3},{value:"Part 4: Multi-Modal Randomization",id:"part-4-multi-modal-randomization",level:2},{value:"Correlated Parameters",id:"correlated-parameters",level:3},{value:"Part 5: Validation",id:"part-5-validation",level:2},{value:"Test on Holdout Randomizations",id:"test-on-holdout-randomizations",level:3},{value:"Summary",id:"summary",level:2},{value:"Resources",id:"resources",level:2}];function c(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.header,{children:(0,i.jsx)(a.h1,{id:"advanced-domain-randomization",children:"Advanced Domain Randomization"})}),"\n",(0,i.jsx)(a.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(a.p,{children:"Advanced domain randomization techniques go beyond basic parameter variation. This includes curriculum learning, adversarial randomization, and automatic domain randomization (ADR) that adapts randomization ranges during training for optimal sim-to-real transfer."}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Curriculum learning for progressive difficulty"}),"\n",(0,i.jsx)(a.li,{children:"Adversarial domain randomization"}),"\n",(0,i.jsx)(a.li,{children:"Automatic Domain Randomization (ADR)"}),"\n",(0,i.jsx)(a.li,{children:"Multi-modal randomization strategies"}),"\n"]}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Estimated Time"}),": 2 hours"]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"part-1-curriculum-learning",children:"Part 1: Curriculum Learning"}),"\n",(0,i.jsx)(a.h3,{id:"progressive-difficulty",children:"Progressive Difficulty"}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Concept"}),": Start easy, gradually increase challenge"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"class CurriculumScheduler:\n    def __init__(self):\n        self.episode = 0\n        self.difficulty = 0.0\n\n    def update(self):\n        self.episode += 1\n        # Linear increase: 0.0 \u2192 1.0 over 1000 episodes\n        self.difficulty = min(1.0, self.episode / 1000.0)\n\n    def get_randomization_range(self, base_range):\n        # Scale randomization by difficulty\n        return base_range * self.difficulty\n\n# Usage\nscheduler = CurriculumScheduler()\n\nfriction_range = scheduler.get_randomization_range([0.5, 1.5])\n# Episode 0: friction in [1.0, 1.0] (no randomization)\n# Episode 500: friction in [0.75, 1.25]\n# Episode 1000+: friction in [0.5, 1.5]\n"})}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"part-2-adversarial-randomization",children:"Part 2: Adversarial Randomization"}),"\n",(0,i.jsx)(a.h3,{id:"maximize-policy-challenge",children:"Maximize Policy Challenge"}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Concept"}),": Sample parameters that cause policy to perform poorly"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"def adversarial_randomization(policy, num_candidates=100):\n    # Sample many randomizations\n    candidates = []\n    for _ in range(num_candidates):\n        params = sample_random_params()\n        reward = evaluate_policy(policy, params)\n        candidates.append((params, reward))\n\n    # Select worst-performing parameters\n    worst_params = min(candidates, key=lambda x: x[1])[0]\n    return worst_params\n"})}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Effect"}),': Policy trains on "hard" scenarios, becomes more robust']}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"part-3-automatic-domain-randomization-adr",children:"Part 3: Automatic Domain Randomization (ADR)"}),"\n",(0,i.jsx)(a.h3,{id:"algorithm",children:"Algorithm"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"class ADR:\n    def __init__(self):\n        self.param_ranges = {\n            'friction': [0.9, 1.1],  # Start narrow\n            'mass': [0.95, 1.05]\n        }\n        self.performance_threshold = 0.8  # 80% success rate\n\n    def update_ranges(self, performance):\n        if performance > self.performance_threshold:\n            # Expand ranges (make harder)\n            for param in self.param_ranges:\n                lower, upper = self.param_ranges[param]\n                self.param_ranges[param] = [lower * 0.95, upper * 1.05]\n        else:\n            # Shrink ranges (make easier)\n            for param in self.param_ranges:\n                lower, upper = self.param_ranges[param]\n                self.param_ranges[param] = [lower * 1.02, upper * 0.98]\n\n# Training loop\nadr = ADR()\nfor episode in range(10000):\n    params = sample_from_ranges(adr.param_ranges)\n    reward = train_episode(params)\n    adr.update_ranges(reward)\n"})}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Benefit"}),": Automatically finds optimal randomization difficulty"]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"part-4-multi-modal-randomization",children:"Part 4: Multi-Modal Randomization"}),"\n",(0,i.jsx)(a.h3,{id:"correlated-parameters",children:"Correlated Parameters"}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Example"}),": Heavy robot \u2192 stronger motors"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"def sample_correlated_params():\n    mass_scale = random.uniform(0.8, 1.2)\n\n    # Correlate motor strength with mass\n    motor_scale = mass_scale * random.uniform(0.95, 1.05)\n\n    # Correlate friction (heavier robot compresses ground)\n    friction = 0.7 + 0.2 * (mass_scale - 1.0)\n\n    return {\n        'mass': mass_scale,\n        'motor_strength': motor_scale,\n        'friction': friction\n    }\n"})}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"part-5-validation",children:"Part 5: Validation"}),"\n",(0,i.jsx)(a.h3,{id:"test-on-holdout-randomizations",children:"Test on Holdout Randomizations"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Training: Randomize in ranges [0.5, 1.5]\n# Holdout: Test on [0.3, 0.5] and [1.5, 1.8]\n\nholdout_performance = evaluate_on_holdout(policy)\nif holdout_performance > 0.7:\n    print("Policy generalizes well!")\n'})}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(a.p,{children:"\u2705 Implemented curriculum learning for progressive training\n\u2705 Applied adversarial randomization for robustness\n\u2705 Used ADR for automatic difficulty adjustment\n\u2705 Validated generalization on holdout sets"}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Congratulations!"})," Module 3 (Isaac Sim + Perception + Navigation + RL) complete!"]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"resources",children:"Resources"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"ADR Paper"}),": ",(0,i.jsx)(a.a,{href:"https://arxiv.org/abs/1910.07113",children:"https://arxiv.org/abs/1910.07113"})]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Curriculum RL"}),": ",(0,i.jsx)(a.a,{href:"https://arxiv.org/abs/2003.04960",children:"https://arxiv.org/abs/2003.04960"})]}),"\n"]})]})}function m(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>s,x:()=>t});var r=n(6540);const i={},o=r.createContext(i);function s(e){const a=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function t(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:a},e.children)}}}]);