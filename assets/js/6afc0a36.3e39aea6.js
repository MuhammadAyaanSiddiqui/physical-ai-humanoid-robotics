"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[4487],{981:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>s,contentTitle:()=>c,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>r});const l=JSON.parse('{"id":"module-4-vla/ch14-multimodal/vla-pipeline","title":"Vision-Language-Action (VLA) Pipeline","description":"Learning Objectives","source":"@site/docs/module-4-vla/ch14-multimodal/vla-pipeline.md","sourceDirName":"module-4-vla/ch14-multimodal","slug":"/module-4-vla/ch14-multimodal/vla-pipeline","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/vla-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/ch14-multimodal/vla-pipeline.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Manipulation and Grasping","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch13-humanoid-control/manipulation"},"next":{"title":"Object Grounding: Language to Vision","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/object-grounding"}}');var t=i(4848),a=i(8453);const o={},c="Vision-Language-Action (VLA) Pipeline",s={},r=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Complete VLA Architecture",id:"complete-vla-architecture",level:2},{value:"Implementation",id:"implementation",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"vision-language-action-vla-pipeline",children:"Vision-Language-Action (VLA) Pipeline"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate voice, vision, language, and action systems"}),"\n",(0,t.jsx)(e.li,{children:"Build end-to-end VLA pipeline: Voice \u2192 LLM \u2192 Actions \u2192 Execution"}),"\n",(0,t.jsx)(e.li,{children:"Handle multi-modal sensor fusion"}),"\n",(0,t.jsx)(e.li,{children:"Implement closed-loop feedback control"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Estimated Time"}),": 4 hours"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"complete-vla-architecture",children:"Complete VLA Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"User Voice Command\n    \u2193\n[Whisper STT] \u2192 Transcript\n    \u2193\n[LLM Planner] \u2192 Action Plan (JSON)\n    \u2193\n[Object Detector] \u2192 Visual Grounding\n    \u2193\n[Action Executor] \u2192 ROS 2 Actions\n    \u2193\n[Robot] \u2192 Feedback\n    \u2193\n[Monitor] \u2192 Success/Failure \u2192 Replan if needed\n"})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\n\nclass VLAPipeline(Node):\n    def __init__(self):\n        super().__init__(\'vla_pipeline\')\n\n        # Subscribe to voice transcripts\n        self.create_subscription(String, \'/voice/transcript\', self.voice_callback, 10)\n\n        # Subscribe to camera feed\n        self.create_subscription(Image, \'/camera/image_raw\', self.vision_callback, 10)\n\n        # Publish action commands\n        self.action_pub = self.create_publisher(String, \'/robot/action\', 10)\n\n        self.get_logger().info(\'VLA Pipeline ready\')\n\n    def voice_callback(self, msg):\n        """Process voice command through complete pipeline"""\n        transcript = msg.data\n\n        # 1. Call LLM planner\n        plan = self.call_llm(transcript)\n\n        # 2. Ground objects in vision\n        grounded_plan = self.ground_objects(plan)\n\n        # 3. Execute actions\n        self.execute_plan(grounded_plan)\n\n    def call_llm(self, transcript):\n        """Call LLM to generate action plan"""\n        # TODO: Implement LLM call\n        return {"actions": []}\n\n    def ground_objects(self, plan):\n        """Map language to visual objects"""\n        # TODO: Implement visual grounding\n        return plan\n\n    def execute_plan(self, plan):\n        """Execute action sequence"""\n        for action in plan[\'actions\']:\n            self.action_pub.publish(String(data=str(action)))\n\ndef main():\n    rclpy.init()\n    node = VLAPipeline()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(e.p,{children:["Continue to ",(0,t.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/object-grounding",children:"Object Grounding \u2192"})]})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>c});var l=i(6540);const t={},a=l.createContext(t);function o(n){const e=l.useContext(a);return l.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),l.createElement(a.Provider,{value:e},n.children)}}}]);