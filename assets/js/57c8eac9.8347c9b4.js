"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[4816],{8019:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vla/ch11-whisper/speech-to-text","title":"Local Speech-to-Text Pipeline with Whisper","description":"Learning Objectives","source":"@site/docs/module-4-vla/ch11-whisper/speech-to-text.md","sourceDirName":"module-4-vla/ch11-whisper","slug":"/module-4-vla/ch11-whisper/speech-to-text","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/speech-to-text","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/ch11-whisper/speech-to-text.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Speech-to-Text with OpenAI Whisper API","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/whisper-api"},"next":{"title":"Voice Command Parsing: Intent and Slot Extraction","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/command-parsing"}}');var r=s(4848),t=s(8453);const l={},a="Local Speech-to-Text Pipeline with Whisper",o={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Part 1: Installing Local Whisper",id:"part-1-installing-local-whisper",level:2},{value:"Step 1: Install OpenAI Whisper (Official)",id:"step-1-install-openai-whisper-official",level:3},{value:"Step 2: Whisper Model Selection",id:"step-2-whisper-model-selection",level:3},{value:"Step 3: Download Models",id:"step-3-download-models",level:3},{value:"Part 2: Python API for Local Inference",id:"part-2-python-api-for-local-inference",level:2},{value:"Step 1: Basic Transcription",id:"step-1-basic-transcription",level:3},{value:"Step 2: GPU Acceleration",id:"step-2-gpu-acceleration",level:3},{value:"Step 3: Transcription Options",id:"step-3-transcription-options",level:3},{value:"Part 3: Faster-Whisper (Optimized Implementation)",id:"part-3-faster-whisper-optimized-implementation",level:2},{value:"Step 1: Install faster-whisper",id:"step-1-install-faster-whisper",level:3},{value:"Step 2: Basic Usage",id:"step-2-basic-usage",level:3},{value:"Step 3: Quantization for Speed",id:"step-3-quantization-for-speed",level:3},{value:"Step 4: Batched Inference (Advanced)",id:"step-4-batched-inference-advanced",level:3},{value:"Part 4: Complete STT Pipeline",id:"part-4-complete-stt-pipeline",level:2},{value:"Step 1: Voice Activity Detection (VAD)",id:"step-1-voice-activity-detection-vad",level:3},{value:"Step 2: Ring Buffer for Continuous Recording",id:"step-2-ring-buffer-for-continuous-recording",level:3},{value:"Step 3: Complete Pipeline with Error Handling",id:"step-3-complete-pipeline-with-error-handling",level:3},{value:"Step 4: ROS 2 Integration",id:"step-4-ros-2-integration",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercise 1: Latency Benchmark",id:"exercise-1-latency-benchmark",level:3},{value:"Exercise 2: Hybrid Cloud/Local System",id:"exercise-2-hybrid-cloudlocal-system",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Issue 1: &quot;CUDA out of memory&quot; error",id:"issue-1-cuda-out-of-memory-error",level:3},{value:"Issue 2: Slow inference on CPU",id:"issue-2-slow-inference-on-cpu",level:3},{value:"Issue 3: Poor accuracy on technical terms",id:"issue-3-poor-accuracy-on-technical-terms",level:3},{value:"Issue 4: High latency spikes",id:"issue-4-high-latency-spikes",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Optimization 1: Batch Processing",id:"optimization-1-batch-processing",level:3},{value:"Optimization 2: Streaming Inference",id:"optimization-2-streaming-inference",level:3},{value:"Summary",id:"summary",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:3},{value:"Video Tutorials",id:"video-tutorials",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"local-speech-to-text-pipeline-with-whisper",children:"Local Speech-to-Text Pipeline with Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Install and run OpenAI Whisper models locally (offline inference)"}),"\n",(0,r.jsx)(n.li,{children:"Optimize Whisper for real-time performance on edge devices"}),"\n",(0,r.jsx)(n.li,{children:"Build a complete STT pipeline with VAD, buffering, and post-processing"}),"\n",(0,r.jsx)(n.li,{children:"Accelerate Whisper inference with faster-whisper (CTranslate2)"}),"\n",(0,r.jsx)(n.li,{children:"Compare cloud API vs. local deployment trade-offs"}),"\n",(0,r.jsx)(n.li,{children:"Implement robust error handling and fallback strategies"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Required Knowledge"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Python programming (threading, async/await)"}),"\n",(0,r.jsx)(n.li,{children:"Audio processing basics (sample rates, formats)"}),"\n",(0,r.jsxs)(n.li,{children:["Completion of ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/audio-capture",children:"Lesson 1: Audio Capture"})]}),"\n",(0,r.jsxs)(n.li,{children:["Completion of ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/whisper-api",children:"Lesson 2: Whisper API"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Required Hardware"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Minimum"}),": Intel Core i5 / AMD Ryzen 5 (CPU-only inference)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recommended"}),": NVIDIA GPU with 4GB+ VRAM (GPU acceleration)","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Jetson Nano: base/tiny models only"}),"\n",(0,r.jsx)(n.li,{children:"Jetson Xavier NX: small/medium models"}),"\n",(0,r.jsx)(n.li,{children:"RTX 3060+: All models including large-v3"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Required Software"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Ubuntu 22.04 or compatible Linux"}),"\n",(0,r.jsx)(n.li,{children:"Python 3.10+"}),"\n",(0,r.jsx)(n.li,{children:"CUDA Toolkit 11.8+ (for GPU acceleration)"}),"\n",(0,r.jsx)(n.li,{children:"FFmpeg"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 3-4 hours"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["Running Whisper ",(0,r.jsx)(n.strong,{children:"locally"})," (on-device) provides several advantages over cloud API:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Factor"}),(0,r.jsx)(n.th,{children:"Local Whisper"}),(0,r.jsx)(n.th,{children:"Cloud API (Lesson 2)"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Privacy"})}),(0,r.jsx)(n.td,{children:"Audio never leaves device"}),(0,r.jsx)(n.td,{children:"Audio sent to OpenAI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Latency"})}),(0,r.jsx)(n.td,{children:"100-500ms (GPU)"}),(0,r.jsx)(n.td,{children:"500-2000ms (network)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Cost"})}),(0,r.jsx)(n.td,{children:"Free (after hardware)"}),(0,r.jsx)(n.td,{children:"$0.006/minute"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Offline"})}),(0,r.jsx)(n.td,{children:"Works without internet"}),(0,r.jsx)(n.td,{children:"Requires connection"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Reliability"})}),(0,r.jsx)(n.td,{children:"No rate limits"}),(0,r.jsx)(n.td,{children:"50 req/min (free tier)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Hardware"})}),(0,r.jsx)(n.td,{children:"GPU recommended"}),(0,r.jsx)(n.td,{children:"CPU-only robot OK"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to use local Whisper"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Production deployments with strict latency requirements (<500ms)"}),"\n",(0,r.jsx)(n.li,{children:"Privacy-sensitive applications (healthcare, defense)"}),"\n",(0,r.jsx)(n.li,{children:"Offline robots (warehouses without Wi-Fi)"}),"\n",(0,r.jsx)(n.li,{children:"High-volume usage (>1000 hours/month \u2192 cheaper than API)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to use cloud API"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Prototyping and development"}),"\n",(0,r.jsx)(n.li,{children:"CPU-only edge devices (no GPU)"}),"\n",(0,r.jsx)(n.li,{children:"Low-volume usage (<100 hours/month)"}),"\n",(0,r.jsx)(n.li,{children:"Need for 99 language support with highest accuracy"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-1-installing-local-whisper",children:"Part 1: Installing Local Whisper"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-install-openai-whisper-official",children:"Step 1: Install OpenAI Whisper (Official)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install FFmpeg (audio processing)\nsudo apt update\nsudo apt install -y ffmpeg\n\n# Install Whisper\npip3 install -U openai-whisper\n\n# Verify installation\nwhisper --help\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Test with sample audio"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Download sample audio\nwget https://github.com/openai/whisper/raw/main/tests/jfk.flac\n\n# Transcribe with small model (244MB)\nwhisper jfk.flac --model small --language en\n\n# Expected output:\n# [00:00.000 --\x3e 00:11.000] And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-whisper-model-selection",children:"Step 2: Whisper Model Selection"}),"\n",(0,r.jsx)(n.p,{children:"Whisper provides 5 model sizes with accuracy/speed trade-offs:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"Disk Size"}),(0,r.jsx)(n.th,{children:"VRAM"}),(0,r.jsx)(n.th,{children:"Relative Speed"}),(0,r.jsx)(n.th,{children:"WER (English)"}),(0,r.jsx)(n.th,{children:"Best For"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"tiny"})}),(0,r.jsx)(n.td,{children:"39M"}),(0,r.jsx)(n.td,{children:"72 MB"}),(0,r.jsx)(n.td,{children:"1 GB"}),(0,r.jsx)(n.td,{children:"32x"}),(0,r.jsx)(n.td,{children:"5.7%"}),(0,r.jsx)(n.td,{children:"Jetson Nano, prototyping"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"base"})}),(0,r.jsx)(n.td,{children:"74M"}),(0,r.jsx)(n.td,{children:"140 MB"}),(0,r.jsx)(n.td,{children:"1 GB"}),(0,r.jsx)(n.td,{children:"16x"}),(0,r.jsx)(n.td,{children:"4.3%"}),(0,r.jsx)(n.td,{children:"Edge devices (Pi 4)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"small"})}),(0,r.jsx)(n.td,{children:"244M"}),(0,r.jsx)(n.td,{children:"461 MB"}),(0,r.jsx)(n.td,{children:"2 GB"}),(0,r.jsx)(n.td,{children:"6x"}),(0,r.jsx)(n.td,{children:"3.5%"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Recommended for robotics"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"medium"})}),(0,r.jsx)(n.td,{children:"769M"}),(0,r.jsx)(n.td,{children:"1.5 GB"}),(0,r.jsx)(n.td,{children:"5 GB"}),(0,r.jsx)(n.td,{children:"2x"}),(0,r.jsx)(n.td,{children:"2.9%"}),(0,r.jsx)(n.td,{children:"High-accuracy applications"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"large-v3"})}),(0,r.jsx)(n.td,{children:"1550M"}),(0,r.jsx)(n.td,{children:"2.9 GB"}),(0,r.jsx)(n.td,{children:"10 GB"}),(0,r.jsx)(n.td,{children:"1x"}),(0,r.jsx)(n.td,{children:"2.4%"}),(0,r.jsx)(n.td,{children:"Cloud/workstation only"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"WER = Word Error Rate"})," (lower is better, 3.5% = 96.5% accuracy)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation for humanoid robots"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jetson Nano"}),": ",(0,r.jsx)(n.code,{children:"tiny"})," or ",(0,r.jsx)(n.code,{children:"base"})," (CPU-only)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jetson Xavier NX"}),": ",(0,r.jsx)(n.code,{children:"small"})," (GPU-accelerated)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Laptop/Desktop with GPU"}),": ",(0,r.jsx)(n.code,{children:"small"})," or ",(0,r.jsx)(n.code,{children:"medium"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cloud server"}),": ",(0,r.jsx)(n.code,{children:"large-v3"})," (highest accuracy)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-download-models",children:"Step 3: Download Models"}),"\n",(0,r.jsx)(n.p,{children:"Models are downloaded automatically on first use, but you can pre-download:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Download specific model\npython3 << EOF\nimport whisper\nmodel = whisper.load_model("small")  # Downloads ~/.cache/whisper/small.pt\nprint(f"Model loaded: {model.dims}")\nEOF\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Manual download"})," (for air-gapped systems):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Download model weights\nwget https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt\n\n# Move to Whisper cache\nmkdir -p ~/.cache/whisper\nmv small.pt ~/.cache/whisper/\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-2-python-api-for-local-inference",children:"Part 2: Python API for Local Inference"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-basic-transcription",children:"Step 1: Basic Transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport whisper\n\n# Load model (cached after first run)\nmodel = whisper.load_model(\"small\")\n\n# Transcribe audio file\nresult = model.transcribe(\"audio.wav\")\n\n# Access transcription\nprint(f\"Text: {result['text']}\")\nprint(f\"Language: {result['language']}\")\n\n# Access segments with timestamps\nfor segment in result['segments']:\n    print(f\"[{segment['start']:.2f}s -> {segment['end']:.2f}s] {segment['text']}\")\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sample output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Text: Move forward three meters and turn left.\nLanguage: en\n[0.00s -> 1.20s]  Move forward three meters\n[1.20s -> 2.40s]  and turn left.\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-gpu-acceleration",children:"Step 2: GPU Acceleration"}),"\n",(0,r.jsx)(n.p,{children:"By default, Whisper uses GPU if available. Verify:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\n\n# Check GPU availability\nprint(f"CUDA available: {torch.cuda.is_available()}")\nprint(f"GPU: {torch.cuda.get_device_name(0)}" if torch.cuda.is_available() else "CPU-only")\n\n# Load model (automatically uses GPU)\nmodel = whisper.load_model("small")\n\n# Check model device\nprint(f"Model device: {next(model.parameters()).device}")\n# Output: cuda:0 (GPU) or cpu (CPU)\n\n# Force CPU (for testing)\nmodel_cpu = whisper.load_model("small", device="cpu")\n\n# Force specific GPU\nmodel_gpu1 = whisper.load_model("small", device="cuda:1")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance comparison"})," (5-second audio):"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Device"}),(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Inference Time"}),(0,r.jsx)(n.th,{children:"Real-Time Factor"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Intel i7-12700 (CPU)"}),(0,r.jsx)(n.td,{children:"small"}),(0,r.jsx)(n.td,{children:"2.5s"}),(0,r.jsx)(n.td,{children:"0.5x (2x slower than real-time)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"NVIDIA RTX 3060 (GPU)"}),(0,r.jsx)(n.td,{children:"small"}),(0,r.jsx)(n.td,{children:"0.3s"}),(0,r.jsx)(n.td,{children:"16x (16x faster than real-time)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jetson Xavier NX (GPU)"}),(0,r.jsx)(n.td,{children:"small"}),(0,r.jsx)(n.td,{children:"1.2s"}),(0,r.jsx)(n.td,{children:"4x"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jetson Nano (CPU)"}),(0,r.jsx)(n.td,{children:"tiny"}),(0,r.jsx)(n.td,{children:"6.0s"}),(0,r.jsx)(n.td,{children:"0.8x"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real-Time Factor (RTF)"}),": ",(0,r.jsx)(n.code,{children:"audio_duration / inference_time"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"RTF > 1.0 = Faster than real-time (good for robotics)"}),"\n",(0,r.jsx)(n.li,{children:"RTF < 1.0 = Slower than real-time (not suitable for real-time applications)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-transcription-options",children:"Step 3: Transcription Options"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'result = model.transcribe(\n    "audio.wav",\n\n    # Language (auto-detect if omitted)\n    language="en",  # ISO 639-1 code\n\n    # Task: transcribe or translate\n    task="transcribe",  # "translate" converts to English\n\n    # Temperature for sampling (0.0 = greedy, higher = more creative)\n    temperature=0.0,  # Recommended for robotics (deterministic)\n\n    # Beam size (higher = more accurate but slower)\n    beam_size=5,  # Default: 5\n\n    # Best_of (number of candidates, higher = better quality)\n    best_of=5,  # Default: 5\n\n    # Compression ratio threshold (detect gibberish)\n    compression_ratio_threshold=2.4,\n\n    # Log probability threshold (filter low-confidence segments)\n    logprob_threshold=-1.0,\n\n    # No speech threshold (skip silent audio)\n    no_speech_threshold=0.6,\n\n    # Verbose (print progress)\n    verbose=False\n)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommended settings for robotics"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Fast, low-latency configuration\nresult = model.transcribe(\n    "audio.wav",\n    language="en",  # Skip language detection (saves 1s)\n    temperature=0.0,  # Deterministic output\n    beam_size=1,  # Greedy decoding (fastest)\n    best_of=1,\n    no_speech_threshold=0.6,  # Skip silence\n    verbose=False\n)\n\n# High-accuracy configuration\nresult = model.transcribe(\n    "audio.wav",\n    temperature=0.0,\n    beam_size=5,  # More thorough search\n    best_of=5,\n    compression_ratio_threshold=2.4,\n    logprob_threshold=-1.0,\n    verbose=True\n)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-3-faster-whisper-optimized-implementation",children:"Part 3: Faster-Whisper (Optimized Implementation)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"faster-whisper"})," is a reimplementation using CTranslate2, achieving ",(0,r.jsx)(n.strong,{children:"4x speedup"})," with same accuracy."]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-install-faster-whisper",children:"Step 1: Install faster-whisper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install faster-whisper\n\n# For GPU acceleration, install CUDA-enabled version\npip3 install faster-whisper[gpu]\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-basic-usage",children:"Step 2: Basic Usage"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom faster_whisper import WhisperModel\n\n# Load model (automatically downloads if not cached)\nmodel = WhisperModel(\n    "small",\n    device="cuda",  # or "cpu"\n    compute_type="float16"  # or "int8" for quantization\n)\n\n# Transcribe\nsegments, info = model.transcribe(\n    "audio.wav",\n    language="en",\n    beam_size=5\n)\n\nprint(f"Detected language: {info.language} (probability: {info.language_probability:.2f})")\n\n# Iterate through segments\nfor segment in segments:\n    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-quantization-for-speed",children:"Step 3: Quantization for Speed"}),"\n",(0,r.jsx)(n.p,{children:"Reduce model precision for faster inference:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Compute Type"}),(0,r.jsx)(n.th,{children:"Precision"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Accuracy"}),(0,r.jsx)(n.th,{children:"VRAM"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"float32"})}),(0,r.jsx)(n.td,{children:"Full"}),(0,r.jsx)(n.td,{children:"1x"}),(0,r.jsx)(n.td,{children:"100%"}),(0,r.jsx)(n.td,{children:"2 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"float16"})}),(0,r.jsx)(n.td,{children:"Half"}),(0,r.jsx)(n.td,{children:"2x"}),(0,r.jsx)(n.td,{children:"99.9%"}),(0,r.jsx)(n.td,{children:"1 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"int8"})}),(0,r.jsx)(n.td,{children:"Quantized"}),(0,r.jsx)(n.td,{children:"4x"}),(0,r.jsx)(n.td,{children:"99.5%"}),(0,r.jsx)(n.td,{children:"500 MB"})]})]})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# float16 (recommended for GPU)\nmodel_fp16 = WhisperModel("small", device="cuda", compute_type="float16")\n\n# int8 (CPU or low-VRAM GPUs)\nmodel_int8 = WhisperModel("small", device="cpu", compute_type="int8")\n\n# Benchmark\nimport time\n\naudio_path = "test.wav"\n\n# float16\nstart = time.time()\nsegments, _ = model_fp16.transcribe(audio_path)\nlist(segments)  # Force evaluation\nprint(f"float16: {time.time() - start:.2f}s")\n\n# int8\nstart = time.time()\nsegments, _ = model_int8.transcribe(audio_path)\nlist(segments)\nprint(f"int8: {time.time() - start:.2f}s")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Typical results (5s audio, RTX 3060)"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"float16: 0.28s\nint8: 0.15s (2x faster, negligible accuracy loss)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-4-batched-inference-advanced",children:"Step 4: Batched Inference (Advanced)"}),"\n",(0,r.jsx)(n.p,{children:"Process multiple audio files in parallel:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from faster_whisper import WhisperModel\nimport concurrent.futures\n\nmodel = WhisperModel("small", device="cuda", compute_type="float16")\n\ndef transcribe_file(audio_path: str) -> str:\n    """Transcribe single file"""\n    segments, _ = model.transcribe(audio_path, beam_size=1)\n    return " ".join([seg.text for seg in segments])\n\n# Transcribe multiple files in parallel\naudio_files = ["cmd1.wav", "cmd2.wav", "cmd3.wav", "cmd4.wav"]\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(transcribe_file, audio_files))\n\nfor file, transcript in zip(audio_files, results):\n    print(f"{file}: {transcript}")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-4-complete-stt-pipeline",children:"Part 4: Complete STT Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"Build a production-ready pipeline with VAD, buffering, and error handling."}),"\n",(0,r.jsx)(n.h3,{id:"step-1-voice-activity-detection-vad",children:"Step 1: Voice Activity Detection (VAD)"}),"\n",(0,r.jsx)(n.p,{children:"Use Silero VAD to detect speech before transcription (saves compute):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install silero-vad\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport torch\ntorch.set_num_threads(1)  # Reduce CPU usage\n\n# Load Silero VAD model\nmodel, utils = torch.hub.load(\n    repo_or_dir=\'snakers4/silero-vad\',\n    model=\'silero_vad\',\n    force_reload=False\n)\n\n(get_speech_timestamps, _, read_audio, *_) = utils\n\ndef detect_speech(audio_path: str) -> bool:\n    """Return True if audio contains speech"""\n    wav = read_audio(audio_path)\n    speech_timestamps = get_speech_timestamps(\n        wav,\n        model,\n        threshold=0.5,  # Sensitivity (0.0-1.0)\n        min_speech_duration_ms=250,  # Minimum speech length\n        min_silence_duration_ms=100\n    )\n    return len(speech_timestamps) > 0\n\n# Usage\nif detect_speech("audio.wav"):\n    print("Speech detected - transcribing...")\n    # Run Whisper\nelse:\n    print("No speech detected - skipping")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance impact"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"VAD inference: ~10ms (on CPU)"}),"\n",(0,r.jsx)(n.li,{children:"Whisper inference: ~300ms (on GPU)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Savings"}),": Skip 90% of silent audio (motors running, no speech)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-ring-buffer-for-continuous-recording",children:"Step 2: Ring Buffer for Continuous Recording"}),"\n",(0,r.jsx)(n.p,{children:'Implement "always listening" system that saves last N seconds on trigger:'}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport pyaudio\nimport wave\nfrom collections import deque\nimport threading\nimport time\n\nclass AudioRingBuffer:\n    def __init__(self, duration_seconds: int = 5, sample_rate: int = 16000):\n        self.duration = duration_seconds\n        self.sample_rate = sample_rate\n        self.chunk_size = 1024\n\n        # Ring buffer (automatically discards old data)\n        buffer_size = int(sample_rate / self.chunk_size * duration_seconds)\n        self.buffer = deque(maxlen=buffer_size)\n\n        # Recording state\n        self.is_recording = False\n        self.p = pyaudio.PyAudio()\n        self.stream = None\n\n    def start(self):\n        """Start continuous recording"""\n        self.is_recording = True\n        self.stream = self.p.open(\n            rate=self.sample_rate,\n            format=pyaudio.paInt16,\n            channels=1,\n            input=True,\n            input_device_index=1,  # ReSpeaker\n            frames_per_buffer=self.chunk_size,\n            stream_callback=self._audio_callback\n        )\n        self.stream.start_stream()\n        print(f"Recording started (buffering last {self.duration}s)")\n\n    def _audio_callback(self, in_data, frame_count, time_info, status):\n        """Callback for incoming audio"""\n        if self.is_recording:\n            self.buffer.append(in_data)\n        return (in_data, pyaudio.paContinue)\n\n    def save_buffer(self, filename: str):\n        """Save ring buffer to WAV file"""\n        with wave.open(filename, \'wb\') as wf:\n            wf.setnchannels(1)\n            wf.setsampwidth(2)\n            wf.setframerate(self.sample_rate)\n            wf.writeframes(b\'\'.join(self.buffer))\n        print(f"Saved {len(self.buffer)} chunks to {filename}")\n\n    def stop(self):\n        """Stop recording"""\n        self.is_recording = False\n        if self.stream:\n            self.stream.stop_stream()\n            self.stream.close()\n        self.p.terminate()\n\n# Usage example\nbuffer = AudioRingBuffer(duration_seconds=5)\nbuffer.start()\n\ntry:\n    while True:\n        command = input("Press Enter to save buffer, or \'q\' to quit: ")\n        if command == \'q\':\n            break\n        buffer.save_buffer(f"capture_{int(time.time())}.wav")\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    buffer.stop()\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use case"}),": Robot continuously buffers audio. When wake word is detected, save last 5 seconds and transcribe."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-complete-pipeline-with-error-handling",children:"Step 3: Complete Pipeline with Error Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom faster_whisper import WhisperModel\nimport torch\nimport wave\nimport time\nfrom pathlib import Path\n\nclass RobustWhisperSTT:\n    def __init__(self, model_size: str = "small", device: str = "cuda"):\n        # Initialize Whisper\n        self.whisper = WhisperModel(\n            model_size,\n            device=device,\n            compute_type="float16" if device == "cuda" else "int8"\n        )\n\n        # Initialize VAD\n        self.vad_model, utils = torch.hub.load(\n            \'snakers4/silero-vad\', \'silero_vad\', force_reload=False\n        )\n        self.get_speech_timestamps, _, self.read_audio, *_ = utils\n\n        print(f"Whisper STT initialized (model={model_size}, device={device})")\n\n    def has_speech(self, audio_path: str) -> bool:\n        """Check if audio contains speech using VAD"""\n        try:\n            wav = self.read_audio(audio_path)\n            speech_timestamps = self.get_speech_timestamps(\n                wav, self.vad_model, threshold=0.5\n            )\n            return len(speech_timestamps) > 0\n        except Exception as e:\n            print(f"VAD error: {e}")\n            return True  # Assume speech on error (fail-safe)\n\n    def transcribe(self, audio_path: str, language: str = "en") -> dict:\n        """\n        Transcribe audio file with error handling\n\n        Returns:\n            dict: {\n                "success": bool,\n                "text": str,\n                "language": str,\n                "duration": float,\n                "inference_time": float,\n                "error": str | None\n            }\n        """\n        result = {\n            "success": False,\n            "text": "",\n            "language": language,\n            "duration": 0.0,\n            "inference_time": 0.0,\n            "error": None\n        }\n\n        try:\n            # Validate file exists\n            audio_file = Path(audio_path)\n            if not audio_file.exists():\n                raise FileNotFoundError(f"Audio file not found: {audio_path}")\n\n            # Get audio duration\n            with wave.open(str(audio_file), \'rb\') as wf:\n                frames = wf.getnframes()\n                rate = wf.getframerate()\n                result["duration"] = frames / float(rate)\n\n            # Check for speech\n            if not self.has_speech(audio_path):\n                result["error"] = "No speech detected"\n                return result\n\n            # Transcribe with timing\n            start_time = time.time()\n            segments, info = self.whisper.transcribe(\n                audio_path,\n                language=language,\n                beam_size=1,  # Fast decoding\n                vad_filter=True,  # Additional VAD filtering\n                vad_parameters=dict(threshold=0.5)\n            )\n\n            # Combine segments\n            text = " ".join([seg.text.strip() for seg in segments])\n            result["inference_time"] = time.time() - start_time\n\n            # Check for empty transcription\n            if not text.strip():\n                result["error"] = "Empty transcription"\n                return result\n\n            # Success\n            result["success"] = True\n            result["text"] = text\n            result["language"] = info.language\n\n        except FileNotFoundError as e:\n            result["error"] = str(e)\n        except Exception as e:\n            result["error"] = f"Transcription failed: {e}"\n\n        return result\n\n# Usage\nstt = RobustWhisperSTT(model_size="small", device="cuda")\n\n# Transcribe single file\nresult = stt.transcribe("command.wav", language="en")\n\nif result["success"]:\n    print(f"Transcript: {result[\'text\']}")\n    print(f"Inference time: {result[\'inference_time\']:.2f}s")\n    print(f"Real-time factor: {result[\'duration\'] / result[\'inference_time\']:.1f}x")\nelse:\n    print(f"Error: {result[\'error\']}")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-4-ros-2-integration",children:"Step 4: ROS 2 Integration"}),"\n",(0,r.jsx)(n.p,{children:"Create a ROS 2 node for local Whisper:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport wave\nimport tempfile\nfrom robust_whisper_stt import RobustWhisperSTT\n\nclass LocalWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'local_whisper_node\')\n\n        # Initialize Whisper\n        self.stt = RobustWhisperSTT(model_size="small", device="cuda")\n\n        # ROS 2 subscribers and publishers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'audio/chunks\',\n            self.audio_callback,\n            10\n        )\n\n        self.transcript_pub = self.create_publisher(\n            String,\n            \'voice/transcript\',\n            10\n        )\n\n        # Audio buffer\n        self.audio_frames = []\n        self.chunk_duration = 3.0\n        self.sample_rate = 16000\n        self.max_frames = int(self.sample_rate / 1024 * self.chunk_duration)\n\n        self.get_logger().info(\'Local Whisper node started\')\n\n    def audio_callback(self, msg: AudioData):\n        """Accumulate audio and transcribe when buffer is full"""\n        self.audio_frames.append(bytes(msg.data))\n\n        if len(self.audio_frames) >= self.max_frames:\n            self.transcribe_buffer()\n            self.audio_frames.clear()\n\n    def transcribe_buffer(self):\n        """Transcribe accumulated audio"""\n        # Save to temporary WAV file\n        with tempfile.NamedTemporaryFile(suffix=\'.wav\', delete=False) as tmp:\n            with wave.open(tmp.name, \'wb\') as wf:\n                wf.setnchannels(1)\n                wf.setsampwidth(2)\n                wf.setframerate(self.sample_rate)\n                wf.writeframes(b\'\'.join(self.audio_frames))\n\n            # Transcribe\n            result = self.stt.transcribe(tmp.name, language="en")\n\n            if result["success"]:\n                # Publish transcript\n                msg = String()\n                msg.data = result["text"]\n                self.transcript_pub.publish(msg)\n\n                self.get_logger().info(\n                    f\'Transcript: {result["text"]} \'\n                    f\'({result["inference_time"]:.2f}s)\'\n                )\n            else:\n                self.get_logger().warn(f\'Transcription failed: {result["error"]}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LocalWhisperNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-latency-benchmark",children:"Exercise 1: Latency Benchmark"}),"\n",(0,r.jsx)(n.p,{children:"Compare different Whisper configurations:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Test 5 audio files (1s, 3s, 5s, 10s, 30s)"}),"\n",(0,r.jsx)(n.li,{children:"Measure inference time for each model size (tiny, base, small, medium)"}),"\n",(0,r.jsx)(n.li,{children:"Calculate real-time factor (RTF)"}),"\n",(0,r.jsx)(n.li,{children:"Generate comparison table"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Starter code"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nfrom faster_whisper import WhisperModel\n\nmodels = ["tiny", "base", "small", "medium"]\naudio_files = ["1s.wav", "3s.wav", "5s.wav", "10s.wav", "30s.wav"]\n\nresults = {}\n\nfor model_name in models:\n    # TODO: Load model\n    # TODO: For each audio file:\n    #   - Transcribe and measure time\n    #   - Calculate RTF = audio_duration / inference_time\n    # TODO: Store results in dictionary\n    pass\n\n# TODO: Print comparison table\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected output format"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"| Model  | 1s    | 3s    | 5s    | 10s   | 30s   | Avg RTF |\n|--------|-------|-------|-------|-------|-------|---------|\n| tiny   | 24x   | 18x   | 15x   | 12x   | 10x   | 15.8x   |\n| base   | 18x   | 14x   | 12x   | 10x   | 8x    | 12.4x   |\n| small  | 12x   | 10x   | 8x    | 7x    | 6x    | 8.6x    |\n| medium | 6x    | 5x    | 4x    | 3.5x  | 3x    | 4.3x    |\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-hybrid-cloudlocal-system",children:"Exercise 2: Hybrid Cloud/Local System"}),"\n",(0,r.jsx)(n.p,{children:"Implement a system that uses local Whisper for low-latency, falls back to cloud API for failures:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Try local Whisper first (timeout: 2 seconds)"}),"\n",(0,r.jsx)(n.li,{children:"If local fails or times out, use OpenAI API"}),"\n",(0,r.jsx)(n.li,{children:"Log which method was used for each transcription"}),"\n",(0,r.jsx)(n.li,{children:"Track success rate and average latency for each method"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Starter code"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from faster_whisper import WhisperModel\nfrom openai import OpenAI\nimport os\nimport time\n\nclass HybridWhisperSTT:\n    def __init__(self):\n        # TODO: Initialize local Whisper\n        # TODO: Initialize OpenAI client\n        self.stats = {"local": [], "cloud": []}\n\n    def transcribe(self, audio_path: str, timeout: float = 2.0) -> dict:\n        """Try local first, fallback to cloud"""\n        # TODO: Try local Whisper with timeout\n        # TODO: On failure/timeout, use cloud API\n        # TODO: Log which method was used + latency\n        pass\n\n# Usage\nhybrid = HybridWhisperSTT()\nresult = hybrid.transcribe("command.wav")\nprint(f"Method: {result[\'method\']}, Latency: {result[\'latency\']:.2f}s")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"issue-1-cuda-out-of-memory-error",children:'Issue 1: "CUDA out of memory" error'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": Model too large for GPU VRAM"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Use smaller model:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'model = WhisperModel("base")  # Instead of "small"\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Use int8 quantization:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'model = WhisperModel("small", compute_type="int8")\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Use CPU:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'model = WhisperModel("small", device="cpu", compute_type="int8")\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Clear GPU cache:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\ntorch.cuda.empty_cache()\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"issue-2-slow-inference-on-cpu",children:"Issue 2: Slow inference on CPU"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": CPU inference is 10-20x slower than GPU"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Use ",(0,r.jsx)(n.code,{children:"tiny"})," or ",(0,r.jsx)(n.code,{children:"base"})," model (faster on CPU)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Enable int8 quantization:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'model = WhisperModel("tiny", device="cpu", compute_type="int8")\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Reduce beam size:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"segments, _ = model.transcribe(audio, beam_size=1)  # Greedy decoding\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Increase chunk duration (transcribe longer segments less frequently):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"CHUNK_DURATION = 5  # seconds (instead of 3)\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"issue-3-poor-accuracy-on-technical-terms",children:"Issue 3: Poor accuracy on technical terms"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": Whisper not trained on robotics terminology"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Add initial prompt with technical terms:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'segments, _ = model.transcribe(\n    audio,\n    initial_prompt="Robotics commands: ROS 2, Nav2, LiDAR, SLAM, Isaac Sim"\n)\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Post-process transcription with keyword replacement:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'replacements = {\n    "ross too": "ROS 2",\n    "lie dar": "LiDAR",\n    "slam": "SLAM",\n    "nav too": "Nav2"\n}\n\ntranscript = result["text"]\nfor wrong, correct in replacements.items():\n    transcript = transcript.replace(wrong, correct)\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Use cloud API for higher accuracy (lesson 2)"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"issue-4-high-latency-spikes",children:"Issue 4: High latency spikes"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": Model reloading or GPU context switching"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Keep model loaded (don't reload for each transcription):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# GOOD: Load once\nmodel = WhisperModel("small")\nfor audio in audio_files:\n    model.transcribe(audio)\n\n# BAD: Reload every time\nfor audio in audio_files:\n    model = WhisperModel("small")  # Slow!\n    model.transcribe(audio)\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Warm up model with dummy audio:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import numpy as np\n\n# Create 1-second silent audio\ndummy_audio = np.zeros(16000, dtype=np.float32)\nmodel.transcribe(dummy_audio)  # Warmup\n\n# Now real transcriptions will be faster\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Pin to GPU:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\ntorch.cuda.set_device(0)  # Lock to GPU 0\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"optimization-1-batch-processing",children:"Optimization 1: Batch Processing"}),"\n",(0,r.jsx)(n.p,{children:"Process multiple audio files at once:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from faster_whisper import WhisperModel\nimport concurrent.futures\n\nmodel = WhisperModel("small", device="cuda", compute_type="float16")\n\ndef transcribe(audio_path):\n    segments, _ = model.transcribe(audio_path, beam_size=1)\n    return " ".join([s.text for s in segments])\n\n# Process 10 files in parallel (CPU-bound, benefits from threading)\naudio_files = [f"cmd_{i}.wav" for i in range(10)]\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(transcribe, audio_files))\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"optimization-2-streaming-inference",children:"Optimization 2: Streaming Inference"}),"\n",(0,r.jsx)(n.p,{children:"For ultra-low latency, process audio in overlapping windows:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\ndef streaming_transcribe(audio_stream, window_seconds=3, overlap_seconds=1):\n    """\n    Transcribe audio stream with overlapping windows\n\n    Args:\n        audio_stream: Iterator of audio chunks (numpy arrays)\n        window_seconds: Size of transcription window\n        overlap_seconds: Overlap between windows\n    """\n    window_size = 16000 * window_seconds\n    overlap_size = 16000 * overlap_seconds\n\n    buffer = np.array([], dtype=np.float32)\n\n    for chunk in audio_stream:\n        buffer = np.concatenate([buffer, chunk])\n\n        # Process when buffer is full\n        if len(buffer) >= window_size:\n            # Transcribe window\n            segments, _ = model.transcribe(buffer[:window_size])\n            text = " ".join([s.text for s in segments])\n            yield text\n\n            # Slide window (keep overlap)\n            buffer = buffer[window_size - overlap_size:]\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": 3-second windows with 1-second overlap = ~2 seconds average latency"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this lesson, you learned to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Install and run OpenAI Whisper models locally for offline inference"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimize Whisper with faster-whisper (4x speedup) and quantization (int8)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Build complete STT pipeline with VAD, ring buffers, and error handling"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Integrate local Whisper with ROS 2 for real-time robotics"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Compare cloud API vs. local deployment trade-offs"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Benchmark performance across model sizes and hardware"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model selection"}),": ",(0,r.jsx)(n.code,{children:"small"})," is best balance for robotics (RTF ~10x on GPU)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Acceleration"}),": faster-whisper + int8 quantization \u2192 4x speedup"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VAD"}),": Silero VAD skips 90% of silent audio (saves compute)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": Local GPU inference: 100-500ms vs. Cloud API: 500-2000ms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trade-off"}),": Cloud = highest accuracy, Local = lowest latency + offline"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsx)(n.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper GitHub"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/guillaumekln/faster-whisper",children:"faster-whisper GitHub"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/snakers4/silero-vad",children:"Silero VAD"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/OpenNMT/CTranslate2",children:"CTranslate2: Fast Inference Engine"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper#available-models-and-languages",children:"Whisper Model Benchmarks"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/guillaumekln/faster-whisper#benchmark",children:"faster-whisper Benchmarks"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"video-tutorials",children:"Video Tutorials"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=_example",children:"Local Whisper Setup Guide"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=_example",children:"Optimizing Whisper for Edge Devices"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.strong,{children:"Lesson 4: Command Parsing"}),", you'll learn to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Extract intent and entities from voice transcripts"}),"\n",(0,r.jsx)(n.li,{children:"Implement slot filling for robotics commands"}),"\n",(0,r.jsx)(n.li,{children:"Map natural language to ROS 2 actions"}),"\n",(0,r.jsx)(n.li,{children:"Handle ambiguous commands with clarification"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Continue to ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/command-parsing",children:"Command Parsing \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);