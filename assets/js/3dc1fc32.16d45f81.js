"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[6284],{3100:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/ch14-multimodal/object-grounding","title":"Object Grounding: Language to Vision","description":"Learning Objectives","source":"@site/docs/module-4-vla/ch14-multimodal/object-grounding.md","sourceDirName":"module-4-vla/ch14-multimodal","slug":"/module-4-vla/ch14-multimodal/object-grounding","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/object-grounding","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/ch14-multimodal/object-grounding.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action (VLA) Pipeline","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/vla-pipeline"},"next":{"title":"Task Monitoring and Execution Feedback","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/task-monitoring"}}');var o=i(4848),a=i(8453);const r={},s="Object Grounding: Language to Vision",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Visual Grounding with CLIP",id:"visual-grounding-with-clip",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"object-grounding-language-to-vision",children:"Object Grounding: Language to Vision"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Map language descriptions to visual objects"}),"\n",(0,o.jsx)(n.li,{children:"Use CLIP for zero-shot object recognition"}),"\n",(0,o.jsx)(n.li,{children:"Implement spatial reasoning (left/right, on/under)"}),"\n",(0,o.jsx)(n.li,{children:"Handle ambiguous references"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Estimated Time"}),": 3 hours"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"visual-grounding-with-clip",children:"Visual Grounding with CLIP"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport clip\nfrom PIL import Image\n\nclass ObjectGrounder:\n    def __init__(self):\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)\n\n    def find_object(self, image, text_query):\n        """\n        Find object in image matching text description\n\n        Args:\n            image: PIL Image\n            text_query: "red cup", "blue ball", etc.\n\n        Returns:\n            Bounding box and confidence\n        """\n        # Preprocess image\n        image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n\n        # Encode text\n        text_input = clip.tokenize([text_query]).to(self.device)\n\n        # Compute similarity\n        with torch.no_grad():\n            image_features = self.model.encode_image(image_input)\n            text_features = self.model.encode_text(text_input)\n\n            # Cosine similarity\n            similarity = (image_features @ text_features.T).item()\n\n        return similarity\n\n# Usage\ngrounder = ObjectGrounder()\n\nimage = Image.open("scene.jpg")\nscore = grounder.find_object(image, "red cup on the table")\nprint(f"Match confidence: {score:.3f}")\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Continue to ",(0,o.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch14-multimodal/task-monitoring",children:"Task Monitoring \u2192"})]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);