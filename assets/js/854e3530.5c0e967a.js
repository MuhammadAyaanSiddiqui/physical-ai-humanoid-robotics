"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[3142],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}},9726:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-isaac/ch8-perception/depth-estimation","title":"Depth Estimation","description":"Overview","source":"@site/docs/module-3-isaac/ch8-perception/depth-estimation.md","sourceDirName":"module-3-isaac/ch8-perception","slug":"/module-3-isaac/ch8-perception/depth-estimation","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/depth-estimation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ch8-perception/depth-estimation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Object Detection with DNN Inference","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/object-detection"},"next":{"title":"6DOF Pose Estimation","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch8-perception/pose-estimation"}}');var s=i(4848),r=i(8453);const o={},a="Depth Estimation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Part 1: Depth Estimation Fundamentals",id:"part-1-depth-estimation-fundamentals",level:2},{value:"Why Depth Matters for Robotics",id:"why-depth-matters-for-robotics",level:3},{value:"Stereo vs. Monocular Depth",id:"stereo-vs-monocular-depth",level:3},{value:"Part 2: Stereo Depth Estimation",id:"part-2-stereo-depth-estimation",level:2},{value:"How Stereo Depth Works",id:"how-stereo-depth-works",level:3},{value:"Step 1: Setup Stereo Camera in Isaac Sim",id:"step-1-setup-stereo-camera-in-isaac-sim",level:3},{value:"Step 2: Install Isaac ROS Stereo Depth",id:"step-2-install-isaac-ros-stereo-depth",level:3},{value:"Step 3: Run Stereo Depth Node",id:"step-3-run-stereo-depth-node",level:3},{value:"Step 4: Visualize Depth in RViz",id:"step-4-visualize-depth-in-rviz",level:3},{value:"Part 3: Monocular Depth Estimation",id:"part-3-monocular-depth-estimation",level:2},{value:"Step 1: Install Isaac ROS Depth Segmentation",id:"step-1-install-isaac-ros-depth-segmentation",level:3},{value:"Step 2: Download Pre-trained MiDaS Model",id:"step-2-download-pre-trained-midas-model",level:3},{value:"Step 3: Run Monocular Depth Node",id:"step-3-run-monocular-depth-node",level:3},{value:"Part 4: 3D Point Cloud Generation",id:"part-4-3d-point-cloud-generation",level:2},{value:"Using ROS 2 depth_image_proc",id:"using-ros-2-depth_image_proc",level:3},{value:"Python Script for Point Cloud Processing",id:"python-script-for-point-cloud-processing",level:3},{value:"Part 5: Combining Depth + Object Detection",id:"part-5-combining-depth--object-detection",level:2},{value:"Step 1: Synchronize Detection and Depth",id:"step-1-synchronize-detection-and-depth",level:3},{value:"Step 2: Convert to 3D World Coordinates",id:"step-2-convert-to-3d-world-coordinates",level:3},{value:"Part 6: Hands-On Exercise",id:"part-6-hands-on-exercise",level:2},{value:"Exercise: 3D Object Localization",id:"exercise-3d-object-localization",level:3},{value:"Requirements",id:"requirements",level:4},{value:"Deliverables",id:"deliverables",level:4},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"depth-estimation",children:"Depth Estimation"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Depth estimation"}),' enables robots to perceive 3D structure from 2D images, answering "how far away is each pixel?" This is critical for navigation (obstacle avoidance), manipulation (grasp planning), and scene understanding. We\'ll explore both ',(0,s.jsx)(n.strong,{children:"stereo depth"})," (using two cameras like human eyes) and ",(0,s.jsx)(n.strong,{children:"monocular depth"})," (single camera with deep learning)."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Stereo depth estimation principles and algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Monocular depth estimation with neural networks"}),"\n",(0,s.jsx)(n.li,{children:"Configure Isaac ROS depth nodes"}),"\n",(0,s.jsx)(n.li,{children:"Generate 3D point clouds from RGB-D data"}),"\n",(0,s.jsx)(n.li,{children:"Combine depth with object detection for 3D localization"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed: Object Detection (previous lesson)"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of camera geometry"}),"\n",(0,s.jsx)(n.li,{children:"Basic linear algebra (transformations, projections)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Estimated Time"}),": 3 hours"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Explain stereo triangulation and disparity maps"}),"\n",(0,s.jsx)(n.li,{children:"Run stereo depth estimation in Isaac ROS"}),"\n",(0,s.jsx)(n.li,{children:"Use monocular depth networks (MiDaS, DPT)"}),"\n",(0,s.jsx)(n.li,{children:"Generate and visualize 3D point clouds"}),"\n",(0,s.jsx)(n.li,{children:"Fuse depth with object detection for 3D bounding boxes"}),"\n",(0,s.jsx)(n.li,{children:"Optimize depth estimation for real-time performance"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-1-depth-estimation-fundamentals",children:"Part 1: Depth Estimation Fundamentals"}),"\n",(0,s.jsx)(n.h3,{id:"why-depth-matters-for-robotics",children:"Why Depth Matters for Robotics"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Task"}),(0,s.jsx)(n.th,{children:"Why Depth is Needed"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Obstacle Avoidance"})}),(0,s.jsx)(n.td,{children:"Know distance to obstacles for safe navigation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Grasping"})}),(0,s.jsx)(n.td,{children:"Calculate 3D position of objects for inverse kinematics"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"3D Mapping"})}),(0,s.jsx)(n.td,{children:"Build volumetric maps (octomap) for path planning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Human Detection"})}),(0,s.jsx)(n.td,{children:"Measure distance to people for safe interaction"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"stereo-vs-monocular-depth",children:"Stereo vs. Monocular Depth"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Method"}),(0,s.jsx)(n.th,{children:"Accuracy"}),(0,s.jsx)(n.th,{children:"Speed"}),(0,s.jsx)(n.th,{children:"Hardware Cost"}),(0,s.jsx)(n.th,{children:"Outdoor Performance"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Stereo"})}),(0,s.jsx)(n.td,{children:"High (mm precision)"}),(0,s.jsx)(n.td,{children:"Fast (30+ FPS)"}),(0,s.jsx)(n.td,{children:"Medium ($100-300)"}),(0,s.jsx)(n.td,{children:"Excellent"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Monocular"})}),(0,s.jsx)(n.td,{children:"Medium (10-20cm error)"}),(0,s.jsx)(n.td,{children:"Medium (15-30 FPS)"}),(0,s.jsx)(n.td,{children:"Low ($50)"}),(0,s.jsx)(n.td,{children:"Good"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"LiDAR"})}),(0,s.jsx)(n.td,{children:"Very High (<1mm)"}),(0,s.jsx)(n.td,{children:"Fast (10-30 FPS)"}),(0,s.jsx)(n.td,{children:"High ($500-$10K)"}),(0,s.jsx)(n.td,{children:"Excellent"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"For this course"}),": We'll cover both stereo (primary) and monocular (fallback when stereo unavailable)."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-2-stereo-depth-estimation",children:"Part 2: Stereo Depth Estimation"}),"\n",(0,s.jsx)(n.h3,{id:"how-stereo-depth-works",children:"How Stereo Depth Works"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Principle"}),": Objects closer to cameras appear at different horizontal positions in left vs. right images. This offset (",(0,s.jsx)(n.strong,{children:"disparity"}),") reveals depth."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Mathematics"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Depth (meters) = (Baseline \xd7 Focal Length) / Disparity\n\nWhere:\n- Baseline: Distance between left/right cameras (e.g., 0.12m)\n- Focal Length: Camera focal length in pixels (e.g., 700px)\n- Disparity: Horizontal pixel offset between left/right images\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Baseline = 0.12m"}),"\n",(0,s.jsx)(n.li,{children:"Focal Length = 700px"}),"\n",(0,s.jsx)(n.li,{children:"Object appears at pixel 400 (left), pixel 380 (right) \u2192 Disparity = 20px"}),"\n",(0,s.jsxs)(n.li,{children:["Depth = (0.12 \xd7 700) / 20 = ",(0,s.jsx)(n.strong,{children:"4.2 meters"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-setup-stereo-camera-in-isaac-sim",children:"Step 1: Setup Stereo Camera in Isaac Sim"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Launch Isaac Sim"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Load warehouse scene"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add ",(0,s.jsx)(n.strong,{children:"Carter robot"})," (has built-in stereo camera)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Left camera: ",(0,s.jsx)(n.code,{children:"/front_stereo_camera/left/image_raw"})]}),"\n",(0,s.jsxs)(n.li,{children:["Right camera: ",(0,s.jsx)(n.code,{children:"/front_stereo_camera/right/image_raw"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Enable ROS 2 Bridge for stereo topics:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# In Isaac Sim Script Editor\nimport omni.isaac.ros2_bridge as ros_bridge\n\nros_bridge.enable_topic('/front_stereo_camera/left/image_raw')\nros_bridge.enable_topic('/front_stereo_camera/right/image_raw')\nros_bridge.enable_topic('/front_stereo_camera/left/camera_info')\nros_bridge.enable_topic('/front_stereo_camera/right/camera_info')\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-2-install-isaac-ros-stereo-depth",children:"Step 2: Install Isaac ROS Stereo Depth"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_image_pipeline.git\ncd ~/ros2_ws\nrosdep install --from-paths src --ignore-src -r -y\ncolcon build --packages-select isaac_ros_stereo_image_proc\nsource install/setup.bash\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-run-stereo-depth-node",children:"Step 3: Run Stereo Depth Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_ros_stereo_image_proc isaac_ros_stereo_image_pipeline.launch.py \\\n  left_image_topic:=/front_stereo_camera/left/image_raw \\\n  right_image_topic:=/front_stereo_camera/right/image_raw \\\n  left_camera_info_topic:=/front_stereo_camera/left/camera_info \\\n  right_camera_info_topic:=/front_stereo_camera/right/camera_info \\\n  output_topic:=/depth_image\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output Topics"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/depth_image"})," - Depth map (16-bit, distance in mm)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/disparity_image"})," - Disparity map (visualization)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/point_cloud"})," - 3D point cloud"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-4-visualize-depth-in-rviz",children:"Step 4: Visualize Depth in RViz"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"rviz2\n"})}),"\n",(0,s.jsx)(n.p,{children:"Add displays:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image"})," \u2192 Topic: ",(0,s.jsx)(n.code,{children:"/depth_image"}),' (set color scheme to "Rainbow")']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PointCloud2"})," \u2192 Topic: ",(0,s.jsx)(n.code,{children:"/point_cloud"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You should see:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Depth image: Closer objects appear red, farther objects blue"}),"\n",(0,s.jsx)(n.li,{children:"Point cloud: 3D visualization of the scene"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-3-monocular-depth-estimation",children:"Part 3: Monocular Depth Estimation"}),"\n",(0,s.jsx)(n.p,{children:"When stereo cameras aren't available, use monocular depth estimation (single camera + deep learning)."}),"\n",(0,s.jsx)(n.h3,{id:"step-1-install-isaac-ros-depth-segmentation",children:"Step 1: Install Isaac ROS Depth Segmentation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_depth_segmentation.git\ncd ~/ros2_ws\nrosdep install --from-paths src --ignore-src -r -y\ncolcon build --packages-select isaac_ros_bi3d isaac_ros_depth_segmentation\nsource install/setup.bash\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-download-pre-trained-midas-model",children:"Step 2: Download Pre-trained MiDaS Model"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"MiDaS"})," is a state-of-the-art monocular depth network."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/models/midas\ncd ~/models/midas\n\n# Download MiDaS v3.0 (DPT-Hybrid model)\nwget https://github.com/isl-org/MiDaS/releases/download/v3_0/dpt_hybrid-midas-501f0c75.pt\n\n# Convert to TensorRT (optional for GPU acceleration)\npython3 convert_midas_to_trt.py dpt_hybrid-midas-501f0c75.pt\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-run-monocular-depth-node",children:"Step 3: Run Monocular Depth Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_ros_depth_segmentation isaac_ros_depth_segmentation.launch.py \\\n  model_file_path:=~/models/midas/dpt_hybrid-midas-501f0c75.engine \\\n  input_image_topic:=/front_camera/image_raw \\\n  output_topic:=/monocular_depth\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance"}),": 15-25 FPS on RTX 4070 Ti"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-4-3d-point-cloud-generation",children:"Part 4: 3D Point Cloud Generation"}),"\n",(0,s.jsx)(n.p,{children:"Depth maps are 2D (image-like). Convert to 3D point clouds for robotics applications."}),"\n",(0,s.jsx)(n.h3,{id:"using-ros-2-depth_image_proc",children:"Using ROS 2 depth_image_proc"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install depth image processing tools\nsudo apt install ros-humble-depth-image-proc\n\n# Launch point cloud converter\nros2 run depth_image_proc point_cloud_xyz_node \\\n  --ros-args \\\n  --remap depth/image_rect:=/depth_image \\\n  --remap depth/camera_info:=/front_stereo_camera/left/camera_info \\\n  --remap points:=/point_cloud_xyz\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": ",(0,s.jsx)(n.code,{children:"/point_cloud_xyz"})," contains ",(0,s.jsx)(n.code,{children:"sensor_msgs/PointCloud2"})," messages"]}),"\n",(0,s.jsx)(n.h3,{id:"python-script-for-point-cloud-processing",children:"Python Script for Point Cloud Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom sensor_msgs.msg import PointCloud2\nimport sensor_msgs_py.point_cloud2 as pc2\n\nclass PointCloudProcessor:\n    def __init__(self):\n        self.node = rclpy.create_node(\'pc_processor\')\n        self.sub = self.node.create_subscription(\n            PointCloud2,\n            \'/point_cloud_xyz\',\n            self.callback,\n            10\n        )\n\n    def callback(self, msg):\n        # Convert ROS PointCloud2 to list of (x, y, z)\n        points = list(pc2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True))\n\n        # Filter points within range (e.g., 0.5m to 5m)\n        filtered = [(x, y, z) for x, y, z in points if 0.5 < z < 5.0]\n\n        print(f"Received {len(points)} points, filtered to {len(filtered)}")\n\n        # Find closest obstacle\n        if filtered:\n            closest = min(filtered, key=lambda p: p[2])  # Minimum Z distance\n            print(f"Closest obstacle at: {closest[2]:.2f}m")\n\nrclpy.init()\nprocessor = PointCloudProcessor()\nrclpy.spin(processor.node)\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-5-combining-depth--object-detection",children:"Part 5: Combining Depth + Object Detection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),': Get 3D position of detected objects (e.g., "Apple is 1.2m away at 30\xb0 right").']}),"\n",(0,s.jsx)(n.h3,{id:"step-1-synchronize-detection-and-depth",children:"Step 1: Synchronize Detection and Depth"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from message_filters import ApproximateTimeSynchronizer, Subscriber\nfrom vision_msgs.msg import Detection2DArray\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass Detection3D:\n    def __init__(self):\n        self.node = rclpy.create_node('detection_3d')\n        self.bridge = CvBridge()\n\n        # Subscribe to detections and depth (synchronized)\n        det_sub = Subscriber(self.node, Detection2DArray, '/detections')\n        depth_sub = Subscriber(self.node, Image, '/depth_image')\n\n        self.sync = ApproximateTimeSynchronizer(\n            [det_sub, depth_sub],\n            queue_size=10,\n            slop=0.1  # 100ms tolerance\n        )\n        self.sync.registerCallback(self.callback)\n\n    def callback(self, det_msg, depth_msg):\n        # Convert depth image to numpy array\n        depth_image = self.bridge.imgmsg_to_cv2(depth_msg, desired_encoding='32FC1')\n\n        for det in det_msg.detections:\n            # Get bounding box center\n            cx = int(det.bbox.center.x)\n            cy = int(det.bbox.center.y)\n\n            # Sample depth at bbox center\n            depth_meters = depth_image[cy, cx]\n\n            # Get class and confidence\n            class_id = det.results[0].id\n            confidence = det.results[0].score\n\n            print(f\"Detected object {class_id} at depth {depth_meters:.2f}m (confidence: {confidence:.2f})\")\n\nrclpy.init()\nnode = Detection3D()\nrclpy.spin(node.node)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-convert-to-3d-world-coordinates",children:"Step 2: Convert to 3D World Coordinates"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def pixel_to_3d(pixel_x, pixel_y, depth_meters, camera_info):\n    """\n    Convert pixel + depth to 3D point in camera frame.\n\n    Args:\n        pixel_x, pixel_y: Pixel coordinates\n        depth_meters: Depth at that pixel\n        camera_info: CameraInfo message (contains intrinsics)\n\n    Returns:\n        (x, y, z) in meters (camera frame)\n    """\n    # Extract camera intrinsics\n    fx = camera_info.k[0]  # Focal length X\n    fy = camera_info.k[4]  # Focal length Y\n    cx = camera_info.k[2]  # Principal point X\n    cy = camera_info.k[5]  # Principal point Y\n\n    # Unproject pixel to 3D\n    z = depth_meters\n    x = (pixel_x - cx) * z / fx\n    y = (pixel_y - cy) * z / fy\n\n    return (x, y, z)\n\n# Usage\ncamera_info = ...  # Receive from /front_camera/camera_info\nbbox_center_x = det.bbox.center.x\nbbox_center_y = det.bbox.center.y\ndepth = depth_image[int(bbox_center_y), int(bbox_center_x)]\n\nx, y, z = pixel_to_3d(bbox_center_x, bbox_center_y, depth, camera_info)\nprint(f"Object 3D position: ({x:.2f}, {y:.2f}, {z:.2f}) meters")\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-6-hands-on-exercise",children:"Part 6: Hands-On Exercise"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3d-object-localization",children:"Exercise: 3D Object Localization"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Detect objects and report their 3D positions for robot grasping."]}),"\n",(0,s.jsx)(n.h4,{id:"requirements",children:"Requirements"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scene"}),": Tabletop with 5 objects (apples, mugs)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Detection"}),": Run YOLOv8 object detector (from previous lesson)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth"}),": Run stereo depth estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion"}),": Combine detection + depth to get 3D positions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": For each detected object, print:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Apple: (x=0.45m, y=-0.12m, z=0.82m) - confidence: 0.93\nMug: (x=0.67m, y=0.23m, z=0.79m) - confidence: 0.88\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"deliverables",children:"Deliverables"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Python script: ",(0,s.jsx)(n.code,{children:"object_3d_localization.py"})]}),"\n",(0,s.jsxs)(n.li,{children:["RViz screenshot showing:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RGB image with 2D bounding boxes"}),"\n",(0,s.jsx)(n.li,{children:"Point cloud with objects visible"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Accuracy test: Manually measure 3 objects' distances, compare with depth estimates (error <10cm)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Estimated Time"}),": 2 hours"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this lesson, you learned:"}),"\n",(0,s.jsx)(n.p,{children:"\u2705 Stereo depth estimation via triangulation\n\u2705 Monocular depth estimation with neural networks (MiDaS)\n\u2705 3D point cloud generation from depth maps\n\u2705 Combining depth with object detection for 3D localization\n\u2705 Real-time depth processing with Isaac ROS"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo depth"})," provides high accuracy for robotics applications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monocular depth"})," is a viable fallback with single cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth + detection"})," enables 3D scene understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS"})," GPU acceleration achieves real-time performance"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["In the next lesson (",(0,s.jsx)(n.strong,{children:"Pose Estimation"}),"), you'll learn:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"6DOF pose estimation (position + orientation)"}),"\n",(0,s.jsx)(n.li,{children:"Object pose from RGB-D data"}),"\n",(0,s.jsx)(n.li,{children:"Grasp pose generation for manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Pose tracking across frames"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo Vision Tutorial"}),": ",(0,s.jsx)(n.a,{href:"https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html",children:"https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MiDaS Paper"}),": ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1907.01341",children:"https://arxiv.org/abs/1907.01341"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Depth"}),": ",(0,s.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_depth_segmentation/index.html",children:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_depth_segmentation/index.html"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Point Cloud Processing"}),": ",(0,s.jsx)(n.a,{href:"http://www.open3d.org/docs/latest/tutorial/geometry/pointcloud.html",children:"http://www.open3d.org/docs/latest/tutorial/geometry/pointcloud.html"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"3D Vision Fundamentals"}),": Multiple View Geometry in Computer Vision (Hartley & Zisserman)"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Real-World Example",type:"tip",children:(0,s.jsx)(n.p,{children:"Self-driving cars use stereo depth (and LiDAR) to measure distances to pedestrians, vehicles, and obstacles in real-time at 30+ FPS. The same techniques work for humanoid robot navigation!"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);