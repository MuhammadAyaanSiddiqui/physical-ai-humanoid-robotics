"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[8784],{6538:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-3-isaac/ch10-rl/policy-training","title":"Policy Training with Isaac Gym & RL","description":"Overview","source":"@site/docs/module-3-isaac/ch10-rl/policy-training.md","sourceDirName":"module-3-isaac/ch10-rl","slug":"/module-3-isaac/ch10-rl/policy-training","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch10-rl/policy-training","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ch10-rl/policy-training.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Behavior Trees for Navigation","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch9-navigation/behavior-trees"},"next":{"title":"Reward Function Design","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch10-rl/reward-design"}}');var s=i(4848),a=i(8453);const t={},l="Policy Training with Isaac Gym & RL",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Part 1: RL Fundamentals",id:"part-1-rl-fundamentals",level:2},{value:"Key Concepts",id:"key-concepts",level:3},{value:"PPO (Proximal Policy Optimization)",id:"ppo-proximal-policy-optimization",level:3},{value:"Part 2: Isaac Gym Setup",id:"part-2-isaac-gym-setup",level:2},{value:"Installation",id:"installation",level:3},{value:"Verify Installation",id:"verify-installation",level:3},{value:"Part 3: Training Humanoid Locomotion",id:"part-3-training-humanoid-locomotion",level:2},{value:"Task: Walk Forward",id:"task-walk-forward",level:3},{value:"Training Script",id:"training-script",level:3},{value:"Training Monitoring",id:"training-monitoring",level:3},{value:"Part 4: Hyperparameter Tuning",id:"part-4-hyperparameter-tuning",level:2},{value:"Key Parameters",id:"key-parameters",level:3},{value:"Common Issues",id:"common-issues",level:3},{value:"Part 5: Deploying Trained Policy",id:"part-5-deploying-trained-policy",level:2},{value:"Export Policy",id:"export-policy",level:3},{value:"Load and Run Policy",id:"load-and-run-policy",level:3},{value:"Summary",id:"summary",level:2},{value:"Resources",id:"resources",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"policy-training-with-isaac-gym--rl",children:"Policy Training with Isaac Gym & RL"})}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Reinforcement Learning (RL)"})," enables robots to learn complex behaviors through trial-and-error in simulation. Isaac Gym provides GPU-accelerated parallel environments where thousands of robots train simultaneously, learning policies (control strategies) for locomotion, manipulation, and navigation."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"RL fundamentals (PPO, SAC algorithms)"}),"\n",(0,s.jsx)(e.li,{children:"Train policies in Isaac Gym for humanoid locomotion"}),"\n",(0,s.jsx)(e.li,{children:"Configure reward functions for desired behaviors"}),"\n",(0,s.jsx)(e.li,{children:"Monitor training progress with TensorBoard"}),"\n",(0,s.jsx)(e.li,{children:"Export trained policies for deployment"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Prerequisites"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Python and PyTorch basics"}),"\n",(0,s.jsx)(e.li,{children:"Understanding of neural networks"}),"\n",(0,s.jsx)(e.li,{children:"Completed Isaac Sim chapters"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Estimated Time"}),": 4 hours"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Understand RL concepts (states, actions, rewards, policies)"}),"\n",(0,s.jsx)(e.li,{children:"Set up Isaac Gym training environment"}),"\n",(0,s.jsx)(e.li,{children:"Train locomotion policy with PPO"}),"\n",(0,s.jsx)(e.li,{children:"Tune hyperparameters for stable training"}),"\n",(0,s.jsx)(e.li,{children:"Deploy trained policy in Isaac Sim"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"part-1-rl-fundamentals",children:"Part 1: RL Fundamentals"}),"\n",(0,s.jsx)(e.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Concept"}),(0,s.jsx)(e.th,{children:"Description"}),(0,s.jsx)(e.th,{children:"Example"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"State"})}),(0,s.jsx)(e.td,{children:"Robot's observation of environment"}),(0,s.jsx)(e.td,{children:"Joint angles, velocities, IMU"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Action"})}),(0,s.jsx)(e.td,{children:"Robot's control output"}),(0,s.jsx)(e.td,{children:"Joint torques"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Reward"})}),(0,s.jsx)(e.td,{children:"Feedback signal (+good, -bad)"}),(0,s.jsx)(e.td,{children:"+1 for forward progress, -10 for falling"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Policy"})}),(0,s.jsx)(e.td,{children:"Strategy mapping states \u2192 actions"}),(0,s.jsx)(e.td,{children:"Neural network"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Value Function"})}),(0,s.jsx)(e.td,{children:"Expected cumulative reward from state"}),(0,s.jsx)(e.td,{children:"Estimate total future reward"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"ppo-proximal-policy-optimization",children:"PPO (Proximal Policy Optimization)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Best for"}),": Robotics (stable, sample-efficient)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Algorithm"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Collect trajectories (robot episodes)"}),"\n",(0,s.jsx)(e.li,{children:"Compute advantages (how good actions were)"}),"\n",(0,s.jsx)(e.li,{children:"Update policy via gradient ascent"}),"\n",(0,s.jsx)(e.li,{children:"Clip updates to prevent large changes (stability)"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"part-2-isaac-gym-setup",children:"Part 2: Isaac Gym Setup"}),"\n",(0,s.jsx)(e.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Download Isaac Gym (NVIDIA Developer account required)\nwget https://developer.nvidia.com/isaac-gym\n\n# Extract and install\ncd isaacgym/python\npip install -e .\n"})}),"\n",(0,s.jsx)(e.h3,{id:"verify-installation",children:"Verify Installation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import isaacgym\nfrom isaacgym import gymapi\n\ngym = gymapi.acquire_gym()\nprint("Isaac Gym initialized successfully!")\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"part-3-training-humanoid-locomotion",children:"Part 3: Training Humanoid Locomotion"}),"\n",(0,s.jsx)(e.h3,{id:"task-walk-forward",children:"Task: Walk Forward"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Goal"}),": Train humanoid to walk forward without falling"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"State"})," (observations):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Base linear/angular velocity (6D)"}),"\n",(0,s.jsx)(e.li,{children:"Joint positions/velocities (24D)"}),"\n",(0,s.jsxs)(e.li,{children:["Gravity vector (3D)\n",(0,s.jsx)(e.strong,{children:"Total"}),": 33D observation space"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Actions"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Target joint positions (12D)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Reward Function"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def compute_reward(self):\n    # Positive rewards\n    forward_velocity_reward = self.base_lin_vel[0]  # Reward forward motion\n    alive_reward = 1.0  # Reward staying upright\n\n    # Negative penalties\n    energy_penalty = -0.01 * torch.sum(torch.square(self.torques))\n    orientation_penalty = -torch.sum(torch.square(self.projected_gravity[:, :2]))\n\n    total_reward = forward_velocity_reward + alive_reward + energy_penalty + orientation_penalty\n    return total_reward\n"})}),"\n",(0,s.jsx)(e.h3,{id:"training-script",children:"Training Script"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from rl_games.torch_runner import Runner\nfrom isaacgymenvs.tasks import isaacgym_task_map\n\n# Load humanoid environment\ntask = isaacgym_task_map["Humanoid"]()\n\n# Configure PPO\nconfig = {\n    "params": {\n        "algo": {\n            "name": "a2c_continuous"\n        },\n        "network": {\n            "name": "actor_critic",\n            "separate": False,\n            "space": {\n                "continuous": {\n                    "mu_activation": "None",\n                    "sigma_activation": "None",\n                    "mu_init": {"name": "default"},\n                    "sigma_init": {"name": "const_initializer", "val": 0},\n                    "fixed_sigma": True\n                }\n            },\n            "mlp": {\n                "units": [256, 128, 64],\n                "activation": "elu",\n                "d2rl": False,\n                "initializer": {"name": "default"}\n            }\n        },\n        "config": {\n            "name": "Humanoid",\n            "env_name": "rlgpu",\n            "multi_gpu": False,\n            "ppo": True,\n            "mixed_precision": True,\n            "normalize_input": True,\n            "normalize_value": True,\n            "value_bootstrap": True,\n            "num_actors": 4096,  # Parallel environments\n            "reward_shaper": {},\n            "normalize_advantage": True,\n            "gamma": 0.99,\n            "tau": 0.95,\n            "learning_rate": 3e-4,\n            "lr_schedule": "adaptive",\n            "kl_threshold": 0.008,\n            "score_to_win": 20000,\n            "max_epochs": 2000,\n            "save_best_after": 100,\n            "save_frequency": 50,\n            "grad_norm": 1.0,\n            "entropy_coef": 0.0,\n            "truncate_grads": True,\n            "e_clip": 0.2,\n            "horizon_length": 16,\n            "minibatch_size": 32768,\n            "mini_epochs": 5,\n            "critic_coef": 4,\n            "clip_value": True,\n            "seq_len": 4,\n            "bounds_loss_coef": 0.0001\n        }\n    }\n}\n\n# Train\nrunner = Runner()\nrunner.load(config)\nrunner.reset()\nrunner.run({})\n'})}),"\n",(0,s.jsx)(e.h3,{id:"training-monitoring",children:"Training Monitoring"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Launch TensorBoard\ntensorboard --logdir=./runs/Humanoid\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Metrics to watch"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"ep_len_mean"}),": Episode length (longer = better)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"ep_rew_mean"}),": Average reward (should increase)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"approxkl"}),": KL divergence (should stay <0.015 for stability)"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Training Time"}),": 1-2 hours on RTX 4070 Ti (2000 epochs, 4096 parallel envs)"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"part-4-hyperparameter-tuning",children:"Part 4: Hyperparameter Tuning"}),"\n",(0,s.jsx)(e.h3,{id:"key-parameters",children:"Key Parameters"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Parameter"}),(0,s.jsx)(e.th,{children:"Effect"}),(0,s.jsx)(e.th,{children:"Tuning Tips"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.code,{children:"learning_rate"})}),(0,s.jsx)(e.td,{children:"Update step size"}),(0,s.jsx)(e.td,{children:"Start 3e-4, reduce if unstable"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.code,{children:"num_actors"})}),(0,s.jsx)(e.td,{children:"Parallel environments"}),(0,s.jsx)(e.td,{children:"More = faster, needs more GPU memory"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.code,{children:"horizon_length"})}),(0,s.jsx)(e.td,{children:"Trajectory length"}),(0,s.jsx)(e.td,{children:"8-32 typical for locomotion"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.code,{children:"gamma"})}),(0,s.jsx)(e.td,{children:"Discount factor"}),(0,s.jsx)(e.td,{children:"0.99 standard (values future rewards)"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.code,{children:"entropy_coef"})}),(0,s.jsx)(e.td,{children:"Exploration bonus"}),(0,s.jsx)(e.td,{children:"0.0-0.01 (higher = more exploration)"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Issue"}),": Policy collapses (reward drops suddenly)\n",(0,s.jsx)(e.strong,{children:"Solution"}),": Reduce ",(0,s.jsx)(e.code,{children:"learning_rate"})," to 1e-4, increase ",(0,s.jsx)(e.code,{children:"kl_threshold"})]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Issue"}),": Training plateaus\n",(0,s.jsx)(e.strong,{children:"Solution"}),": Increase ",(0,s.jsx)(e.code,{children:"entropy_coef"})," for more exploration"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"part-5-deploying-trained-policy",children:"Part 5: Deploying Trained Policy"}),"\n",(0,s.jsx)(e.h3,{id:"export-policy",children:"Export Policy"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Save checkpoint\ntorch.save({\n    'model_state_dict': agent.model.state_dict(),\n    'optimizer_state_dict': agent.optimizer.state_dict(),\n}, 'humanoid_policy.pth')\n"})}),"\n",(0,s.jsx)(e.h3,{id:"load-and-run-policy",children:"Load and Run Policy"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch\n\n# Load trained model\npolicy = ActorCritic(obs_dim=33, action_dim=12)\npolicy.load_state_dict(torch.load('humanoid_policy.pth')['model_state_dict'])\npolicy.eval()\n\n# Run in Isaac Sim\nwith torch.no_grad():\n    action = policy(observation)\n    robot.apply_action(action)\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"\u2705 Trained humanoid locomotion with PPO in Isaac Gym\n\u2705 Configured reward functions for walking behavior\n\u2705 Monitored training with TensorBoard\n\u2705 Deployed trained policy"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Next"}),": Reward function design for custom tasks."]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"resources",children:"Resources"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Isaac Gym"}),": ",(0,s.jsx)(e.a,{href:"https://developer.nvidia.com/isaac-gym",children:"https://developer.nvidia.com/isaac-gym"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RL Games"}),": ",(0,s.jsx)(e.a,{href:"https://github.com/Denys88/rl_games",children:"https://github.com/Denys88/rl_games"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"PPO Paper"}),": ",(0,s.jsx)(e.a,{href:"https://arxiv.org/abs/1707.06347",children:"https://arxiv.org/abs/1707.06347"})]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>l});var r=i(6540);const s={},a=r.createContext(s);function t(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);