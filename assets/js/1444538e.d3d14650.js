"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[4473],{4674:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vla/ch12-llm-planning/prompt-engineering","title":"Prompt Engineering for Robotics Tasks","description":"Learning Objectives","source":"@site/docs/module-4-vla/ch12-llm-planning/prompt-engineering.md","sourceDirName":"module-4-vla/ch12-llm-planning","slug":"/module-4-vla/ch12-llm-planning/prompt-engineering","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/prompt-engineering","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/ch12-llm-planning/prompt-engineering.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LLM Integration for Cognitive Robot Planning","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/llm-integration"},"next":{"title":"Action Generation and ROS 2 Conversion","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/action-generation"}}');var o=t(4848),s=t(8453);const r={},a="Prompt Engineering for Robotics Tasks",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Part 1: Basic Prompt Structure",id:"part-1-basic-prompt-structure",level:2},{value:"Step 1: System Prompt Template",id:"step-1-system-prompt-template",level:3},{value:"Step 2: User Prompt Template",id:"step-2-user-prompt-template",level:3},{value:"Part 2: Few-Shot Learning",id:"part-2-few-shot-learning",level:2},{value:"Step 1: Few-Shot Examples for Consistent Output",id:"step-1-few-shot-examples-for-consistent-output",level:3},{value:"Step 2: Testing Few-Shot vs. Zero-Shot",id:"step-2-testing-few-shot-vs-zero-shot",level:3},{value:"Part 3: Chain-of-Thought Reasoning",id:"part-3-chain-of-thought-reasoning",level:2},{value:"Step 1: Enable Step-by-Step Reasoning",id:"step-1-enable-step-by-step-reasoning",level:3},{value:"Step 2: Safety Constraint Checking",id:"step-2-safety-constraint-checking",level:3},{value:"Part 4: Contextual Prompting",id:"part-4-contextual-prompting",level:2},{value:"Step 1: Environment-Aware Planning",id:"step-1-environment-aware-planning",level:3},{value:"Step 2: Dynamic Constraint Injection",id:"step-2-dynamic-constraint-injection",level:3},{value:"Part 5: Prompt Optimization",id:"part-5-prompt-optimization",level:2},{value:"Step 1: Token Reduction Techniques",id:"step-1-token-reduction-techniques",level:3},{value:"Step 2: Prompt Caching (For Repeated Queries)",id:"step-2-prompt-caching-for-repeated-queries",level:3},{value:"Part 6: Domain-Specific Prompt Libraries",id:"part-6-domain-specific-prompt-libraries",level:2},{value:"Step 1: Task-Specific Templates",id:"step-1-task-specific-templates",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercise 1: Prompt A/B Testing",id:"exercise-1-prompt-ab-testing",level:3},{value:"Exercise 2: Safety Validator",id:"exercise-2-safety-validator",level:3},{value:"Summary",id:"summary",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Official Guides",id:"official-guides",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Prompt Libraries",id:"prompt-libraries",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"prompt-engineering-for-robotics-tasks",children:"Prompt Engineering for Robotics Tasks"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Design effective prompts for robotics planning tasks"}),"\n",(0,o.jsx)(e.li,{children:"Use few-shot learning to improve LLM output quality"}),"\n",(0,o.jsx)(e.li,{children:"Implement chain-of-thought reasoning for complex tasks"}),"\n",(0,o.jsx)(e.li,{children:"Create reusable prompt templates for common scenarios"}),"\n",(0,o.jsx)(e.li,{children:"Handle edge cases and constraint specification"}),"\n",(0,o.jsx)(e.li,{children:"Optimize prompts for cost and latency"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Required Knowledge"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["Completion of ",(0,o.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/llm-integration",children:"Lesson 1: LLM Integration"})]}),"\n",(0,o.jsx)(e.li,{children:"Understanding of robotics action primitives"}),"\n",(0,o.jsx)(e.li,{children:"JSON formatting"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Required Software"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"OpenAI or Anthropic API access"}),"\n",(0,o.jsx)(e.li,{children:"Python 3.10+"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Estimated Time"}),": 2-3 hours"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Effective prompt engineering is critical for reliable robot behavior. Poor prompts lead to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Inconsistent output formats"}),"\n",(0,o.jsx)(e.li,{children:"Missed safety constraints"}),"\n",(0,o.jsx)(e.li,{children:"Inefficient action plans"}),"\n",(0,o.jsx)(e.li,{children:"Hallucinated capabilities"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Key Principles"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Be specific"}),": Define exact output format and constraints"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Use examples"}),": Show desired behavior with few-shot learning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Add context"}),": Provide robot capabilities and environment state"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Think step-by-step"}),": Use chain-of-thought for complex reasoning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Validate output"}),": Check constraints before execution"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"part-1-basic-prompt-structure",children:"Part 1: Basic Prompt Structure"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-system-prompt-template",children:"Step 1: System Prompt Template"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'SYSTEM_PROMPT = """You are a planning assistant for a humanoid robot with the following capabilities:\n\nNAVIGATION:\n- move(direction, distance): Move forward/backward (max 10m)\n- turn(direction, angle): Rotate left/right (0-180\xb0)\n- go_to(location): Navigate to named location\n\nMANIPULATION:\n- pick(object, color): Grasp object\n- place(object, location): Put object down\n- release(): Open gripper\n\nPERCEPTION:\n- find(object): Locate object in environment\n- scan(): Get list of visible objects\n\nCONSTRAINTS:\n- Maximum payload: 5kg\n- Cannot pick multiple objects simultaneously\n- Must find object before picking\n- Must be at location before placing\n\nOUTPUT FORMAT:\nReturn valid JSON with this structure:\n{\n  "plan": [\n    {"action": "move", "direction": "forward", "distance": 2.0},\n    {"action": "pick", "object": "cube", "color": "red"}\n  ],\n  "reasoning": "Brief explanation of plan"\n}\n"""\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h3,{id:"step-2-user-prompt-template",children:"Step 2: User Prompt Template"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"def create_user_prompt(command: str, environment: dict) -> str:\n    \"\"\"Generate user prompt with command and environment state\"\"\"\n    return f\"\"\"\nCOMMAND: {command}\n\nCURRENT STATE:\n- Robot location: {environment.get('location', 'unknown')}\n- Visible objects: {', '.join(environment.get('visible_objects', []))}\n- Gripper: {'holding ' + environment.get('held_object', '') if environment.get('held_object') else 'empty'}\n\nGenerate an action plan to complete the command.\n\"\"\"\n\n# Usage\nenv = {\n    'location': 'living_room',\n    'visible_objects': ['red_cube', 'blue_ball', 'table'],\n    'held_object': None\n}\n\nuser_prompt = create_user_prompt(\"Pick up the red cube\", env)\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"part-2-few-shot-learning",children:"Part 2: Few-Shot Learning"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-few-shot-examples-for-consistent-output",children:"Step 1: Few-Shot Examples for Consistent Output"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'FEW_SHOT_EXAMPLES = """\nExample 1:\nCOMMAND: Move forward 3 meters\nOUTPUT:\n{\n  "plan": [\n    {"action": "move", "direction": "forward", "distance": 3.0}\n  ],\n  "reasoning": "Simple forward movement within 10m limit"\n}\n\nExample 2:\nCOMMAND: Go to the kitchen and pick up the red cup\nOUTPUT:\n{\n  "plan": [\n    {"action": "go_to", "location": "kitchen"},\n    {"action": "find", "object": "cup"},\n    {"action": "pick", "object": "cup", "color": "red"}\n  ],\n  "reasoning": "Navigate to kitchen, locate cup visually, then grasp it"\n}\n\nExample 3:\nCOMMAND: Pick up the cube and place it on the table\nOUTPUT:\n{\n  "plan": [\n    {"action": "find", "object": "cube"},\n    {"action": "pick", "object": "cube"},\n    {"action": "go_to", "location": "table"},\n    {"action": "place", "object": "cube", "location": "table"},\n    {"action": "release"}\n  ],\n  "reasoning": "Find cube first, pick it up, navigate to table, place and release"\n}\n\nNow process the following command:\n"""\n\n# Combine with system prompt\ndef create_few_shot_prompt(command: str) -> str:\n    messages = [\n        {"role": "system", "content": SYSTEM_PROMPT},\n        {"role": "user", "content": FEW_SHOT_EXAMPLES + f"\\nCOMMAND: {command}"}\n    ]\n    return messages\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h3,{id:"step-2-testing-few-shot-vs-zero-shot",children:"Step 2: Testing Few-Shot vs. Zero-Shot"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom openai import OpenAI\nimport json\n\nclient = OpenAI()\n\ndef zero_shot_plan(command: str) -> dict:\n    """Generate plan without examples"""\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {"role": "system", "content": SYSTEM_PROMPT},\n            {"role": "user", "content": f"COMMAND: {command}"}\n        ],\n        response_format={"type": "json_object"},\n        temperature=0.0\n    )\n    return json.loads(response.choices[0].message.content)\n\ndef few_shot_plan(command: str) -> dict:\n    """Generate plan with examples"""\n    messages = create_few_shot_prompt(command)\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=messages,\n        response_format={"type": "json_object"},\n        temperature=0.0\n    )\n    return json.loads(response.choices[0].message.content)\n\n# Compare\ncommand = "Bring me the blue ball from the kitchen"\n\nprint("=== ZERO-SHOT ===")\nprint(json.dumps(zero_shot_plan(command), indent=2))\n\nprint("\\n=== FEW-SHOT ===")\nprint(json.dumps(few_shot_plan(command), indent=2))\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Typical improvement"}),": Few-shot reduces missing steps (e.g., forgetting ",(0,o.jsx)(e.code,{children:"find"})," before ",(0,o.jsx)(e.code,{children:"pick"}),") by ~70%."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"part-3-chain-of-thought-reasoning",children:"Part 3: Chain-of-Thought Reasoning"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-enable-step-by-step-reasoning",children:"Step 1: Enable Step-by-Step Reasoning"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'COT_SYSTEM_PROMPT = """You are a planning assistant for a humanoid robot.\n\nWhen given a command, think through it step-by-step:\n1. What is the goal?\n2. What is the current state?\n3. What actions are needed?\n4. Are there any constraints violated?\n5. Generate the final plan\n\nBe explicit about your reasoning before outputting the plan.\n"""\n\ndef chain_of_thought_plan(command: str, environment: dict) -> dict:\n    """Generate plan with chain-of-thought reasoning"""\n    messages = [\n        {"role": "system", "content": COT_SYSTEM_PROMPT},\n        {"role": "user", "content": f"""\nCOMMAND: {command}\n\nENVIRONMENT:\n{json.dumps(environment, indent=2)}\n\nThink step-by-step, then provide your final plan in JSON format.\n"""}\n    ]\n\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=messages,\n        temperature=0.0\n    )\n\n    # Extract reasoning and plan\n    content = response.choices[0].message.content\n\n    # Parse JSON from response (may need extraction if mixed with text)\n    import re\n    json_match = re.search(r\'\\{.*\\}\', content, re.DOTALL)\n    if json_match:\n        plan = json.loads(json_match.group())\n        return {\n            "reasoning": content[:json_match.start()].strip(),\n            "plan": plan\n        }\n\n    return {"reasoning": content, "plan": None}\n\n# Usage\nenv = {\n    \'location\': \'bedroom\',\n    \'visible_objects\': [\'lamp\', \'book\'],\n    \'held_object\': None\n}\n\nresult = chain_of_thought_plan("Turn off the lamp in the living room", env)\nprint("REASONING:", result[\'reasoning\'])\nprint("\\nPLAN:", json.dumps(result[\'plan\'], indent=2))\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h3,{id:"step-2-safety-constraint-checking",children:"Step 2: Safety Constraint Checking"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'SAFETY_PROMPT = """Before generating a plan, verify these safety constraints:\n\nSAFETY RULES:\n1. Never exceed 10m movement in single action\n2. Never pick objects > 5kg (check object database)\n3. Must release object before picking new one\n4. Cannot navigate while holding fragile objects\n5. Must confirm object exists before manipulation\n\nIf command violates constraints, respond with:\n{\n  "error": "constraint_violation",\n  "violated_rule": "<rule number>",\n  "suggestion": "<alternative approach>"\n}\n\nOtherwise, provide the action plan.\n"""\n\ndef safe_plan(command: str, environment: dict) -> dict:\n    """Generate plan with safety validation"""\n    messages = [\n        {"role": "system", "content": SYSTEM_PROMPT + "\\n\\n" + SAFETY_PROMPT},\n        {"role": "user", "content": create_user_prompt(command, environment)}\n    ]\n\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=messages,\n        response_format={"type": "json_object"},\n        temperature=0.0\n    )\n\n    result = json.loads(response.choices[0].message.content)\n\n    if "error" in result:\n        print(f"\u26a0\ufe0f Safety violation: {result[\'violated_rule\']}")\n        print(f"Suggestion: {result[\'suggestion\']}")\n        return None\n\n    return result\n\n# Test with unsafe command\nenv = {\'location\': \'warehouse\', \'held_object\': \'box_20kg\'}\nunsafe_result = safe_plan("Pick up the engine block", env)  # Violates 5kg limit\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"part-4-contextual-prompting",children:"Part 4: Contextual Prompting"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-environment-aware-planning",children:"Step 1: Environment-Aware Planning"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"def create_contextual_prompt(command: str, context: dict) -> str:\n    \"\"\"Generate prompt with rich environmental context\"\"\"\n\n    # Build context description\n    context_desc = f\"\"\"\nROBOT STATE:\n- Position: {context['robot']['position']}\n- Orientation: {context['robot']['orientation']}\xb0\n- Battery: {context['robot']['battery']}%\n- Gripper: {'occupied' if context['robot']['holding'] else 'empty'}\n\nENVIRONMENT:\n- Room: {context['environment']['room']}\n- Visible objects: {len(context['environment']['objects'])} items\n  {chr(10).join([f\"  \u2022 {obj['name']} ({obj['color']}, {obj['distance']:.1f}m away)\" for obj in context['environment']['objects']])}\n- Obstacles: {', '.join(context['environment']['obstacles'])}\n\nTASK HISTORY (last 3 actions):\n{chr(10).join([f\"  {i+1}. {action}\" for i, action in enumerate(context['history'][-3:])])}\n\nCOMMAND: {command}\n\nGenerate an efficient plan considering the current context.\n\"\"\"\n    return context_desc\n\n# Rich context example\ncontext = {\n    'robot': {\n        'position': {'x': 2.5, 'y': 1.0},\n        'orientation': 90,\n        'battery': 75,\n        'holding': None\n    },\n    'environment': {\n        'room': 'kitchen',\n        'objects': [\n            {'name': 'cup', 'color': 'red', 'distance': 1.2},\n            {'name': 'plate', 'color': 'white', 'distance': 2.5}\n        ],\n        'obstacles': ['table', 'chair']\n    },\n    'history': [\n        'move(forward, 1.0)',\n        'turn(left, 90)',\n        'scan()'\n    ]\n}\n\nprompt = create_contextual_prompt(\"Pick up the red cup\", context)\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h3,{id:"step-2-dynamic-constraint-injection",children:"Step 2: Dynamic Constraint Injection"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'def add_dynamic_constraints(base_prompt: str, constraints: list) -> str:\n    """Add runtime constraints to prompt"""\n\n    constraint_text = "\\nADDITIONAL CONSTRAINTS:\\n"\n    for i, constraint in enumerate(constraints, 1):\n        constraint_text += f"{i}. {constraint}\\n"\n\n    return base_prompt + constraint_text\n\n# Example: Add time constraint\nconstraints = [\n    "Complete task in under 30 seconds (assume 2s per action)",\n    "Avoid moving near the window (fragile items)",\n    "Prioritize battery conservation (minimize distance)"\n]\n\nenhanced_prompt = add_dynamic_constraints(SYSTEM_PROMPT, constraints)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"part-5-prompt-optimization",children:"Part 5: Prompt Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-token-reduction-techniques",children:"Step 1: Token Reduction Techniques"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# VERBOSE (expensive)\nVERBOSE_PROMPT = """\nYou are an advanced artificial intelligence system designed to assist with planning and control of humanoid robotic systems. Your primary function is to interpret natural language commands provided by human operators and translate them into structured sequences of low-level robotic actions...\n"""\n\n# CONCISE (cost-effective)\nCONCISE_PROMPT = """\nRobot planner. Convert commands to action sequences.\n\nActions: move(dir,dist), turn(dir,angle), pick(obj,color), place(obj,loc)\nConstraints: 10m max move, 5kg max load\nOutput: JSON action array\n"""\n\n# Token comparison\nimport tiktoken\n\nenc = tiktoken.encoding_for_model("gpt-4o-mini")\nprint(f"Verbose: {len(enc.encode(VERBOSE_PROMPT))} tokens")\nprint(f"Concise: {len(enc.encode(CONCISE_PROMPT))} tokens")\n# Output: Verbose: 87 tokens, Concise: 34 tokens (60% reduction)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h3,{id:"step-2-prompt-caching-for-repeated-queries",children:"Step 2: Prompt Caching (For Repeated Queries)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class PromptCache:\n    """Cache system prompts to reduce costs"""\n\n    def __init__(self):\n        self.system_prompt = SYSTEM_PROMPT  # Cached, sent once\n        self.conversation_history = []\n\n    def add_user_message(self, command: str):\n        """Add user command to conversation"""\n        self.conversation_history.append({\n            "role": "user",\n            "content": command\n        })\n\n    def get_messages(self) -> list:\n        """Get full message history with cached system prompt"""\n        return [\n            {"role": "system", "content": self.system_prompt},\n            *self.conversation_history\n        ]\n\n    def add_assistant_response(self, response: str):\n        """Add assistant response to history"""\n        self.conversation_history.append({\n            "role": "assistant",\n            "content": response\n        })\n\n# Usage (reuse system prompt across multiple commands)\ncache = PromptCache()\n\ncommands = ["Move forward", "Turn left", "Pick up cube"]\n\nfor cmd in commands:\n    cache.add_user_message(cmd)\n\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=cache.get_messages()\n    )\n\n    cache.add_assistant_response(response.choices[0].message.content)\n\n# System prompt sent only once, saving tokens\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"part-6-domain-specific-prompt-libraries",children:"Part 6: Domain-Specific Prompt Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-task-specific-templates",children:"Step 1: Task-Specific Templates"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'PROMPT_LIBRARY = {\n    "navigation": {\n        "system": """Navigation specialist. Plan efficient paths.\nActions: move, turn, go_to\nOptimize for: shortest path, obstacle avoidance""",\n\n        "examples": [\n            {\n                "command": "Go to the kitchen",\n                "output": {"plan": [{"action": "go_to", "location": "kitchen"}]}\n            }\n        ]\n    },\n\n    "manipulation": {\n        "system": """Manipulation specialist. Plan pick-and-place tasks.\nActions: find, pick, place, release\nConstraints: 5kg max, must find before pick""",\n\n        "examples": [\n            {\n                "command": "Pick up the red cube",\n                "output": {"plan": [\n                    {"action": "find", "object": "cube"},\n                    {"action": "pick", "object": "cube", "color": "red"}\n                ]}\n            }\n        ]\n    },\n\n    "compound": {\n        "system": """Multi-task planner. Combine navigation and manipulation.\nAll actions available. Break complex tasks into steps.""",\n\n        "examples": [\n            {\n                "command": "Bring me the cup from the kitchen",\n                "output": {"plan": [\n                    {"action": "go_to", "location": "kitchen"},\n                    {"action": "find", "object": "cup"},\n                    {"action": "pick", "object": "cup"},\n                    {"action": "go_to", "location": "user"},\n                    {"action": "place", "object": "cup", "location": "user"}\n                ]}\n            }\n        ]\n    }\n}\n\ndef select_prompt_template(command: str) -> dict:\n    """Select appropriate prompt based on command type"""\n\n    # Simple keyword matching (could use classifier)\n    if any(word in command.lower() for word in [\'move\', \'go\', \'navigate\']):\n        return PROMPT_LIBRARY[\'navigation\']\n    elif any(word in command.lower() for word in [\'pick\', \'grab\', \'place\']):\n        return PROMPT_LIBRARY[\'manipulation\']\n    else:\n        return PROMPT_LIBRARY[\'compound\']\n\n# Usage\ncommand = "Pick up the red cube"\ntemplate = select_prompt_template(command)\nprint(f"Using template: {template[\'system\']}")\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,o.jsx)(e.h3,{id:"exercise-1-prompt-ab-testing",children:"Exercise 1: Prompt A/B Testing"}),"\n",(0,o.jsx)(e.p,{children:"Compare different prompt strategies:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Test 3 prompt variants (zero-shot, few-shot, chain-of-thought)"}),"\n",(0,o.jsx)(e.li,{children:"Use 10 test commands"}),"\n",(0,o.jsx)(e.li,{children:"Measure: success rate, avg tokens, action count"}),"\n",(0,o.jsx)(e.li,{children:"Identify best strategy for each command type"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Starter code"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"test_commands = [\n    \"Move forward 5 meters\",\n    \"Go to the kitchen and find the cup\",\n    \"Pick up all red objects\",  # Edge case\n    # ... 7 more\n]\n\nresults = {\n    'zero_shot': [],\n    'few_shot': [],\n    'chain_of_thought': []\n}\n\nfor cmd in test_commands:\n    # TODO: Test each strategy\n    # TODO: Validate output format\n    # TODO: Count tokens and actions\n    # TODO: Record success/failure\n    pass\n\n# TODO: Generate comparison report\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h3,{id:"exercise-2-safety-validator",children:"Exercise 2: Safety Validator"}),"\n",(0,o.jsx)(e.p,{children:"Build a prompt that checks plans for safety violations:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Input: Generated action plan (JSON)"}),"\n",(0,o.jsx)(e.li,{children:"Output: List of safety violations (if any)"}),"\n",(0,o.jsx)(e.li,{children:"Check: distance limits, weight limits, object dependencies"}),"\n",(0,o.jsx)(e.li,{children:"Suggest fixes for violations"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Starter code"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'VALIDATOR_PROMPT = """\nYou are a safety validator for robot action plans.\n\nCheck this plan for violations:\n{plan}\n\nRULES:\n1. Max move distance: 10m\n2. Max object weight: 5kg\n3. Must find before pick\n4. Must pick before place\n\nOutput format:\n{\n  "valid": true/false,\n  "violations": [\n    {"rule": 1, "action_index": 2, "fix": "suggestion"}\n  ]\n}\n"""\n\ndef validate_plan(plan: dict) -> dict:\n    # TODO: Format validator prompt\n    # TODO: Call LLM\n    # TODO: Parse validation result\n    pass\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"In this lesson, you learned to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"\u2705 Structure effective prompts with clear instructions and constraints"}),"\n",(0,o.jsx)(e.li,{children:"\u2705 Use few-shot learning to improve output consistency"}),"\n",(0,o.jsx)(e.li,{children:"\u2705 Implement chain-of-thought reasoning for complex tasks"}),"\n",(0,o.jsx)(e.li,{children:"\u2705 Create contextual prompts with environment state"}),"\n",(0,o.jsx)(e.li,{children:"\u2705 Optimize prompts for cost (token reduction)"}),"\n",(0,o.jsx)(e.li,{children:"\u2705 Build reusable prompt libraries for common tasks"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Few-shot examples"})," reduce errors by 70% vs. zero-shot"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Chain-of-thought"})," improves safety validation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Concise prompts"})," save 50-60% on token costs"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Domain-specific templates"})," improve quality for specialized tasks"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,o.jsx)(e.h3,{id:"official-guides",children:"Official Guides"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/prompt-engineering",children:"OpenAI Prompt Engineering Guide"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://docs.anthropic.com/claude/prompt-library",children:"Anthropic Prompt Library"})}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2201.11903",children:"Chain-of-Thought Prompting"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2005.14165",children:"Few-Shot Learning with Language Models"})}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"prompt-libraries",children:"Prompt Libraries"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://github.com/f/awesome-chatgpt-prompts",children:"Awesome ChatGPT Prompts"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://python.langchain.com/docs/modules/model_io/prompts/",children:"LangChain Prompt Templates"})}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,o.jsxs)(e.p,{children:["In ",(0,o.jsx)(e.strong,{children:"Lesson 3: Action Generation"}),", you'll learn to:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Convert LLM outputs to ROS 2 action messages"}),"\n",(0,o.jsx)(e.li,{children:"Validate and sanitize generated plans"}),"\n",(0,o.jsx)(e.li,{children:"Handle execution failures and replanning"}),"\n",(0,o.jsx)(e.li,{children:"Monitor action execution with feedback loops"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:["Continue to ",(0,o.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/action-generation",children:"Action Generation \u2192"})]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);