"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[5931],{3575:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>t,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"module-3-isaac/ch10-rl/reward-design","title":"Reward Function Design","description":"Overview","source":"@site/docs/module-3-isaac/ch10-rl/reward-design.md","sourceDirName":"module-3-isaac/ch10-rl","slug":"/module-3-isaac/ch10-rl/reward-design","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch10-rl/reward-design","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ch10-rl/reward-design.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Policy Training with Isaac Gym & RL","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch10-rl/policy-training"},"next":{"title":"Sim-to-Real Transfer","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ch10-rl/sim-to-real"}}');var i=r(4848),a=r(8453);const l={},d="Reward Function Design",t={},o=[{value:"Overview",id:"overview",level:2},{value:"Part 1: Reward Shaping Principles",id:"part-1-reward-shaping-principles",level:2},{value:"Sparse vs. Dense Rewards",id:"sparse-vs-dense-rewards",level:3},{value:"Multi-Component Rewards",id:"multi-component-rewards",level:3},{value:"Part 2: Common Reward Hacking",id:"part-2-common-reward-hacking",level:2},{value:"Issue 1: Spinning in Place",id:"issue-1-spinning-in-place",level:3},{value:"Issue 2: Tip-Toeing",id:"issue-2-tip-toeing",level:3},{value:"Part 3: Reward Components",id:"part-3-reward-components",level:2},{value:"Distance-Based",id:"distance-based",level:3},{value:"Velocity-Based",id:"velocity-based",level:3},{value:"Orientation-Based",id:"orientation-based",level:3},{value:"Contact-Based",id:"contact-based",level:3},{value:"Part 4: Debugging Rewards",id:"part-4-debugging-rewards",level:2},{value:"Logging Reward Components",id:"logging-reward-components",level:3},{value:"Visualization",id:"visualization",level:3},{value:"Summary",id:"summary",level:2},{value:"Resources",id:"resources",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"reward-function-design",children:"Reward Function Design"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Reward engineering"})," is critical for RL success. A well-designed reward function guides the policy to desired behaviors while avoiding unintended shortcuts. This lesson covers principles, common pitfalls, and techniques for crafting effective reward functions."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"What You'll Learn"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Reward shaping principles"}),"\n",(0,i.jsx)(n.li,{children:"Avoid reward hacking and degenerate solutions"}),"\n",(0,i.jsx)(n.li,{children:"Design multi-objective rewards"}),"\n",(0,i.jsx)(n.li,{children:"Debug reward functions with logging"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Estimated Time"}),": 2 hours"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-1-reward-shaping-principles",children:"Part 1: Reward Shaping Principles"}),"\n",(0,i.jsx)(n.h3,{id:"sparse-vs-dense-rewards",children:"Sparse vs. Dense Rewards"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Sparse"}),": Reward only at goal (e.g., +100 for reaching target, 0 otherwise)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pros"}),": Simple, well-defined"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cons"}),": Hard to learn (rare positive signal)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Dense"}),": Reward every step (e.g., -distance_to_goal)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pros"}),": Faster learning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cons"}),": Risk of reward hacking"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"multi-component-rewards",children:"Multi-Component Rewards"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def compute_reward(self):\n    # Primary objective\n    forward_reward = 2.0 * self.base_lin_vel[0]\n\n    # Secondary objectives\n    upright_reward = 1.0 * (self.projected_gravity[2] > 0.9)\n    energy_penalty = -0.001 * torch.sum(torch.square(self.torques))\n    alive_bonus = 0.5\n\n    total = forward_reward + upright_reward + energy_penalty + alive_bonus\n    return total\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-2-common-reward-hacking",children:"Part 2: Common Reward Hacking"}),"\n",(0,i.jsx)(n.h3,{id:"issue-1-spinning-in-place",children:"Issue 1: Spinning in Place"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Bad Reward"}),": ",(0,i.jsx)(n.code,{children:"reward = angular_velocity"}),"\n",(0,i.jsx)(n.strong,{children:"Hack"}),": Robot spins rapidly (high reward, no forward progress)\n",(0,i.jsx)(n.strong,{children:"Fix"}),": ",(0,i.jsx)(n.code,{children:"reward = forward_velocity - 0.1 * abs(angular_velocity)"})]}),"\n",(0,i.jsx)(n.h3,{id:"issue-2-tip-toeing",children:"Issue 2: Tip-Toeing"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Bad Reward"}),": ",(0,i.jsx)(n.code,{children:"reward = height"}),"\n",(0,i.jsx)(n.strong,{children:"Hack"}),": Robot balances on toes (high center of mass)\n",(0,i.jsx)(n.strong,{children:"Fix"}),": ",(0,i.jsx)(n.code,{children:"reward = height * (contact_area > threshold)"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-3-reward-components",children:"Part 3: Reward Components"}),"\n",(0,i.jsx)(n.h3,{id:"distance-based",children:"Distance-Based"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"distance_to_goal = torch.norm(self.root_pos[:, :2] - goal_pos, dim=1)\nreward = -distance_to_goal  # Minimize distance\n"})}),"\n",(0,i.jsx)(n.h3,{id:"velocity-based",children:"Velocity-Based"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"desired_vel = 1.0  # m/s\nactual_vel = self.base_lin_vel[0]\nreward = -abs(desired_vel - actual_vel)  # Match desired speed\n"})}),"\n",(0,i.jsx)(n.h3,{id:"orientation-based",children:"Orientation-Based"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Penalize tilting\ngravity_z = self.projected_gravity[:, 2]  # Should be ~1.0 when upright\nreward = gravity_z  # Reward staying vertical\n"})}),"\n",(0,i.jsx)(n.h3,{id:"contact-based",children:"Contact-Based"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Penalize excessive foot slippage\nfoot_slip = torch.norm(self.foot_velocities_xy, dim=1)\nreward = -0.1 * foot_slip\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-4-debugging-rewards",children:"Part 4: Debugging Rewards"}),"\n",(0,i.jsx)(n.h3,{id:"logging-reward-components",children:"Logging Reward Components"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"self.extras['forward_reward'] = forward_reward.mean()\nself.extras['energy_penalty'] = energy_penalty.mean()\nself.extras['alive_bonus'] = alive_bonus.mean()\n\n# View in TensorBoard\n"})}),"\n",(0,i.jsx)(n.h3,{id:"visualization",children:"Visualization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Plot reward over episode\nimport matplotlib.pyplot as plt\n\nplt.plot(episode_rewards)\nplt.xlabel('Timestep')\nplt.ylabel('Reward')\nplt.title('Reward Progress')\nplt.show()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"\u2705 Designed multi-component reward functions\n\u2705 Avoided reward hacking with constraints\n\u2705 Debugged rewards with logging"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": Sim-to-real transfer strategies."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reward Engineering"}),": ",(0,i.jsx)(n.a,{href:"https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html",children:"https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>d});var s=r(6540);const i={},a=s.createContext(i);function l(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);