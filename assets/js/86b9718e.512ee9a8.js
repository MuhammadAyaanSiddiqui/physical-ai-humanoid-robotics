"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[3999],{3773:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vla/ch12-llm-planning/llm-integration","title":"LLM Integration for Cognitive Robot Planning","description":"Learning Objectives","source":"@site/docs/module-4-vla/ch12-llm-planning/llm-integration.md","sourceDirName":"module-4-vla/ch12-llm-planning","slug":"/module-4-vla/ch12-llm-planning/llm-integration","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/llm-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/ch12-llm-planning/llm-integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice Command Parsing: Intent and Slot Extraction","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/command-parsing"},"next":{"title":"Prompt Engineering for Robotics Tasks","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/prompt-engineering"}}');var i=t(4848),o=t(8453);const r={},l="LLM Integration for Cognitive Robot Planning",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Part 1: OpenAI GPT-4 Setup",id:"part-1-openai-gpt-4-setup",level:2},{value:"Step 1: Create API Key",id:"step-1-create-api-key",level:3},{value:"Step 2: Install OpenAI SDK",id:"step-2-install-openai-sdk",level:3},{value:"Step 3: Test API Connection",id:"step-3-test-api-connection",level:3},{value:"Part 2: Anthropic Claude Setup",id:"part-2-anthropic-claude-setup",level:2},{value:"Step 1: Create API Key",id:"step-1-create-api-key-1",level:3},{value:"Step 2: Install Anthropic SDK",id:"step-2-install-anthropic-sdk",level:3},{value:"Step 3: Test API Connection",id:"step-3-test-api-connection-1",level:3},{value:"Part 3: Structured Output Generation",id:"part-3-structured-output-generation",level:2},{value:"Step 1: JSON Mode (GPT-4 only)",id:"step-1-json-mode-gpt-4-only",level:3},{value:"Step 2: Function Calling (Structured Tool Use)",id:"step-2-function-calling-structured-tool-use",level:3},{value:"Step 3: Prompt Engineering for Structured Output (Claude)",id:"step-3-prompt-engineering-for-structured-output-claude",level:3},{value:"Part 4: ROS 2 Integration",id:"part-4-ros-2-integration",level:2},{value:"Step 1: LLM Planning Service",id:"step-1-llm-planning-service",level:3},{value:"Step 2: LLM Planning Node",id:"step-2-llm-planning-node",level:3},{value:"Step 3: Test LLM Planning Service",id:"step-3-test-llm-planning-service",level:3},{value:"Part 5: Error Handling and Optimization",id:"part-5-error-handling-and-optimization",level:2},{value:"Step 1: Retry Logic with Exponential Backoff",id:"step-1-retry-logic-with-exponential-backoff",level:3},{value:"Step 2: Timeout Handling",id:"step-2-timeout-handling",level:3},{value:"Step 3: Cost Tracking",id:"step-3-cost-tracking",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercise 1: Multi-LLM Comparison",id:"exercise-1-multi-llm-comparison",level:3},{value:"Exercise 2: Caching for Repeated Commands",id:"exercise-2-caching-for-repeated-commands",level:3},{value:"Summary",id:"summary",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Cost Optimization",id:"cost-optimization",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"llm-integration-for-cognitive-robot-planning",children:"LLM Integration for Cognitive Robot Planning"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Set up OpenAI GPT-4 and Anthropic Claude APIs for robot planning"}),"\n",(0,i.jsx)(e.li,{children:"Compare LLM capabilities for robotics tasks (GPT-4, Claude, Llama)"}),"\n",(0,i.jsx)(e.li,{children:"Implement structured output generation (JSON mode, function calling)"}),"\n",(0,i.jsx)(e.li,{children:"Handle API rate limits, timeouts, and error recovery"}),"\n",(0,i.jsx)(e.li,{children:"Integrate LLMs with ROS 2 for real-time decision making"}),"\n",(0,i.jsx)(e.li,{children:"Optimize costs and latency for production deployments"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Required Knowledge"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Python programming (async/await, JSON)"}),"\n",(0,i.jsx)(e.li,{children:"REST API concepts"}),"\n",(0,i.jsx)(e.li,{children:"ROS 2 basics (topics, services, actions)"}),"\n",(0,i.jsxs)(e.li,{children:["Completion of ",(0,i.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/audio-capture",children:"Chapter 11: Voice Recognition"})]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Required Accounts"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"OpenAI API key (GPT-4 access) OR"}),"\n",(0,i.jsx)(e.li,{children:"Anthropic API key (Claude access)"}),"\n",(0,i.jsx)(e.li,{children:"(Optional) Hugging Face account for open-source models"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Required Software"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Python 3.10+"}),"\n",(0,i.jsxs)(e.li,{children:["OpenAI Python SDK (",(0,i.jsx)(e.code,{children:"openai>=1.0.0"}),")"]}),"\n",(0,i.jsxs)(e.li,{children:["Anthropic Python SDK (",(0,i.jsx)(e.code,{children:"anthropic>=0.8.0"}),")"]}),"\n",(0,i.jsx)(e.li,{children:"ROS 2 Humble"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Estimated Time"}),": 3-4 hours"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Large Language Models (LLMs) enable robots to understand complex, free-form natural language commands and generate multi-step action plans. Unlike rule-based parsers (Chapter 11), LLMs can:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Handle ambiguity"}),': "Go get me something to drink" \u2192 Plan to find kitchen, locate beverages, select appropriate item']}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-step planning"}),': "Clean the room" \u2192 Pick up objects, place in correct locations, vacuum floor']}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Contextual reasoning"}),': "Is it safe to move forward?" \u2192 Analyze sensor data, evaluate risks, provide explanation']}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error recovery"}),": If action fails, propose alternative approaches"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"LLM Options for Robotics"}),":"]}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Model"}),(0,i.jsx)(e.th,{children:"Provider"}),(0,i.jsx)(e.th,{children:"Strengths"}),(0,i.jsx)(e.th,{children:"Weaknesses"}),(0,i.jsx)(e.th,{children:"Cost (1M tokens)"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"GPT-4 Turbo"})}),(0,i.jsx)(e.td,{children:"OpenAI"}),(0,i.jsx)(e.td,{children:"Best reasoning, JSON mode"}),(0,i.jsx)(e.td,{children:"Expensive, rate limits"}),(0,i.jsx)(e.td,{children:"$10 (input) / $30 (output)"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Claude 3.5 Sonnet"})}),(0,i.jsx)(e.td,{children:"Anthropic"}),(0,i.jsx)(e.td,{children:"Long context (200k), fast"}),(0,i.jsx)(e.td,{children:"No JSON mode"}),(0,i.jsx)(e.td,{children:"$3 (input) / $15 (output)"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"GPT-4o mini"})}),(0,i.jsx)(e.td,{children:"OpenAI"}),(0,i.jsx)(e.td,{children:"Fast, cheap, good quality"}),(0,i.jsx)(e.td,{children:"Less capable than GPT-4"}),(0,i.jsx)(e.td,{children:"$0.15 (input) / $0.60 (output)"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Llama 3 70B"})}),(0,i.jsx)(e.td,{children:"Meta"}),(0,i.jsx)(e.td,{children:"Free (self-hosted), privacy"}),(0,i.jsx)(e.td,{children:"Requires GPU, slower"}),(0,i.jsx)(e.td,{children:"Free (hardware cost)"})]})]})]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Recommended for this course"}),": GPT-4o mini (best cost/performance for learning) or Claude 3.5 Sonnet (best quality/cost for production)."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"part-1-openai-gpt-4-setup",children:"Part 1: OpenAI GPT-4 Setup"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-api-key",children:"Step 1: Create API Key"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["Sign up at ",(0,i.jsx)(e.a,{href:"https://platform.openai.com/",children:"platform.openai.com"})]}),"\n",(0,i.jsxs)(e.li,{children:["Navigate to ",(0,i.jsx)(e.strong,{children:"API Keys"})," \u2192 ",(0,i.jsx)(e.strong,{children:"Create new secret key"})]}),"\n",(0,i.jsxs)(e.li,{children:["Name: ",(0,i.jsx)(e.code,{children:"robotics-planning"})]}),"\n",(0,i.jsxs)(e.li,{children:["Permissions: ",(0,i.jsx)(e.code,{children:"GPT-4"})," access"]}),"\n",(0,i.jsxs)(e.li,{children:["Copy key: ",(0,i.jsx)(e.code,{children:"sk-proj-..."})]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Secure storage"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'echo "OPENAI_API_KEY=sk-proj-..." >> .env\necho ".env" >> .gitignore  # Never commit API keys!\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-2-install-openai-sdk",children:"Step 2: Install OpenAI SDK"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"pip3 install openai python-dotenv\n\n# Verify installation\npython3 -c \"import openai; print(f'OpenAI SDK v{openai.__version__}')\"\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-3-test-api-connection",children:"Step 3: Test API Connection"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n# Test with a simple prompt\nresponse = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {"role": "system", "content": "You are a helpful robotics assistant."},\n        {"role": "user", "content": "Explain what ROS 2 is in one sentence."}\n    ],\n    max_tokens=100,\n    temperature=0.0  # Deterministic output\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected output"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"ROS 2 is a flexible, modular robotics middleware framework that provides tools and libraries for building robot applications with support for real-time communication, hardware abstraction, and distributed systems.\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"part-2-anthropic-claude-setup",children:"Part 2: Anthropic Claude Setup"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-create-api-key-1",children:"Step 1: Create API Key"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["Sign up at ",(0,i.jsx)(e.a,{href:"https://console.anthropic.com/",children:"console.anthropic.com"})]}),"\n",(0,i.jsxs)(e.li,{children:["Navigate to ",(0,i.jsx)(e.strong,{children:"API Keys"})," \u2192 ",(0,i.jsx)(e.strong,{children:"Create Key"})]}),"\n",(0,i.jsxs)(e.li,{children:["Name: ",(0,i.jsx)(e.code,{children:"robotics-planning"})]}),"\n",(0,i.jsxs)(e.li,{children:["Copy key: ",(0,i.jsx)(e.code,{children:"sk-ant-..."})]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'echo "ANTHROPIC_API_KEY=sk-ant-..." >> .env\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-2-install-anthropic-sdk",children:"Step 2: Install Anthropic SDK"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"pip3 install anthropic\n\n# Verify\npython3 -c \"import anthropic; print(f'Anthropic SDK v{anthropic.__version__}')\"\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-3-test-api-connection-1",children:"Step 3: Test API Connection"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport os\nfrom dotenv import load_dotenv\nfrom anthropic import Anthropic\n\nload_dotenv()\n\nclient = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))\n\nresponse = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=100,\n    temperature=0.0,\n    messages=[\n        {"role": "user", "content": "Explain what ROS 2 is in one sentence."}\n    ]\n)\n\nprint(response.content[0].text)\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected output"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"ROS 2 (Robot Operating System 2) is an open-source middleware framework that provides communication tools, device drivers, and libraries for building and controlling robotic systems across distributed computing environments.\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"part-3-structured-output-generation",children:"Part 3: Structured Output Generation"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-json-mode-gpt-4-only",children:"Step 1: JSON Mode (GPT-4 only)"}),"\n",(0,i.jsx)(e.p,{children:"Force GPT-4 to output valid JSON:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom openai import OpenAI\nimport json\nimport os\n\nclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\nresponse = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {\n            "role": "system",\n            "content": "You are a robot planning assistant. Output only valid JSON."\n        },\n        {\n            "role": "user",\n            "content": "Parse this command into a robot action: \'Move forward 3 meters and pick up the red cube\'"\n        }\n    ],\n    response_format={"type": "json_object"},  # Force JSON output\n    temperature=0.0\n)\n\n# Parse JSON response\nresult = json.loads(response.choices[0].message.content)\nprint(json.dumps(result, indent=2))\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Output"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-json",children:'{\n  "actions": [\n    {\n      "type": "move",\n      "direction": "forward",\n      "distance": 3.0,\n      "unit": "meters"\n    },\n    {\n      "type": "pick",\n      "object": "cube",\n      "color": "red"\n    }\n  ]\n}\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-2-function-calling-structured-tool-use",children:"Step 2: Function Calling (Structured Tool Use)"}),"\n",(0,i.jsx)(e.p,{children:'Define functions LLM can "call" to interact with robot:'}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom openai import OpenAI\nimport json\nimport os\n\nclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n# Define available robot functions\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "move_robot",\n            "description": "Move the robot in a specified direction",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "direction": {\n                        "type": "string",\n                        "enum": ["forward", "backward", "left", "right"],\n                        "description": "Direction to move"\n                    },\n                    "distance": {\n                        "type": "number",\n                        "description": "Distance in meters"\n                    }\n                },\n                "required": ["direction", "distance"]\n            }\n        }\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "pick_object",\n            "description": "Pick up an object",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "object": {\n                        "type": "string",\n                        "description": "Type of object (e.g., cube, ball, cup)"\n                    },\n                    "color": {\n                        "type": "string",\n                        "description": "Color of the object"\n                    }\n                },\n                "required": ["object"]\n            }\n        }\n    }\n]\n\n# User command\nresponse = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {\n            "role": "user",\n            "content": "Move forward 3 meters and pick up the red cube"\n        }\n    ],\n    tools=tools,\n    tool_choice="auto"  # Let model decide which functions to call\n)\n\n# Extract function calls\nfor tool_call in response.choices[0].message.tool_calls:\n    function_name = tool_call.function.name\n    function_args = json.loads(tool_call.function.arguments)\n\n    print(f"Function: {function_name}")\n    print(f"Arguments: {json.dumps(function_args, indent=2)}")\n    print()\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Output"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'Function: move_robot\nArguments: {\n  "direction": "forward",\n  "distance": 3.0\n}\n\nFunction: pick_object\nArguments: {\n  "object": "cube",\n  "color": "red"\n}\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-3-prompt-engineering-for-structured-output-claude",children:"Step 3: Prompt Engineering for Structured Output (Claude)"}),"\n",(0,i.jsx)(e.p,{children:"Claude doesn't have native JSON mode, so use prompt engineering:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom anthropic import Anthropic\nimport json\nimport os\n\nclient = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))\n\nsystem_prompt = """You are a robot planning assistant. You MUST respond with valid JSON only. No explanations, no markdown, just JSON.\n\nOutput format:\n{\n  "actions": [\n    {"type": "move", "direction": "forward", "distance": 3.0},\n    {"type": "pick", "object": "cube", "color": "red"}\n  ]\n}"""\n\nresponse = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=500,\n    temperature=0.0,\n    system=system_prompt,\n    messages=[\n        {\n            "role": "user",\n            "content": "Parse: \'Move forward 3 meters and pick up the red cube\'"\n        }\n    ]\n)\n\n# Extract JSON from response\nresult_text = response.content[0].text\nresult = json.loads(result_text)\nprint(json.dumps(result, indent=2))\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"part-4-ros-2-integration",children:"Part 4: ROS 2 Integration"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-llm-planning-service",children:"Step 1: LLM Planning Service"}),"\n",(0,i.jsx)(e.p,{children:"Create a ROS 2 service for LLM-based planning:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Define service"})," (",(0,i.jsx)(e.code,{children:"LLMPlan.srv"}),"):"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"# Request\nstring command  # Natural language command\n\n---\n\n# Response\nbool success\nstring plan_json  # JSON-formatted action plan\nstring error_message\n"})}),"\n",(0,i.jsx)(e.p,{children:"Build:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"colcon build --packages-select my_robot_interfaces\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-2-llm-planning-node",children:"Step 2: LLM Planning Node"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom my_robot_interfaces.srv import LLMPlan\nfrom openai import OpenAI\nimport json\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass LLMPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planning_node\')\n\n        # Initialize OpenAI client\n        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n        # Define robot functions\n        self.tools = [\n            {\n                "type": "function",\n                "function": {\n                    "name": "navigate",\n                    "description": "Navigate to a location or move in a direction",\n                    "parameters": {\n                        "type": "object",\n                        "properties": {\n                            "action": {\n                                "type": "string",\n                                "enum": ["move", "turn", "go_to"],\n                                "description": "Type of navigation action"\n                            },\n                            "direction": {\n                                "type": "string",\n                                "enum": ["forward", "backward", "left", "right"]\n                            },\n                            "distance": {"type": "number"},\n                            "angle": {"type": "number"},\n                            "location": {"type": "string"}\n                        },\n                        "required": ["action"]\n                    }\n                }\n            },\n            {\n                "type": "function",\n                "function": {\n                    "name": "manipulate",\n                    "description": "Pick, place, or grasp objects",\n                    "parameters": {\n                        "type": "object",\n                        "properties": {\n                            "action": {\n                                "type": "string",\n                                "enum": ["pick", "place", "release"]\n                            },\n                            "object": {"type": "string"},\n                            "color": {"type": "string"},\n                            "location": {"type": "string"}\n                        },\n                        "required": ["action", "object"]\n                    }\n                }\n            },\n            {\n                "type": "function",\n                "function": {\n                    "name": "perceive",\n                    "description": "Look for objects or scan the environment",\n                    "parameters": {\n                        "type": "object",\n                        "properties": {\n                            "action": {\n                                "type": "string",\n                                "enum": ["find", "scan", "look_at"]\n                            },\n                            "object": {"type": "string"}\n                        },\n                        "required": ["action"]\n                    }\n                }\n            }\n        ]\n\n        # Create service\n        self.srv = self.create_service(\n            LLMPlan,\n            \'llm_plan\',\n            self.plan_callback\n        )\n\n        self.get_logger().info(\'LLM Planning Node ready\')\n\n    def plan_callback(self, request, response):\n        """Generate action plan from natural language command"""\n        try:\n            command = request.command\n            self.get_logger().info(f\'Planning for: {command}\')\n\n            # Call GPT-4 with function calling\n            completion = self.client.chat.completions.create(\n                model="gpt-4o-mini",\n                messages=[\n                    {\n                        "role": "system",\n                        "content": "You are a robot planning assistant. Generate a sequence of actions to complete the user\'s request."\n                    },\n                    {\n                        "role": "user",\n                        "content": command\n                    }\n                ],\n                tools=self.tools,\n                tool_choice="auto",\n                temperature=0.0\n            )\n\n            # Extract tool calls\n            tool_calls = completion.choices[0].message.tool_calls\n\n            if not tool_calls:\n                response.success = False\n                response.error_message = "No valid actions generated"\n                return response\n\n            # Convert to JSON plan\n            plan = {"actions": []}\n\n            for tool_call in tool_calls:\n                action = {\n                    "function": tool_call.function.name,\n                    "parameters": json.loads(tool_call.function.arguments)\n                }\n                plan["actions"].append(action)\n\n            response.success = True\n            response.plan_json = json.dumps(plan, indent=2)\n            response.error_message = ""\n\n            self.get_logger().info(f\'Generated {len(plan["actions"])} actions\')\n\n        except Exception as e:\n            response.success = False\n            response.error_message = str(e)\n            self.get_logger().error(f\'Planning failed: {e}\')\n\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlanningNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-3-test-llm-planning-service",children:"Step 3: Test LLM Planning Service"}),"\n",(0,i.jsx)(e.p,{children:"Terminal 1 (start node):"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'source /opt/ros/humble/setup.bash\nexport OPENAI_API_KEY="sk-proj-..."\npython3 llm_planning_node.py\n'})}),"\n",(0,i.jsx)(e.p,{children:"Terminal 2 (call service):"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash\n\nros2 service call /llm_plan my_robot_interfaces/srv/LLMPlan \\\n  \"{command: 'Go to the kitchen and pick up the red cup'}\"\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected response"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'success: true\nplan_json: |\n  {\n    "actions": [\n      {\n        "function": "navigate",\n        "parameters": {\n          "action": "go_to",\n          "location": "kitchen"\n        }\n      },\n      {\n        "function": "perceive",\n        "parameters": {\n          "action": "find",\n          "object": "cup"\n        }\n      },\n      {\n        "function": "manipulate",\n        "parameters": {\n          "action": "pick",\n          "object": "cup",\n          "color": "red"\n        }\n      }\n    ]\n  }\nerror_message: \'\'\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"part-5-error-handling-and-optimization",children:"Part 5: Error Handling and Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-retry-logic-with-exponential-backoff",children:"Step 1: Retry Logic with Exponential Backoff"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import time\nfrom openai import OpenAI, RateLimitError, APIError\n\ndef call_llm_with_retry(client, messages, max_retries=3):\n    """Call LLM with exponential backoff retry"""\n    for attempt in range(max_retries):\n        try:\n            response = client.chat.completions.create(\n                model="gpt-4o-mini",\n                messages=messages,\n                temperature=0.0,\n                max_tokens=500\n            )\n            return response\n\n        except RateLimitError:\n            wait_time = 2 ** attempt  # 1s, 2s, 4s\n            print(f"Rate limit hit. Retrying in {wait_time}s...")\n            time.sleep(wait_time)\n\n        except APIError as e:\n            print(f"API error: {e}")\n            if attempt < max_retries - 1:\n                time.sleep(1)\n            else:\n                raise\n\n    raise Exception("Max retries exceeded")\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-2-timeout-handling",children:"Step 2: Timeout Handling"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import asyncio\nfrom openai import AsyncOpenAI\n\nasync def call_llm_with_timeout(client, messages, timeout=5.0):\n    """Call LLM with timeout"""\n    try:\n        response = await asyncio.wait_for(\n            client.chat.completions.create(\n                model="gpt-4o-mini",\n                messages=messages,\n                temperature=0.0\n            ),\n            timeout=timeout\n        )\n        return response\n\n    except asyncio.TimeoutError:\n        print(f"LLM call timed out after {timeout}s")\n        return None\n\n# Usage\nasync def main():\n    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n    messages = [{"role": "user", "content": "Plan: Go to the kitchen"}]\n\n    response = await call_llm_with_timeout(client, messages, timeout=5.0)\n\n    if response:\n        print(response.choices[0].message.content)\n    else:\n        print("Using fallback planner...")\n\nasyncio.run(main())\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"step-3-cost-tracking",children:"Step 3: Cost Tracking"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import tiktoken\n\nclass CostTracker:\n    def __init__(self, model="gpt-4o-mini"):\n        self.model = model\n        self.encoding = tiktoken.encoding_for_model(model)\n\n        # Pricing per 1M tokens (as of 2024)\n        self.pricing = {\n            "gpt-4o-mini": {"input": 0.15, "output": 0.60},\n            "gpt-4-turbo": {"input": 10.0, "output": 30.0},\n            "claude-3-5-sonnet-20241022": {"input": 3.0, "output": 15.0}\n        }\n\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n\n    def count_tokens(self, text: str) -> int:\n        """Count tokens in text"""\n        return len(self.encoding.encode(text))\n\n    def log_request(self, messages: list, response):\n        """Log token usage from request"""\n        # Count input tokens\n        input_tokens = sum([\n            self.count_tokens(msg[\'content\'])\n            for msg in messages\n        ])\n\n        # Get output tokens from response\n        output_tokens = response.usage.completion_tokens\n\n        self.total_input_tokens += input_tokens\n        self.total_output_tokens += output_tokens\n\n        # Calculate cost\n        cost = self.calculate_cost()\n\n        print(f"Tokens: {input_tokens} in, {output_tokens} out")\n        print(f"Total cost: ${cost:.4f}")\n\n    def calculate_cost(self) -> float:\n        """Calculate total cost in USD"""\n        input_cost = (self.total_input_tokens / 1_000_000) * self.pricing[self.model]["input"]\n        output_cost = (self.total_output_tokens / 1_000_000) * self.pricing[self.model]["output"]\n        return input_cost + output_cost\n\n# Usage\ntracker = CostTracker(model="gpt-4o-mini")\n\nresponse = client.chat.completions.create(...)\ntracker.log_request(messages, response)\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,i.jsx)(e.h3,{id:"exercise-1-multi-llm-comparison",children:"Exercise 1: Multi-LLM Comparison"}),"\n",(0,i.jsx)(e.p,{children:"Compare GPT-4, Claude, and a local model on the same planning task:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement functions for each LLM (GPT-4, Claude, Llama 3)"}),"\n",(0,i.jsx)(e.li,{children:"Test with 10 robot commands"}),"\n",(0,i.jsx)(e.li,{children:"Compare: latency, cost, accuracy, JSON validity"}),"\n",(0,i.jsx)(e.li,{children:"Generate comparison table"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Starter code"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import time\n\nclass LLMBenchmark:\n    def __init__(self):\n        self.gpt_client = OpenAI(...)\n        self.claude_client = Anthropic(...)\n\n    def benchmark(self, command: str) -> dict:\n        """Benchmark all LLMs on a command"""\n        results = {}\n\n        # GPT-4\n        start = time.time()\n        gpt_response = self.gpt_client.chat.completions.create(...)\n        results[\'gpt4\'] = {\n            \'latency\': time.time() - start,\n            \'output\': gpt_response.choices[0].message.content\n        }\n\n        # TODO: Add Claude\n        # TODO: Add local model (Llama 3)\n        # TODO: Calculate costs\n        # TODO: Validate JSON\n\n        return results\n\n# Test commands\ncommands = [\n    "Go to the kitchen",\n    "Pick up the red cube and place it on the table",\n    # ... 8 more\n]\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h3,{id:"exercise-2-caching-for-repeated-commands",children:"Exercise 2: Caching for Repeated Commands"}),"\n",(0,i.jsx)(e.p,{children:"Implement a cache to avoid re-calling LLM for identical commands:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Hash input command"}),"\n",(0,i.jsx)(e.li,{children:"Check cache before calling LLM"}),"\n",(0,i.jsx)(e.li,{children:"Store response with TTL (time-to-live)"}),"\n",(0,i.jsx)(e.li,{children:"Track cache hit rate"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Starter code"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import hashlib\nimport time\n\nclass LLMCache:\n    def __init__(self, ttl=3600):\n        self.cache = {}  # {hash: (response, timestamp)}\n        self.ttl = ttl\n\n    def get(self, command: str):\n        # TODO: Hash command\n        # TODO: Check if in cache and not expired\n        # TODO: Return cached response or None\n        pass\n\n    def set(self, command: str, response: str):\n        # TODO: Hash command\n        # TODO: Store with timestamp\n        pass\n\n# Usage\ncache = LLMCache(ttl=3600)  # 1 hour TTL\n\ncached = cache.get(command)\nif cached:\n    print("Cache hit!")\n    response = cached\nelse:\n    response = client.chat.completions.create(...)\n    cache.set(command, response)\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this lesson, you learned to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u2705 Set up OpenAI GPT-4 and Anthropic Claude APIs"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 Generate structured JSON output with JSON mode and function calling"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 Integrate LLMs with ROS 2 services for cognitive planning"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 Handle errors, timeouts, and rate limits"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 Track costs and optimize token usage"}),"\n",(0,i.jsx)(e.li,{children:"\u2705 Compare LLM options for robotics applications"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"GPT-4o mini"}),": Best for learning (cheap, fast, good quality)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Claude 3.5 Sonnet"}),": Best for production (long context, lower cost)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Function calling"}),": Most reliable way to get structured actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cost optimization"}),": Cache frequent commands, use shorter prompts"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error handling"}),": Always implement retries and timeouts"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,i.jsx)(e.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Reference"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://docs.anthropic.com/claude/reference/getting-started",children:"Anthropic Claude API"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"Function Calling Guide"})}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://github.com/openai/tiktoken",children:"tiktoken (Token Counter)"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://openai.com/pricing",children:"OpenAI Pricing Calculator"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://www.anthropic.com/pricing",children:"Anthropic Pricing"})}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2201.07207",children:"Language Models as Zero-Shot Planners"})}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2204.01691",children:"Do As I Can, Not As I Say (SayCan)"})," - Google's LLM for robotics"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,i.jsxs)(e.p,{children:["In ",(0,i.jsx)(e.strong,{children:"Lesson 2: Prompt Engineering"}),", you'll learn to:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Design robotics-specific prompts for consistent output"}),"\n",(0,i.jsx)(e.li,{children:"Use few-shot examples to improve planning quality"}),"\n",(0,i.jsx)(e.li,{children:"Handle edge cases and error scenarios"}),"\n",(0,i.jsx)(e.li,{children:"Build a prompt library for common tasks"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["Continue to ",(0,i.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch12-llm-planning/prompt-engineering",children:"Prompt Engineering \u2192"})]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>l});var s=t(6540);const i={},o=s.createContext(i);function r(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);