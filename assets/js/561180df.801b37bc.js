"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[9994],{5495:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vla/ch11-whisper/whisper-api","title":"Speech-to-Text with OpenAI Whisper API","description":"Learning Objectives","source":"@site/docs/module-4-vla/ch11-whisper/whisper-api.md","sourceDirName":"module-4-vla/ch11-whisper","slug":"/module-4-vla/ch11-whisper/whisper-api","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/whisper-api","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/ch11-whisper/whisper-api.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Audio Capture with ReSpeaker Microphone","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/audio-capture"},"next":{"title":"Local Speech-to-Text Pipeline with Whisper","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/speech-to-text"}}');var r=i(4848),t=i(8453);const a={},o="Speech-to-Text with OpenAI Whisper API",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Part 1: OpenAI API Setup",id:"part-1-openai-api-setup",level:2},{value:"Step 1: Create OpenAI Account and API Key",id:"step-1-create-openai-account-and-api-key",level:3},{value:"Step 2: Install OpenAI Python SDK",id:"step-2-install-openai-python-sdk",level:3},{value:"Step 3: Test API Connectivity",id:"step-3-test-api-connectivity",level:3},{value:"Part 2: Transcription Modes and Parameters",id:"part-2-transcription-modes-and-parameters",level:2},{value:"Step 1: Response Formats",id:"step-1-response-formats",level:3},{value:"Step 2: Verbose JSON with Timestamps",id:"step-2-verbose-json-with-timestamps",level:3},{value:"Step 3: Language Detection and Translation",id:"step-3-language-detection-and-translation",level:3},{value:"Step 4: Prompt Engineering for Context",id:"step-4-prompt-engineering-for-context",level:3},{value:"Part 3: Production Implementation",id:"part-3-production-implementation",level:2},{value:"Step 1: Async Transcription for Low Latency",id:"step-1-async-transcription-for-low-latency",level:3},{value:"Step 2: Error Handling and Retries",id:"step-2-error-handling-and-retries",level:3},{value:"Step 3: Audio Format Optimization",id:"step-3-audio-format-optimization",level:3},{value:"Step 4: Streaming Audio Transcription",id:"step-4-streaming-audio-transcription",level:3},{value:"Part 4: ROS 2 Integration",id:"part-4-ros-2-integration",level:2},{value:"Step 1: Create Whisper Transcription Node",id:"step-1-create-whisper-transcription-node",level:3},{value:"Step 2: Test ROS 2 Integration",id:"step-2-test-ros-2-integration",level:3},{value:"Step 3: Launch File for Complete Pipeline",id:"step-3-launch-file-for-complete-pipeline",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercise 1: Cost Monitoring",id:"exercise-1-cost-monitoring",level:3},{value:"Exercise 2: Multilingual Command System",id:"exercise-2-multilingual-command-system",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Issue 1: &quot;Invalid file format&quot; error",id:"issue-1-invalid-file-format-error",level:3},{value:"Issue 2: High latency (&gt;3 seconds)",id:"issue-2-high-latency-3-seconds",level:3},{value:"Issue 3: Poor transcription accuracy",id:"issue-3-poor-transcription-accuracy",level:3},{value:"Issue 4: Rate limit errors",id:"issue-4-rate-limit-errors",level:3},{value:"Cost Optimization Strategies",id:"cost-optimization-strategies",level:2},{value:"Strategy 1: Silence Detection",id:"strategy-1-silence-detection",level:3},{value:"Strategy 2: Wake Word Detection",id:"strategy-2-wake-word-detection",level:3},{value:"Strategy 3: Caching Frequent Commands",id:"strategy-3-caching-frequent-commands",level:3},{value:"Summary",id:"summary",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Cost Calculators",id:"cost-calculators",level:3},{value:"Alternative Solutions",id:"alternative-solutions",level:3},{value:"Video Tutorials",id:"video-tutorials",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function d(e){const n={a:"a",code:"code",del:"del",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"speech-to-text-with-openai-whisper-api",children:"Speech-to-Text with OpenAI Whisper API"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Set up OpenAI Whisper API for cloud-based speech recognition"}),"\n",(0,r.jsx)(n.li,{children:"Configure authentication and API endpoints"}),"\n",(0,r.jsx)(n.li,{children:"Implement audio transcription with language detection"}),"\n",(0,r.jsx)(n.li,{children:"Handle streaming vs. batch transcription modes"}),"\n",(0,r.jsx)(n.li,{children:"Optimize API usage for cost and latency"}),"\n",(0,r.jsx)(n.li,{children:"Integrate Whisper transcriptions with ROS 2 voice command systems"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Required Knowledge"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Python programming (async/await, HTTP requests)"}),"\n",(0,r.jsx)(n.li,{children:"Basic understanding of REST APIs"}),"\n",(0,r.jsx)(n.li,{children:"Audio file formats (WAV, MP3, FLAC)"}),"\n",(0,r.jsxs)(n.li,{children:["Completion of ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/audio-capture",children:"Lesson 1: Audio Capture"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Required Software"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Python 3.10+"}),"\n",(0,r.jsxs)(n.li,{children:["OpenAI Python SDK (",(0,r.jsx)(n.code,{children:"openai>=1.0.0"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble (for integration exercises)"}),"\n",(0,r.jsx)(n.li,{children:"Audio files from previous lesson"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Required Accounts"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"OpenAI API account with billing enabled"}),"\n",(0,r.jsx)(n.li,{children:"API key with Whisper access"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 2-3 hours"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["OpenAI's ",(0,r.jsx)(n.strong,{children:"Whisper"})," is a state-of-the-art automatic speech recognition (ASR) model trained on 680,000 hours of multilingual data. Key advantages for robotics applications:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"99 languages supported"}),": Multilingual voice commands for global deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robust to noise"}),": Handles motor noise, background speech, accents"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Automatic language detection"}),": No need to specify input language"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Timestamped transcriptions"}),": Word-level timing for synchronization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cloud-based"}),": No GPU required on robot (offload to OpenAI servers)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Whisper API vs. Local Whisper"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"Whisper API (Cloud)"}),(0,r.jsx)(n.th,{children:"Local Whisper (Edge)"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Latency"})}),(0,r.jsx)(n.td,{children:"500-2000ms (network)"}),(0,r.jsx)(n.td,{children:"100-500ms (GPU-dependent)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Cost"})}),(0,r.jsx)(n.td,{children:"$0.006/minute"}),(0,r.jsx)(n.td,{children:"Free (hardware cost)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Accuracy"})}),(0,r.jsx)(n.td,{children:"Highest (large-v3 model)"}),(0,r.jsx)(n.td,{children:"Good (base/small models)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Privacy"})}),(0,r.jsx)(n.td,{children:"Audio sent to OpenAI"}),(0,r.jsx)(n.td,{children:"Audio stays on device"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Hardware"})}),(0,r.jsx)(n.td,{children:"CPU-only robot"}),(0,r.jsx)(n.td,{children:"GPU required (CUDA)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Reliability"})}),(0,r.jsx)(n.td,{children:"Depends on internet"}),(0,r.jsx)(n.td,{children:"Offline-capable"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommended Use Case"}),": Use Whisper API for prototyping and cloud-connected robots. Switch to local Whisper for production deployments with strict latency/privacy requirements (covered in Lesson 3)."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-1-openai-api-setup",children:"Part 1: OpenAI API Setup"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-openai-account-and-api-key",children:"Step 1: Create OpenAI Account and API Key"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Sign up at ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/",children:"platform.openai.com"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Navigate to ",(0,r.jsx)(n.strong,{children:"API Keys"})," section (",(0,r.jsx)(n.a,{href:"https://platform.openai.com/api-keys",children:"https://platform.openai.com/api-keys"}),")"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Click ",(0,r.jsx)(n.strong,{children:"Create new secret key"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Name: ",(0,r.jsx)(n.code,{children:"whisper-robotics"})]}),"\n",(0,r.jsxs)(n.li,{children:["Permissions: ",(0,r.jsx)(n.code,{children:"Whisper"})," access only (for security)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Copy the API key"})," (you won't see it again):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Secure storage"}),": Never hardcode API keys in code!"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Create .env file (add to .gitignore)\necho "OPENAI_API_KEY=sk-proj-..." > .env\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-install-openai-python-sdk",children:"Step 2: Install OpenAI Python SDK"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install OpenAI SDK (version 1.0+ has updated API)\npip3 install openai python-dotenv\n\n# Verify installation\npython3 -c \"import openai; print(f'OpenAI SDK v{openai.__version__}')\"\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"OpenAI SDK v1.6.1\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-test-api-connectivity",children:"Step 3: Test API Connectivity"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"test_whisper_api.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from .env\nload_dotenv()\n\n# Initialize client\nclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n# Test with a simple audio file\naudio_file_path = "test.wav"  # From previous lesson\n\ntry:\n    with open(audio_file_path, "rb") as audio_file:\n        # Transcribe audio\n        transcription = client.audio.transcriptions.create(\n            model="whisper-1",  # Current Whisper API model (large-v3)\n            file=audio_file,\n            response_format="text"\n        )\n\n    print(f"Transcription: {transcription}")\n\nexcept FileNotFoundError:\n    print(f"Error: {audio_file_path} not found")\n    print("Record audio first with: arecord -D plughw:1,0 -f S16_LE -r 16000 -c 1 -d 5 test.wav")\n\nexcept Exception as e:\n    print(f"API Error: {e}")\n'})}),"\n",(0,r.jsx)(n.p,{children:"Run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 test_whisper_api.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Transcription: Hello, this is a test of the Whisper speech recognition system.\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"If you get an authentication error"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Verify API key is correct in ",(0,r.jsx)(n.code,{children:".env"})]}),"\n",(0,r.jsx)(n.li,{children:"Check billing is enabled on OpenAI account"}),"\n",(0,r.jsx)(n.li,{children:"Ensure no typos in environment variable name"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-2-transcription-modes-and-parameters",children:"Part 2: Transcription Modes and Parameters"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-response-formats",children:"Step 1: Response Formats"}),"\n",(0,r.jsx)(n.p,{children:"Whisper API supports multiple output formats:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Format"}),(0,r.jsx)(n.th,{children:"Output Type"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"text"})}),(0,r.jsx)(n.td,{children:"Plain string"}),(0,r.jsx)(n.td,{children:"Simple transcription"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"json"})}),(0,r.jsx)(n.td,{children:"JSON object"}),(0,r.jsx)(n.td,{children:"Metadata (language, duration)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"verbose_json"})}),(0,r.jsx)(n.td,{children:"Detailed JSON"}),(0,r.jsx)(n.td,{children:"Word timestamps, confidence"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"srt"})}),(0,r.jsx)(n.td,{children:"SubRip subtitles"}),(0,r.jsx)(n.td,{children:"Video captioning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vtt"})}),(0,r.jsx)(n.td,{children:"WebVTT subtitles"}),(0,r.jsx)(n.td,{children:"Web video"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: JSON format with metadata"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'transcription = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file,\n    response_format="json"\n)\n\nprint(f"Text: {transcription.text}")\nprint(f"Language: {transcription.language}")  # Auto-detected\nprint(f"Duration: {transcription.duration}s")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sample output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "text": "Move forward three meters and turn left.",\n  "language": "en",\n  "duration": 2.5\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-verbose-json-with-timestamps",children:"Step 2: Verbose JSON with Timestamps"}),"\n",(0,r.jsx)(n.p,{children:"For applications requiring word-level timing (e.g., lip-sync, subtitle alignment):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'transcription = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file,\n    response_format="verbose_json",\n    timestamp_granularities=["word", "segment"]\n)\n\n# Access word-level timestamps\nfor word in transcription.words:\n    print(f"{word.word}: {word.start}s - {word.end}s")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sample output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Move: 0.0s - 0.24s\nforward: 0.24s - 0.56s\nthree: 0.56s - 0.84s\nmeters: 0.84s - 1.20s\nand: 1.20s - 1.32s\nturn: 1.32s - 1.60s\nleft: 1.60s - 1.88s\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use case for robotics"}),': Synchronize robot actions with spoken commands (e.g., start moving when "forward" is detected, not after full sentence).']}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-language-detection-and-translation",children:"Step 3: Language Detection and Translation"}),"\n",(0,r.jsx)(n.p,{children:"Whisper automatically detects the input language, but you can override:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Auto-detect language (default)\ntranscription = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file\n)\n\n# Force specific language (improves accuracy if known)\ntranscription = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file,\n    language="es"  # ISO-639-1 code (es=Spanish, fr=French, etc.)\n)\n\n# Translate non-English to English\ntranslation = client.audio.translations.create(\n    model="whisper-1",\n    file=audio_file  # Input: Spanish audio\n    # Output: English text\n)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Input audio: "Mu\xe9vete hacia adelante tres metros" (Spanish)'}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"transcriptions.create()"}),' \u2192 "Mu\xe9vete hacia adelante tres metros"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"translations.create()"}),' \u2192 "Move forward three meters"']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Robotics use case"}),": Deploy same robot in multiple countries - translate voice commands to English for unified processing."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-4-prompt-engineering-for-context",children:"Step 4: Prompt Engineering for Context"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"prompt"})," parameter provides context to improve transcription accuracy:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Without prompt (may mishear technical terms)\ntranscription = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file\n)\n# Result: "Navigate to the ROS to topic" (incorrect)\n\n# With prompt (provides domain-specific vocabulary)\ntranscription = client.audio.transcriptions.create(\n    model="whisper-1",\n    file=audio_file,\n    prompt="This is a robotics command using ROS 2 navigation. Common terms: Nav2, ROS, LiDAR, SLAM, costmap."\n)\n# Result: "Navigate to the ROS 2 topic" (correct)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Best practices"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Include domain-specific terms (ROS 2, Isaac Sim, Nav2, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Add context about expected command structure"}),"\n",(0,r.jsx)(n.li,{children:"Keep prompt under 224 tokens (~1000 characters)"}),"\n",(0,r.jsx)(n.li,{children:"Update prompt based on observed errors"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-3-production-implementation",children:"Part 3: Production Implementation"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-async-transcription-for-low-latency",children:"Step 1: Async Transcription for Low Latency"}),"\n",(0,r.jsx)(n.p,{children:"Blocking I/O hurts real-time performance. Use async/await:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport asyncio\nimport os\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\n\nload_dotenv()\nclient = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\nasync def transcribe_audio(audio_path: str) -> str:\n    """Asynchronously transcribe audio file"""\n    try:\n        with open(audio_path, "rb") as audio_file:\n            transcription = await client.audio.transcriptions.create(\n                model="whisper-1",\n                file=audio_file,\n                response_format="text",\n                prompt="Robotics voice commands for ROS 2 navigation and manipulation."\n            )\n        return transcription\n    except Exception as e:\n        print(f"Transcription error: {e}")\n        return ""\n\nasync def main():\n    # Transcribe multiple files in parallel\n    files = ["command1.wav", "command2.wav", "command3.wav"]\n    tasks = [transcribe_audio(f) for f in files]\n\n    # Wait for all transcriptions to complete\n    results = await asyncio.gather(*tasks)\n\n    for file, result in zip(files, results):\n        print(f"{file}: {result}")\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sequential (blocking): 3 files \xd7 800ms = 2400ms total"}),"\n",(0,r.jsx)(n.li,{children:"Parallel (async): max(800ms, 800ms, 800ms) = 800ms total"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3x speedup"})," for batch processing"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-error-handling-and-retries",children:"Step 2: Error Handling and Retries"}),"\n",(0,r.jsx)(n.p,{children:"Handle network failures and rate limits:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\ndef transcribe_with_retry(audio_path: str, max_retries: int = 3) -> str:\n    """Transcribe with exponential backoff retry"""\n    for attempt in range(max_retries):\n        try:\n            with open(audio_path, "rb") as audio_file:\n                transcription = client.audio.transcriptions.create(\n                    model="whisper-1",\n                    file=audio_file,\n                    response_format="text"\n                )\n            return transcription\n\n        except RateLimitError:\n            # Rate limit hit - wait and retry\n            wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n            print(f"Rate limit hit. Retrying in {wait_time}s...")\n            time.sleep(wait_time)\n\n        except APIConnectionError as e:\n            # Network issue - retry\n            print(f"Connection error: {e}. Retrying...")\n            time.sleep(1)\n\n        except APIError as e:\n            # API error (e.g., invalid file format)\n            print(f"API error: {e}")\n            return ""  # Don\'t retry for API errors\n\n    print(f"Max retries ({max_retries}) exceeded")\n    return ""\n\n# Usage\nresult = transcribe_with_retry("command.wav")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Error handling checklist"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Network failures \u2192 Retry with backoff"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Rate limits \u2192 Backoff and retry"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Invalid audio format \u2192 Log error, don't retry"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Authentication errors \u2192 Alert developer"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-audio-format-optimization",children:"Step 3: Audio Format Optimization"}),"\n",(0,r.jsx)(n.p,{children:"Whisper API accepts multiple formats, but some are more efficient:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Supported formats"}),": MP3, MP4, MPEG, MPGA, M4A, WAV, WEBM, FLAC"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendations"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Format"}),(0,r.jsx)(n.th,{children:"Bitrate"}),(0,r.jsx)(n.th,{children:"File Size (5s)"}),(0,r.jsx)(n.th,{children:"Quality"}),(0,r.jsx)(n.th,{children:"Best For"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"FLAC"})}),(0,r.jsx)(n.td,{children:"Lossless"}),(0,r.jsx)(n.td,{children:"500 KB"}),(0,r.jsx)(n.td,{children:"Perfect"}),(0,r.jsx)(n.td,{children:"Archival"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"WAV"})}),(0,r.jsx)(n.td,{children:"256 kbps"}),(0,r.jsx)(n.td,{children:"320 KB"}),(0,r.jsx)(n.td,{children:"Perfect"}),(0,r.jsx)(n.td,{children:"Development"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"MP3"})}),(0,r.jsx)(n.td,{children:"32 kbps"}),(0,r.jsx)(n.td,{children:"20 KB"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"Production"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"OPUS"})}),(0,r.jsx)(n.td,{children:"24 kbps"}),(0,r.jsx)(n.td,{children:"15 KB"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"Bandwidth-limited"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Convert WAV to MP3 for production"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import subprocess\n\ndef convert_to_mp3(wav_path: str, mp3_path: str):\n    """Convert WAV to MP3 with optimal settings for speech"""\n    subprocess.run([\n        "ffmpeg",\n        "-i", wav_path,\n        "-codec:a", "libmp3lame",\n        "-b:a", "32k",  # 32 kbps (sufficient for speech)\n        "-ar", "16000",  # 16 kHz sample rate\n        "-ac", "1",      # Mono\n        mp3_path\n    ], check=True)\n\n# Usage\nconvert_to_mp3("command.wav", "command.mp3")\n\n# Transcribe compressed file (16x smaller)\nwith open("command.mp3", "rb") as audio_file:\n    transcription = client.audio.transcriptions.create(\n        model="whisper-1",\n        file=audio_file\n    )\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost savings"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"WAV: 320 KB \xd7 $0.006/minute = $0.0032 per 5s"}),"\n",(0,r.jsx)(n.li,{children:"MP3 (32kbps): 20 KB \xd7 $0.006/minute = $0.0002 per 5s"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"16x cost reduction"})," with negligible quality loss for speech"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-4-streaming-audio-transcription",children:"Step 4: Streaming Audio Transcription"}),"\n",(0,r.jsx)(n.p,{children:"For real-time applications, transcribe audio chunks as they arrive:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport io\nimport wave\nfrom openai import OpenAI\nimport pyaudio\n\nclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n# Audio configuration\nCHUNK_DURATION = 3  # Transcribe every 3 seconds\nRATE = 16000\nCHUNK_SIZE = RATE * CHUNK_DURATION  # 48000 samples\n\n# Initialize PyAudio\np = pyaudio.PyAudio()\nstream = p.open(\n    rate=RATE,\n    format=pyaudio.paInt16,\n    channels=1,\n    input=True,\n    input_device_index=1,  # ReSpeaker\n    frames_per_buffer=1024\n)\n\nprint("Streaming transcription (speak in 3-second chunks)...")\n\ntry:\n    while True:\n        # Record chunk\n        frames = []\n        for _ in range(0, int(RATE / 1024 * CHUNK_DURATION)):\n            data = stream.read(1024)\n            frames.append(data)\n\n        # Create WAV in memory\n        wav_buffer = io.BytesIO()\n        with wave.open(wav_buffer, \'wb\') as wf:\n            wf.setnchannels(1)\n            wf.setsampwidth(2)\n            wf.setframerate(RATE)\n            wf.writeframes(b\'\'.join(frames))\n\n        wav_buffer.seek(0)\n        wav_buffer.name = "chunk.wav"  # Required by OpenAI SDK\n\n        # Transcribe chunk\n        transcription = client.audio.transcriptions.create(\n            model="whisper-1",\n            file=wav_buffer,\n            response_format="text"\n        )\n\n        if transcription.strip():  # Only print non-empty\n            print(f">> {transcription}")\n\nexcept KeyboardInterrupt:\n    print("\\nStopping...")\nfinally:\n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"3-second chunks \u2192 ~800ms transcription latency"}),"\n",(0,r.jsxs)(n.li,{children:["Total latency: 3000ms (record) + 800ms (transcribe) = ",(0,r.jsx)(n.strong,{children:"3.8s"})]}),"\n",(0,r.jsx)(n.li,{children:"Suitable for conversational AI (acceptable delay)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optimization"}),": Use 1-second chunks for lower latency (but higher API costs)."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-4-ros-2-integration",children:"Part 4: ROS 2 Integration"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-whisper-transcription-node",children:"Step 1: Create Whisper Transcription Node"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"whisper_node.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport os\nimport io\nimport wave\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n\n        # OpenAI client\n        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n        # ROS 2 subscribers and publishers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'audio/chunks\',  # From audio capture node\n            self.audio_callback,\n            10\n        )\n\n        self.transcript_pub = self.create_publisher(\n            String,\n            \'voice/transcript\',\n            10\n        )\n\n        # Audio buffer\n        self.audio_frames = []\n        self.chunk_duration = 3.0  # seconds\n        self.sample_rate = 16000\n        self.max_frames = int(self.sample_rate / 1024 * self.chunk_duration)\n\n        self.get_logger().info(\'Whisper node started\')\n\n    def audio_callback(self, msg: AudioData):\n        """Accumulate audio chunks and transcribe when buffer is full"""\n        self.audio_frames.append(bytes(msg.data))\n\n        if len(self.audio_frames) >= self.max_frames:\n            # Transcribe accumulated audio\n            self.transcribe_buffer()\n            self.audio_frames.clear()\n\n    def transcribe_buffer(self):\n        """Transcribe accumulated audio frames"""\n        try:\n            # Create WAV in memory\n            wav_buffer = io.BytesIO()\n            with wave.open(wav_buffer, \'wb\') as wf:\n                wf.setnchannels(1)\n                wf.setsampwidth(2)\n                wf.setframerate(self.sample_rate)\n                wf.writeframes(b\'\'.join(self.audio_frames))\n\n            wav_buffer.seek(0)\n            wav_buffer.name = "chunk.wav"\n\n            # Transcribe\n            transcription = self.client.audio.transcriptions.create(\n                model="whisper-1",\n                file=wav_buffer,\n                response_format="text",\n                prompt="ROS 2 robotics voice commands for navigation and manipulation."\n            )\n\n            if transcription.strip():\n                # Publish transcript\n                msg = String()\n                msg.data = transcription\n                self.transcript_pub.publish(msg)\n                self.get_logger().info(f\'Transcript: {transcription}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Transcription failed: {e}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-2-test-ros-2-integration",children:"Step 2: Test ROS 2 Integration"}),"\n",(0,r.jsx)(n.p,{children:"Terminal 1 (audio capture - from previous lesson):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash\npython3 audio_publisher.py\n"})}),"\n",(0,r.jsx)(n.p,{children:"Terminal 2 (Whisper transcription):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'source /opt/ros/humble/setup.bash\nexport OPENAI_API_KEY="sk-proj-..."\npython3 whisper_node.py\n'})}),"\n",(0,r.jsx)(n.p,{children:"Terminal 3 (monitor transcripts):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash\nros2 topic echo /voice/transcript\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected output"})," (when you speak):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'data: "Move forward three meters and turn left."\n---\ndata: "Pick up the red cube on the table."\n---\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-launch-file-for-complete-pipeline",children:"Step 3: Launch File for Complete Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"whisper_pipeline.launch.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Audio capture node\n        Node(\n            package='voice_control',\n            executable='audio_publisher.py',\n            name='audio_publisher',\n            output='screen'\n        ),\n\n        # Whisper transcription node\n        Node(\n            package='voice_control',\n            executable='whisper_node.py',\n            name='whisper_node',\n            output='screen',\n            parameters=[{\n                'chunk_duration': 3.0,\n                'sample_rate': 16000\n            }]\n        ),\n\n        # Command parser node (next lesson)\n        Node(\n            package='voice_control',\n            executable='command_parser.py',\n            name='command_parser',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,r.jsx)(n.p,{children:"Launch:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch voice_control whisper_pipeline.launch.py\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-cost-monitoring",children:"Exercise 1: Cost Monitoring"}),"\n",(0,r.jsx)(n.p,{children:"Implement a cost tracker to monitor Whisper API usage:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Track total audio minutes transcribed"}),"\n",(0,r.jsx)(n.li,{children:"Calculate cost at $0.006/minute"}),"\n",(0,r.jsx)(n.li,{children:"Log daily usage to CSV file"}),"\n",(0,r.jsx)(n.li,{children:"Alert when approaching budget threshold"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Starter code"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import csv\nfrom datetime import datetime\n\nclass CostTracker:\n    def __init__(self, budget_limit: float = 10.0):\n        self.total_minutes = 0.0\n        self.budget_limit = budget_limit  # USD\n        self.cost_per_minute = 0.006\n        self.log_file = "whisper_usage.csv"\n\n    def log_transcription(self, duration_seconds: float):\n        """Log transcription and update cost"""\n        # TODO: Convert duration to minutes\n        # TODO: Update total_minutes\n        # TODO: Calculate current cost\n        # TODO: Write to CSV (date, duration, cost)\n        # TODO: Check if approaching budget limit (90%)\n        pass\n\n    def get_current_cost(self) -> float:\n        """Calculate current month\'s cost"""\n        # TODO: Return total_minutes * cost_per_minute\n        pass\n\n# Usage in WhisperNode\ntracker = CostTracker(budget_limit=10.0)\n\n# After each transcription\ntracker.log_transcription(duration_seconds=3.0)\ncurrent_cost = tracker.get_current_cost()\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Bonus"}),": Create a ROS 2 service to query current cost."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-multilingual-command-system",children:"Exercise 2: Multilingual Command System"}),"\n",(0,r.jsx)(n.p,{children:"Extend the Whisper node to handle commands in multiple languages:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detect language of spoken command"}),"\n",(0,r.jsx)(n.li,{children:"Translate non-English commands to English"}),"\n",(0,r.jsx)(n.li,{children:"Route commands to appropriate language-specific handlers"}),"\n",(0,r.jsx)(n.li,{children:"Support at least 3 languages (English, Spanish, Mandarin)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Starter code"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def transcribe_multilingual(audio_file) -> dict:\n    """Transcribe and detect language"""\n    # TODO: Use response_format="verbose_json"\n    # TODO: Extract language from response\n    # TODO: If not English, call translations.create()\n    # TODO: Return dict with original, translated, and language\n    pass\n\n# Expected return\n{\n    "original": "Mu\xe9vete hacia adelante",\n    "translated": "Move forward",\n    "language": "es"\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"issue-1-invalid-file-format-error",children:'Issue 1: "Invalid file format" error'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": Audio file not in supported format or corrupted"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Verify file format:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"file command.wav\n# Should show: RIFF (little-endian) data, WAVE audio\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Check file size (must be < 25 MB):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ls -lh command.wav\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Re-encode with FFmpeg:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ffmpeg -i command.wav -ar 16000 -ac 1 -c:a pcm_s16le fixed.wav\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"issue-2-high-latency-3-seconds",children:"Issue 2: High latency (>3 seconds)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": Large audio files or network congestion"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Reduce chunk duration (3s \u2192 1s)"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Compress audio (WAV \u2192 MP3):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Use MP3 instead of WAV\nsubprocess.run(["ffmpeg", "-i", "chunk.wav", "-b:a", "32k", "chunk.mp3"])\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Use async API for parallel transcription"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Check network latency:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ping api.openai.com\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"issue-3-poor-transcription-accuracy",children:"Issue 3: Poor transcription accuracy"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": Noisy audio, unclear speech, missing context"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Enable beamforming"})," (from previous lesson):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"tuning.write('STATNOISEONOFF', 1)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Add domain-specific prompt"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'prompt="ROS 2 navigation commands: Nav2, SLAM, costmap, LiDAR, move_base"\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Check audio quality"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aplay command.wav  # Should be clear, minimal noise\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Increase sample rate"})," (if using low-quality mic):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"RATE = 24000  # Higher quality (but larger files)\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"issue-4-rate-limit-errors",children:"Issue 4: Rate limit errors"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cause"}),": Exceeding 50 requests/minute (free tier) or 500 req/min (paid tier)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Implement request queue with rate limiting:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, max_requests: int = 50, window: int = 60):\n        self.max_requests = max_requests\n        self.window = window  # seconds\n        self.requests = deque()\n\n    def wait_if_needed(self):\n        now = time.time()\n        # Remove old requests outside window\n        while self.requests and self.requests[0] < now - self.window:\n            self.requests.popleft()\n\n        # Check if at limit\n        if len(self.requests) >= self.max_requests:\n            sleep_time = self.requests[0] + self.window - now\n            print(f"Rate limit reached. Waiting {sleep_time:.1f}s...")\n            time.sleep(sleep_time)\n\n        self.requests.append(now)\n\n# Usage\nlimiter = RateLimiter(max_requests=50, window=60)\n\nlimiter.wait_if_needed()\ntranscription = client.audio.transcriptions.create(...)\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Upgrade to paid tier for higher limits"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"cost-optimization-strategies",children:"Cost Optimization Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"strategy-1-silence-detection",children:"Strategy 1: Silence Detection"}),"\n",(0,r.jsx)(n.p,{children:"Don't transcribe silent audio (saves API calls):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\ndef is_silent(audio_data: bytes, threshold: int = 500) -> bool:\n    """Detect if audio chunk is silent"""\n    samples = np.frombuffer(audio_data, dtype=np.int16)\n    rms = np.sqrt(np.mean(samples**2))\n    return rms < threshold\n\n# Only transcribe non-silent chunks\nif not is_silent(audio_chunk):\n    transcription = client.audio.transcriptions.create(...)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Savings"}),": ~80% reduction in API calls for typical robot environments (lots of silence)."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"strategy-2-wake-word-detection",children:"Strategy 2: Wake Word Detection"}),"\n",(0,r.jsx)(n.p,{children:'Use local lightweight model to detect wake word ("Hey robot"), then use Whisper for full command:'}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import pvporcupine\n\n# Initialize Porcupine (local wake word detection)\nporcupine = pvporcupine.create(\n    access_key="YOUR_PICOVOICE_KEY",\n    keywords=["jarvis", "alexa", "computer"]\n)\n\n# Listen for wake word (offline, free)\nif porcupine.process(audio_frame) >= 0:\n    print("Wake word detected! Activating Whisper...")\n\n    # Now use Whisper for full command\n    transcription = client.audio.transcriptions.create(...)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Savings"}),": Only pay for commands after wake word (not continuous transcription)."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"strategy-3-caching-frequent-commands",children:"Strategy 3: Caching Frequent Commands"}),"\n",(0,r.jsx)(n.p,{children:"Cache transcriptions of common commands:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import hashlib\n\nclass TranscriptionCache:\n    def __init__(self):\n        self.cache = {}\n\n    def get_audio_hash(self, audio_data: bytes) -> str:\n        """Create hash of audio data"""\n        return hashlib.md5(audio_data).hexdigest()\n\n    def get(self, audio_data: bytes) -> str | None:\n        """Get cached transcription"""\n        audio_hash = self.get_audio_hash(audio_data)\n        return self.cache.get(audio_hash)\n\n    def set(self, audio_data: bytes, transcription: str):\n        """Cache transcription"""\n        audio_hash = self.get_audio_hash(audio_data)\n        self.cache[audio_hash] = transcription\n\n# Usage\ncache = TranscriptionCache()\n\ncached = cache.get(audio_data)\nif cached:\n    print(f"Cache hit: {cached}")\n    transcription = cached\nelse:\n    transcription = client.audio.transcriptions.create(...)\n    cache.set(audio_data, transcription)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Savings"}),": ~20% reduction for robots with repetitive commands."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this lesson, you learned to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Set up OpenAI Whisper API with authentication and error handling"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Use multiple response formats (text, JSON, verbose JSON with timestamps)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Implement async transcription for low-latency applications"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimize audio formats (WAV \u2192 MP3) for cost savings"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Integrate Whisper with ROS 2 for voice-controlled robotics"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Apply cost optimization strategies (silence detection, caching, wake words)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper API is ideal for prototyping (99 languages, robust to noise)"}),"\n",(0,r.jsx)(n.li,{children:"Use async API and MP3 compression for production systems"}),"\n",(0,r.jsx)(n.li,{children:"Add domain-specific prompts to improve accuracy on technical terms"}),"\n",(0,r.jsx)(n.li,{children:"Implement rate limiting and error handling for reliability"}),"\n",(0,r.jsxs)(n.li,{children:["Cost: ",(0,r.jsx)(n.del,{children:"$0.006/minute ("}),"$0.36/hour of continuous listening)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsx)(n.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/audio",children:"OpenAI Whisper API Reference"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper/blob/main/model-card.md",children:"Whisper Model Card"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/openai-python",children:"OpenAI Python SDK"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"Robust Speech Recognition via Large-Scale Weak Supervision (Whisper Paper)"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://cdn.openai.com/papers/whisper.pdf",children:"Whisper: Robust Speech Recognition"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"cost-calculators",children:"Cost Calculators"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://openai.com/pricing",children:"OpenAI Pricing"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://whisper.ai/calculator",children:"Whisper Cost Calculator"})," (unofficial)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"alternative-solutions",children:"Alternative Solutions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Local Whisper"}),": ",(0,r.jsx)(n.a,{href:"https://github.com/guillaumekln/faster-whisper",children:"faster-whisper"})," (4x faster, GPU-accelerated)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Streaming ASR"}),": ",(0,r.jsx)(n.a,{href:"https://deepgram.com/",children:"Deepgram"})," (real-time streaming)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Offline ASR"}),": ",(0,r.jsx)(n.a,{href:"https://alphacephei.com/vosk/",children:"Vosk"})," (free, offline, 20+ languages)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"video-tutorials",children:"Video Tutorials"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=_example",children:"Whisper API Tutorial"})," - API setup walkthrough"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=_example",children:"Building Voice-Controlled Robots"})," - ROS 2 integration"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.strong,{children:"Lesson 3: Speech-to-Text Pipeline"}),", you'll learn to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Run local Whisper models on edge devices (Jetson, Raspberry Pi)"}),"\n",(0,r.jsx)(n.li,{children:"Optimize Whisper inference with TensorRT"}),"\n",(0,r.jsx)(n.li,{children:"Build a complete transcription pipeline with error handling"}),"\n",(0,r.jsx)(n.li,{children:"Compare cloud vs. edge ASR performance"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Continue to ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/ch11-whisper/speech-to-text",children:"Speech-to-Text Pipeline \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);